{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LMEéŠ…å…ˆç‰©éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n\n## åˆ†ææ¦‚è¦\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆM1-M2, M2-M3, M3-M4ï¼‰ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹æ€§ã‚’è©³ç´°ã«åˆ†æã—ã€GARCHç³»ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯ç‰¹æ€§ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚\n\n### åˆ†æç›®æ¨™\n- ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®æ¤œå‡º\n- GARCHãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬\n- ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆVaRã€ESï¼‰ã®ç®—å‡º\n- ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã®è­˜åˆ¥\n\n### æœŸå¾…ã•ã‚Œã‚‹æˆæœ\n- å„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å‹•å‘ã®ç†è§£\n- å‹•çš„ãƒªã‚¹ã‚¯ç®¡ç†ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n- ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹å–å¼•æˆ¦ç•¥ã®åŸºç›¤\n- ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚¹ã‚¯ã‚·ãƒŠãƒªã‚ª",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psycopg2\nfrom sqlalchemy import create_engine\nimport warnings\nfrom datetime import datetime, timedelta\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nimport os\n\n# Volatility modeling\nfrom arch import arch_model\nfrom arch.unitroot import ADF\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.preprocessing import StandardScaler\n\n# Risk calculation\nfrom scipy.stats import norm, t\nfrom scipy.optimize import minimize\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Data retrieval and preprocessing\ndef get_db_connection():\n    \"\"\"Get PostgreSQL database connection\"\"\"\n    try:\n        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n        return engine\n    except Exception as e:\n        print(f\"Database connection error: {e}\")\n        return None\n\ndef load_and_prepare_data():\n    \"\"\"Retrieve spread data and calculate daily returns\"\"\"\n    engine = get_db_connection()\n    \n    query = \"\"\"\n    SELECT \n        trade_date,\n        contract_month,\n        close_price\n    FROM lme_copper_futures \n    WHERE contract_month IN (1, 2, 3, 4)\n        AND close_price IS NOT NULL\n        AND close_price > 0\n    ORDER BY trade_date, contract_month\n    \"\"\"\n    \n    df = pd.read_sql(query, engine)\n    df['trade_date'] = pd.to_datetime(df['trade_date'])\n    \n    # Pivot and calculate spreads\n    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n    \n    # Calculate spreads\n    spreads_df = pd.DataFrame(index=pivot_df.index)\n    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n    \n    # Calculate daily returns\n    spreads_df['M1_M2_return'] = spreads_df['M1_M2_spread'].pct_change()\n    spreads_df['M2_M3_return'] = spreads_df['M2_M3_spread'].pct_change()\n    spreads_df['M3_M4_return'] = spreads_df['M3_M4_spread'].pct_change()\n    \n    # Also calculate log differences (log returns)\n    spreads_df['M1_M2_log_return'] = np.log(spreads_df['M1_M2_spread'].abs()).diff()\n    spreads_df['M2_M3_log_return'] = np.log(spreads_df['M2_M3_spread'].abs()).diff()\n    spreads_df['M3_M4_log_return'] = np.log(spreads_df['M3_M4_spread'].abs()).diff()\n    \n    return spreads_df.dropna()\n\n# Retrieve data\nspreads_data = load_and_prepare_data()\nprint(f\"âœ… Data retrieval complete: {len(spreads_data):,} records\")\nprint(f\"ğŸ“… Analysis period: {spreads_data.index.min()} to {spreads_data.index.max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. åŸºæœ¬ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_basic_volatility(df):\n    \"\"\"Basic volatility analysis\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    \n    print(\"ğŸ“Š Basic Volatility Statistics:\")\n    print(\"=\" * 50)\n    \n    volatility_stats = pd.DataFrame()\n    \n    for col in return_columns:\n        spread_name = col.replace('_return', '').replace('_', '-')\n        returns = df[col].dropna()\n        \n        # Basic statistics\n        volatility_stats[spread_name] = [\n            returns.std() * 100,                    # Daily volatility (%)\n            returns.std() * np.sqrt(252) * 100,     # Annualized volatility (%)\n            returns.skew(),                         # Skewness\n            returns.kurtosis(),                     # Kurtosis (excess kurtosis)\n            (returns.abs() > 2*returns.std()).sum(), # Number of 2Ïƒ exceedances\n            (returns.abs() > 3*returns.std()).sum(), # Number of 3Ïƒ exceedances\n            len(returns)                            # Valid data count\n        ]\n    \n    volatility_stats.index = [\n        'Daily Volatility (%)', 'Annualized Volatility (%)', 'Skewness', 'Kurtosis',\n        '2Ïƒ Exceedances', '3Ïƒ Exceedances', 'Data Count'\n    ]\n    \n    print(volatility_stats.round(4))\n    \n    # Normality tests\n    print(\"\\nğŸ”¬ Normality Test Results:\")\n    print(\"-\" * 40)\n    \n    for col in return_columns:\n        spread_name = col.replace('_return', '').replace('_', '-')\n        returns = df[col].dropna()\n        \n        # Jarque-Bera test\n        jb_stat, jb_p = stats.jarque_bera(returns)\n        \n        # Shapiro-Wilk test (with sample size limit)\n        if len(returns) <= 5000:\n            sw_stat, sw_p = stats.shapiro(returns)\n        else:\n            sw_stat, sw_p = stats.shapiro(returns.sample(5000, random_state=42))\n        \n        print(f\"{spread_name}:\")\n        print(f\"  Jarque-Bera: Statistic={jb_stat:.4f}, p-value={jb_p:.4f}\")\n        print(f\"  Shapiro-Wilk: Statistic={sw_stat:.4f}, p-value={sw_p:.4f}\")\n        print(f\"  Result: {'Non-normal distribution' if jb_p < 0.05 else 'Possibly normal distribution'}\")\n    \n    return volatility_stats\n\n# Execute basic volatility analysis\nvol_stats = analyze_basic_volatility(spreads_data)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_volatility_analysis(df):\n    \"\"\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æã®å¯è¦–åŒ–\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=(\n            'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒªã‚¿ãƒ¼ãƒ³æ™‚ç³»åˆ—',\n            'ãƒªã‚¿ãƒ¼ãƒ³åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰',\n            'ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ30æ—¥ï¼‰',\n            'Q-Qãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¯¾æ­£è¦åˆ†å¸ƒï¼‰',\n            'çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³è‡ªå·±ç›¸é–¢',\n            'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°'\n        ),\n        vertical_spacing=0.08\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    # 1. Return time series\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        fig.add_trace(\n            go.Scatter(\n                x=df.index,\n                y=df[col] * 100,  # Convert to percentage\n                name=f'{name} ãƒªã‚¿ãƒ¼ãƒ³',\n                line=dict(color=color, width=0.8),\n                opacity=0.7\n            ),\n            row=1, col=1\n        )\n    \n    # 2. Return distribution\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        fig.add_trace(\n            go.Histogram(\n                x=df[col] * 100,\n                name=f'{name} åˆ†å¸ƒ',\n                nbinsx=50,\n                opacity=0.7,\n                marker_color=color\n            ),\n            row=1, col=2\n        )\n    \n    # 3. Rolling volatility\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        rolling_vol = df[col].rolling(window=30).std() * np.sqrt(252) * 100\n        fig.add_trace(\n            go.Scatter(\n                x=df.index,\n                y=rolling_vol,\n                name=f'{name} ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n                line=dict(color=color, width=1.5)\n            ),\n            row=2, col=1\n        )\n    \n    # 4. Q-Q plot (M1-M2 only)\n    from scipy.stats import probplot\n    returns_clean = df['M1_M2_return'].dropna()\n    theoretical_q, sample_q = probplot(returns_clean, dist=\"norm\")\n    \n    fig.add_trace(\n        go.Scatter(\n            x=theoretical_q,\n            y=sample_q,\n            mode='markers',\n            name='M1-M2 Q-Q',\n            marker=dict(color='blue', size=3)\n        ),\n        row=2, col=2\n    )\n    \n    # Theoretical line\n    fig.add_trace(\n        go.Scatter(\n            x=[theoretical_q.min(), theoretical_q.max()],\n            y=[sample_q.min(), sample_q.max()],\n            mode='lines',\n            name='ç†è«–ç·š',\n            line=dict(color='red', dash='dash')\n        ),\n        row=2, col=2\n    )\n    \n    # 5. Absolute return autocorrelation (volatility clustering detection)\n    abs_returns = df['M1_M2_return'].abs().dropna()\n    lags = range(1, 21)\n    autocorrs = [abs_returns.autocorr(lag=lag) for lag in lags]\n    \n    fig.add_trace(\n        go.Bar(\n            x=list(lags),\n            y=autocorrs,\n            name='M1-M2 çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³è‡ªå·±ç›¸é–¢',\n            marker_color='lightblue'\n        ),\n        row=3, col=1\n    )\n    \n    # Significance threshold lines (5% level)\n    n = len(abs_returns)\n    threshold = 1.96 / np.sqrt(n)\n    fig.add_hline(y=threshold, line_dash=\"dash\", line_color=\"red\", \n                 line_width=1, row=3, col=1)\n    fig.add_hline(y=-threshold, line_dash=\"dash\", line_color=\"red\", \n                 line_width=1, row=3, col=1)\n    \n    # 6. Volatility clustering visualization\n    squared_returns = (df['M1_M2_return'] ** 2).dropna()\n    fig.add_trace(\n        go.Scatter(\n            x=df.index[:len(squared_returns)],\n            y=squared_returns * 10000,  # basis points\n            name='M1-M2 äºŒä¹—ãƒªã‚¿ãƒ¼ãƒ³',\n            line=dict(color='purple', width=1),\n            fill='tonexty',\n            fillcolor='rgba(128,0,128,0.3)'\n        ),\n        row=3, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æ\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=1000,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"ãƒªã‚¿ãƒ¼ãƒ³ (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"é »åº¦\", row=1, col=2)\n    fig.update_xaxes(title_text=\"ãƒªã‚¿ãƒ¼ãƒ³ (%)\", row=1, col=2)\n    \n    fig.update_yaxes(title_text=\"å¹´ç‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (%)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"æ¨™æœ¬åˆ†ä½æ•°\", row=2, col=2)\n    fig.update_xaxes(title_text=\"ç†è«–åˆ†ä½æ•°\", row=2, col=2)\n    \n    fig.update_yaxes(title_text=\"è‡ªå·±ç›¸é–¢\", row=3, col=1)\n    fig.update_xaxes(title_text=\"ãƒ©ã‚°\", row=3, col=1)\n    \n    fig.update_yaxes(title_text=\"äºŒä¹—ãƒªã‚¿ãƒ¼ãƒ³ (bp)\", row=3, col=2)\n    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=3, col=2)\n    \n    return fig\n\n# Volatility analysis chart\nvol_chart = plot_volatility_analysis(spreads_data)\nvol_chart.show()\n\n# Save image\nos.makedirs('../generated_images', exist_ok=True)\nvol_chart.write_image('../generated_images/adjacent_spreads_volatility_analysis.png', \n                     width=1200, height=1000, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. ARCHåŠ¹æœæ¤œå®š",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_arch_effects(df):\n",
    "    \"\"\"ARCHåŠ¹æœï¼ˆæ¡ä»¶ä»˜ãåˆ†æ•£ä¸å‡ä¸€æ€§ï¼‰ã®æ¤œå®š\"\"\"\n",
    "    from arch.unitroot import DFGLS\n",
    "    from statsmodels.stats.diagnostic import het_arch\n",
    "    \n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"ğŸ”¬ ARCHåŠ¹æœæ¤œå®šçµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    arch_test_results = {}\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        returns = df[col].dropna() * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå¤‰æ›\n",
    "        \n",
    "        print(f\"\\n{name}ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰:\")\n",
    "        \n",
    "        # 1. åŸºæœ¬çµ±è¨ˆé‡\n",
    "        print(f\"  ãƒ‡ãƒ¼ã‚¿æ•°: {len(returns):,}\")\n",
    "        print(f\"  å¹³å‡: {returns.mean():.4f}%\")\n",
    "        print(f\"  æ¨™æº–åå·®: {returns.std():.4f}%\")\n",
    "        \n",
    "        # 2. å®šå¸¸æ€§æ¤œå®šï¼ˆADFï¼‰\n",
    "        adf_result = adfuller(returns)\n",
    "        print(f\"  ADFæ¤œå®š: çµ±è¨ˆé‡={adf_result[0]:.4f}, på€¤={adf_result[1]:.4f}\")\n",
    "        print(f\"    çµæœ: {'å®šå¸¸' if adf_result[1] < 0.05 else 'éå®šå¸¸'}\")\n",
    "        \n",
    "        # 3. Ljung-Boxæ¤œå®šï¼ˆç³»åˆ—ç›¸é–¢ï¼‰\n",
    "        try:\n",
    "            lb_result = acorr_ljungbox(returns, lags=10, return_df=True)\n",
    "            lb_pvalue = lb_result['lb_pvalue'].iloc[-1]  # 10ãƒ©ã‚°ã®çµæœ\n",
    "            print(f\"  Ljung-Boxæ¤œå®š(10ãƒ©ã‚°): på€¤={lb_pvalue:.4f}\")\n",
    "            print(f\"    çµæœ: {'ç³»åˆ—ç›¸é–¢ã‚ã‚Š' if lb_pvalue < 0.05 else 'ç³»åˆ—ç›¸é–¢ãªã—'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Ljung-Boxæ¤œå®š: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            lb_pvalue = np.nan\n",
    "        \n",
    "        # 4. ARCH-LMæ¤œå®šï¼ˆæ¡ä»¶ä»˜ãåˆ†æ•£ä¸å‡ä¸€æ€§ï¼‰\n",
    "        try:\n",
    "            # è¤‡æ•°ã®ãƒ©ã‚°ã§æ¤œå®š\n",
    "            for lag in [5, 10, 15]:\n",
    "                lm_stat, lm_pvalue, _, _ = het_arch(returns, nlags=lag)\n",
    "                print(f\"  ARCH-LMæ¤œå®š({lag}ãƒ©ã‚°): çµ±è¨ˆé‡={lm_stat:.4f}, på€¤={lm_pvalue:.4f}\")\n",
    "                if lag == 10:  # 10ãƒ©ã‚°ã®çµæœã‚’ä¿å­˜\n",
    "                    main_arch_pvalue = lm_pvalue\n",
    "            \n",
    "            print(f\"    çµæœ: {'ARCHåŠ¹æœã‚ã‚Š' if main_arch_pvalue < 0.05 else 'ARCHåŠ¹æœãªã—'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ARCH-LMæ¤œå®š: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            main_arch_pvalue = np.nan\n",
    "        \n",
    "        # 5. çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³ã®è‡ªå·±ç›¸é–¢æ¤œå®š\n",
    "        abs_returns = returns.abs()\n",
    "        try:\n",
    "            abs_lb_result = acorr_ljungbox(abs_returns, lags=10, return_df=True)\n",
    "            abs_lb_pvalue = abs_lb_result['lb_pvalue'].iloc[-1]\n",
    "            print(f\"  çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³Ljung-Boxæ¤œå®š: på€¤={abs_lb_pvalue:.4f}\")\n",
    "            print(f\"    çµæœ: {'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚ã‚Š' if abs_lb_pvalue < 0.05 else 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãªã—'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³Ljung-Boxæ¤œå®š: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            abs_lb_pvalue = np.nan\n",
    "        \n",
    "        # çµæœã‚’ä¿å­˜\n",
    "        arch_test_results[name] = {\n",
    "            'data_count': len(returns),\n",
    "            'mean': returns.mean(),\n",
    "            'std': returns.std(),\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'is_stationary': adf_result[1] < 0.05,\n",
    "            'ljungbox_pvalue': lb_pvalue,\n",
    "            'has_serial_correlation': lb_pvalue < 0.05 if not np.isnan(lb_pvalue) else None,\n",
    "            'arch_lm_pvalue': main_arch_pvalue,\n",
    "            'has_arch_effects': main_arch_pvalue < 0.05 if not np.isnan(main_arch_pvalue) else None,\n",
    "            'abs_ljungbox_pvalue': abs_lb_pvalue,\n",
    "            'has_volatility_clustering': abs_lb_pvalue < 0.05 if not np.isnan(abs_lb_pvalue) else None\n",
    "        }\n",
    "    \n",
    "    return arch_test_results\n",
    "\n",
    "# ARCHåŠ¹æœæ¤œå®šå®Ÿè¡Œ\n",
    "arch_results = test_arch_effects(spreads_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. GARCHãƒ¢ãƒ‡ãƒ«æ¨å®š",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_garch_models(df):\n",
    "    \"\"\"GARCHç³»ãƒ¢ãƒ‡ãƒ«ã®æ¨å®š\"\"\"\n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    garch_models = {}\n",
    "    \n",
    "    print(\"ğŸ“ˆ GARCHãƒ¢ãƒ‡ãƒ«æ¨å®šçµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        returns = df[col].dropna() * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå¤‰æ›\n",
    "        \n",
    "        print(f\"\\n{name}ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰:\")\n",
    "        \n",
    "        models_to_estimate = {\n",
    "            'GARCH(1,1)': arch_model(returns, vol='Garch', p=1, q=1),\n",
    "            'EGARCH(1,1)': arch_model(returns, vol='EGARCH', p=1, q=1),\n",
    "            'GJR-GARCH(1,1)': arch_model(returns, vol='GARCH', p=1, o=1, q=1)\n",
    "        }\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        for model_name, model in models_to_estimate.items():\n",
    "            try:\n",
    "                # ãƒ¢ãƒ‡ãƒ«æ¨å®š\n",
    "                result = model.fit(disp='off', show_warning=False)\n",
    "                \n",
    "                # AIC/BICè¨ˆç®—\n",
    "                aic = result.aic\n",
    "                bic = result.bic\n",
    "                log_likelihood = result.loglikelihood\n",
    "                \n",
    "                print(f\"  {model_name}:\")\n",
    "                print(f\"    å¯¾æ•°å°¤åº¦: {log_likelihood:.4f}\")\n",
    "                print(f\"    AIC: {aic:.4f}\")\n",
    "                print(f\"    BIC: {bic:.4f}\")\n",
    "                \n",
    "                # æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "                conditional_volatility = result.conditional_volatility\n",
    "                \n",
    "                model_results[model_name] = {\n",
    "                    'model': result,\n",
    "                    'aic': aic,\n",
    "                    'bic': bic,\n",
    "                    'log_likelihood': log_likelihood,\n",
    "                    'conditional_volatility': conditional_volatility,\n",
    "                    'residuals': result.resid,\n",
    "                    'standardized_residuals': result.std_resid\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name}: æ¨å®šã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "                continue\n",
    "        \n",
    "        # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆAICåŸºæº–ï¼‰\n",
    "        if model_results:\n",
    "            best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['aic'])\n",
    "            print(f\"  \\n  ğŸ“Š æœ€é©ãƒ¢ãƒ‡ãƒ«ï¼ˆAICåŸºæº–ï¼‰: {best_model_name}\")\n",
    "            \n",
    "            garch_models[name] = {\n",
    "                'all_models': model_results,\n",
    "                'best_model_name': best_model_name,\n",
    "                'best_model': model_results[best_model_name]\n",
    "            }\n",
    "    \n",
    "    return garch_models\n",
    "\n",
    "# GARCHãƒ¢ãƒ‡ãƒ«æ¨å®š\n",
    "garch_models = estimate_garch_models(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_garch_results(garch_models, spreads_data):\n    \"\"\"GARCHãƒ¢ãƒ‡ãƒ«çµæœã®å¯è¦–åŒ–\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=(\n            'M1-M2: ãƒªã‚¿ãƒ¼ãƒ³ã¨æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n            'M1-M2: æ¨™æº–åŒ–æ®‹å·®',\n            'M2-M3: ãƒªã‚¿ãƒ¼ãƒ³ã¨æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n            'M2-M3: æ¨™æº–åŒ–æ®‹å·®',\n            'M3-M4: ãƒªã‚¿ãƒ¼ãƒ³ã¨æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n            'M3-M4: æ¨™æº–åŒ–æ®‹å·®'\n        ),\n        vertical_spacing=0.08\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        if name in garch_models:\n            best_model = garch_models[name]['best_model']\n            \n            returns = spreads_data[col].dropna() * 100\n            cond_vol = best_model['conditional_volatility']\n            std_resid = best_model['standardized_residuals']\n            \n            # Returns and conditional volatility\n            fig.add_trace(\n                go.Scatter(\n                    x=returns.index,\n                    y=returns,\n                    name=f'{name} ãƒªã‚¿ãƒ¼ãƒ³',\n                    line=dict(color=color, width=0.8),\n                    opacity=0.7\n                ),\n                row=i+1, col=1\n            )\n            \n            # Â±2Ïƒ bands of conditional volatility\n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=2 * cond_vol,\n                    name=f'{name} +2Ïƒ',\n                    line=dict(color='red', dash='dash', width=1),\n                    showlegend=False\n                ),\n                row=i+1, col=1\n            )\n            \n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=-2 * cond_vol,\n                    name=f'{name} -2Ïƒ',\n                    line=dict(color='red', dash='dash', width=1),\n                    fill='tonexty',\n                    fillcolor='rgba(255,0,0,0.1)',\n                    showlegend=False\n                ),\n                row=i+1, col=1\n            )\n            \n            # Standardized residuals\n            fig.add_trace(\n                go.Scatter(\n                    x=std_resid.index,\n                    y=std_resid,\n                    name=f'{name} æ¨™æº–åŒ–æ®‹å·®',\n                    line=dict(color='purple', width=0.8),\n                    mode='markers',\n                    marker=dict(size=2)\n                ),\n                row=i+1, col=2\n            )\n            \n            # Â±2Ïƒ lines\n            fig.add_hline(y=2, line_dash=\"dash\", line_color=\"red\", \n                         line_width=1, row=i+1, col=2)\n            fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"red\", \n                         line_width=1, row=i+1, col=2)\n            fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n                         line_width=1, row=i+1, col=2)\n    \n    fig.update_layout(\n        title=dict(\n            text=\"GARCHãƒ¢ãƒ‡ãƒ«æ¨å®šçµæœ\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=1000,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    for i in range(3):\n        fig.update_yaxes(title_text=\"ãƒªã‚¿ãƒ¼ãƒ³ (%)\", row=i+1, col=1)\n        fig.update_yaxes(title_text=\"æ¨™æº–åŒ–æ®‹å·®\", row=i+1, col=2)\n        if i == 2:\n            fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=i+1, col=1)\n            fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=i+1, col=2)\n    \n    return fig\n\n# Visualization of GARCH model results\ngarch_chart = plot_garch_results(garch_models, spreads_data)\ngarch_chart.show()\n\n# Save image\ngarch_chart.write_image('../generated_images/adjacent_spreads_garch_models.png', \n                       width=1200, height=1000, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. ãƒªã‚¹ã‚¯æŒ‡æ¨™è¨ˆç®—",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_metrics(garch_models, spreads_data):\n",
    "    \"\"\"VaRã€Expected Shortfallç­‰ã®ãƒªã‚¹ã‚¯æŒ‡æ¨™è¨ˆç®—\"\"\"\n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    confidence_levels = [0.95, 0.99, 0.995]\n",
    "    \n",
    "    print(\"âš ï¸ ãƒªã‚¹ã‚¯æŒ‡æ¨™è¨ˆç®—çµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    risk_metrics = {}\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        if name not in garch_models:\n",
    "            continue\n",
    "            \n",
    "        returns = spreads_data[col].dropna() * 100\n",
    "        best_model = garch_models[name]['best_model']\n",
    "        cond_vol = best_model['conditional_volatility']\n",
    "        std_resid = best_model['standardized_residuals']\n",
    "        \n",
    "        print(f\"\\n{name}ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰:\")\n",
    "        \n",
    "        current_volatility = cond_vol.iloc[-1]  # æœ€æ–°ã®æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "        print(f\"  æœ€æ–°æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {current_volatility:.4f}%\")\n",
    "        \n",
    "        spread_risk_metrics = {\n",
    "            'current_volatility': current_volatility,\n",
    "            'conditional_volatility': cond_vol,\n",
    "            'var': {},\n",
    "            'expected_shortfall': {},\n",
    "            'parametric_var': {},\n",
    "            'historical_var': {}\n",
    "        }\n",
    "        \n",
    "        # å„ä¿¡é ¼æ°´æº–ã§ãƒªã‚¹ã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—\n",
    "        for alpha in confidence_levels:\n",
    "            print(f\"\\n  ä¿¡é ¼æ°´æº– {alpha*100:.1f}%:\")\n",
    "            \n",
    "            # 1. ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«VaR\n",
    "            historical_var = np.percentile(returns, (1-alpha)*100)\n",
    "            \n",
    "            # 2. ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯VaRï¼ˆæ­£è¦åˆ†å¸ƒä»®å®šï¼‰\n",
    "            parametric_var = norm.ppf(1-alpha) * current_volatility\n",
    "            \n",
    "            # 3. GARCH-VaRï¼ˆtåˆ†å¸ƒä»®å®šï¼‰\n",
    "            # æ¨™æº–åŒ–æ®‹å·®ã®åˆ†å¸ƒã‚’tåˆ†å¸ƒã§ãƒ•ã‚£ãƒƒãƒˆ\n",
    "            try:\n",
    "                clean_std_resid = std_resid.dropna()\n",
    "                df_fitted, loc_fitted, scale_fitted = stats.t.fit(clean_std_resid)\n",
    "                garch_var = t.ppf(1-alpha, df=df_fitted, loc=loc_fitted, scale=scale_fitted) * current_volatility\n",
    "            except:\n",
    "                garch_var = parametric_var  # ãƒ•ã‚£ãƒƒãƒˆã«å¤±æ•—ã—ãŸå ´åˆã¯æ­£è¦åˆ†å¸ƒã‚’ä½¿ç”¨\n",
    "            \n",
    "            # 4. Expected Shortfall (CVaR)\n",
    "            var_threshold = historical_var\n",
    "            tail_returns = returns[returns <= var_threshold]\n",
    "            if len(tail_returns) > 0:\n",
    "                expected_shortfall = tail_returns.mean()\n",
    "            else:\n",
    "                expected_shortfall = historical_var\n",
    "            \n",
    "            print(f\"    ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«VaR: {historical_var:.4f}%\")\n",
    "            print(f\"    ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯VaR: {parametric_var:.4f}%\")\n",
    "            print(f\"    GARCH-VaR: {garch_var:.4f}%\")\n",
    "            print(f\"    Expected Shortfall: {expected_shortfall:.4f}%\")\n",
    "            \n",
    "            # çµæœã‚’ä¿å­˜\n",
    "            alpha_key = f'{alpha*100:.1f}%'\n",
    "            spread_risk_metrics['historical_var'][alpha_key] = historical_var\n",
    "            spread_risk_metrics['parametric_var'][alpha_key] = parametric_var\n",
    "            spread_risk_metrics['var'][alpha_key] = garch_var\n",
    "            spread_risk_metrics['expected_shortfall'][alpha_key] = expected_shortfall\n",
    "        \n",
    "        # è¿½åŠ ã®ãƒªã‚¹ã‚¯æŒ‡æ¨™\n",
    "        max_drawdown = calculate_max_drawdown(returns)\n",
    "        avg_vol_30d = cond_vol.rolling(window=30).mean().iloc[-1]\n",
    "        vol_of_vol = cond_vol.rolling(window=30).std().iloc[-1]\n",
    "        \n",
    "        print(f\"\\n  è¿½åŠ ãƒªã‚¹ã‚¯æŒ‡æ¨™:\")\n",
    "        print(f\"    æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³: {max_drawdown:.4f}%\")\n",
    "        print(f\"    30æ—¥å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {avg_vol_30d:.4f}%\")\n",
    "        print(f\"    ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {vol_of_vol:.4f}%\")\n",
    "        \n",
    "        spread_risk_metrics.update({\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'avg_volatility_30d': avg_vol_30d,\n",
    "            'volatility_of_volatility': vol_of_vol\n",
    "        })\n",
    "        \n",
    "        risk_metrics[name] = spread_risk_metrics\n",
    "    \n",
    "    return risk_metrics\n",
    "\n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã®è¨ˆç®—\"\"\"\n",
    "    cumulative = (1 + returns/100).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max * 100\n",
    "    return drawdown.min()\n",
    "\n",
    "# ãƒªã‚¹ã‚¯æŒ‡æ¨™è¨ˆç®—\n",
    "risk_metrics = calculate_risk_metrics(garch_models, spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_risk_metrics(risk_metrics, spreads_data):\n    \"\"\"ãƒªã‚¹ã‚¯æŒ‡æ¨™ã®å¯è¦–åŒ–\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=(\n            'VaRæ¯”è¼ƒï¼ˆ95%ä¿¡é ¼æ°´æº–ï¼‰',\n            'æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨ç§»',\n            'VaRãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«VaR vs å®Ÿç¸¾ï¼‰',\n            'ãƒªã‚¹ã‚¯æŒ‡æ¨™ã‚µãƒãƒªãƒ¼'\n        )\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    # 1. VaR comparison\n    var_types = ['ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«VaR', 'ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯VaR', 'GARCH-VaR']\n    var_keys = ['historical_var', 'parametric_var', 'var']\n    \n    x_pos = np.arange(len(spread_names))\n    bar_width = 0.25\n    \n    for i, (var_type, var_key) in enumerate(zip(var_types, var_keys)):\n        var_values = []\n        for name in spread_names:\n            if name in risk_metrics:\n                var_values.append(abs(risk_metrics[name][var_key]['95.0%']))\n            else:\n                var_values.append(0)\n        \n        fig.add_trace(\n            go.Bar(\n                x=[f\"{name}<br>({var_type})\" for name in spread_names],\n                y=var_values,\n                name=var_type,\n                marker_color=colors[i],\n                opacity=0.8\n            ),\n            row=1, col=1\n        )\n    \n    # 2. Conditional volatility evolution\n    for i, (name, color) in enumerate(zip(spread_names, colors)):\n        if name in risk_metrics:\n            cond_vol = risk_metrics[name]['conditional_volatility']\n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=cond_vol,\n                    name=f'{name} æ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n                    line=dict(color=color, width=1.5)\n                ),\n                row=1, col=2\n            )\n    \n    # 3. VaR backtest (M1-M2 only)\n    if 'M1-M2' in risk_metrics:\n        returns = spreads_data['M1_M2_return'].dropna() * 100\n        historical_var_95 = risk_metrics['M1-M2']['historical_var']['95.0%']\n        \n        fig.add_trace(\n            go.Scatter(\n                x=returns.index,\n                y=returns,\n                name='M1-M2 ãƒªã‚¿ãƒ¼ãƒ³',\n                line=dict(color='blue', width=0.8),\n                opacity=0.7\n            ),\n            row=2, col=1\n        )\n        \n        # VaR line\n        fig.add_hline(y=historical_var_95, line_dash=\"dash\", line_color=\"red\", \n                     line_width=2, row=2, col=1)\n        \n        # Highlight VaR violations\n        var_violations = returns[returns < historical_var_95]\n        if len(var_violations) > 0:\n            fig.add_trace(\n                go.Scatter(\n                    x=var_violations.index,\n                    y=var_violations,\n                    mode='markers',\n                    name='VaRé•å',\n                    marker=dict(color='red', size=6, symbol='x')\n                ),\n                row=2, col=1\n            )\n        \n        # VaR violation rate\n        violation_rate = len(var_violations) / len(returns) * 100\n        expected_rate = 5.0  # Expected violation rate for 95% VaR\n        \n    # 4. Risk metrics summary (table)\n    summary_data = []\n    for name in spread_names:\n        if name in risk_metrics:\n            metrics = risk_metrics[name]\n            summary_data.append([\n                name,\n                f\"{metrics['current_volatility']:.3f}%\",\n                f\"{abs(metrics['var']['95.0%']):.3f}%\",\n                f\"{abs(metrics['expected_shortfall']['95.0%']):.3f}%\",\n                f\"{metrics['max_drawdown']:.3f}%\"\n            ])\n    \n    fig.add_trace(\n        go.Table(\n            header=dict(\n                values=['ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰', 'ç¾åœ¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£', '95% VaR', '95% ES', 'æœ€å¤§DD'],\n                fill_color='lightblue',\n                font=dict(size=12)\n            ),\n            cells=dict(\n                values=list(zip(*summary_data)) if summary_data else [[], [], [], [], []],\n                fill_color='white',\n                font=dict(size=11)\n            )\n        ),\n        row=2, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"ãƒªã‚¹ã‚¯æŒ‡æ¨™åˆ†æ\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=800,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"VaR (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (%)\", row=1, col=2)\n    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=1, col=2)\n    \n    fig.update_yaxes(title_text=\"ãƒªã‚¿ãƒ¼ãƒ³ (%)\", row=2, col=1)\n    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=2, col=1)\n    \n    return fig\n\n# Visualization of risk metrics\nrisk_chart = plot_risk_metrics(risk_metrics, spreads_data)\nrisk_chart.show()\n\n# Save image\nrisk_chart.write_image('../generated_images/adjacent_spreads_risk_metrics.png', \n                      width=1200, height=800, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_volatility_regimes(risk_metrics, spreads_data):\n",
    "    \"\"\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ\"\"\"\n",
    "    print(\"ğŸ­ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    regime_analysis = {}\n",
    "    \n",
    "    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if name not in risk_metrics:\n",
    "            continue\n",
    "            \n",
    "        cond_vol = risk_metrics[name]['conditional_volatility']\n",
    "        \n",
    "        print(f\"\\n{name}ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰:\")\n",
    "        \n",
    "        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®åˆ†ä½æ•°ã§ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡\n",
    "        vol_25 = cond_vol.quantile(0.25)\n",
    "        vol_50 = cond_vol.quantile(0.50)\n",
    "        vol_75 = cond_vol.quantile(0.75)\n",
    "        vol_95 = cond_vol.quantile(0.95)\n",
    "        \n",
    "        print(f\"  ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†ä½æ•°:\")\n",
    "        print(f\"    25%åˆ†ä½: {vol_25:.4f}%\")\n",
    "        print(f\"    50%åˆ†ä½: {vol_50:.4f}%\")\n",
    "        print(f\"    75%åˆ†ä½: {vol_75:.4f}%\")\n",
    "        print(f\"    95%åˆ†ä½: {vol_95:.4f}%\")\n",
    "        \n",
    "        # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡\n",
    "        regimes = pd.Series(index=cond_vol.index, dtype='object')\n",
    "        regimes[cond_vol <= vol_25] = 'Low'\n",
    "        regimes[(cond_vol > vol_25) & (cond_vol <= vol_50)] = 'Medium-Low'\n",
    "        regimes[(cond_vol > vol_50) & (cond_vol <= vol_75)] = 'Medium-High'\n",
    "        regimes[cond_vol > vol_75] = 'High'\n",
    "        \n",
    "        # ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆ\n",
    "        regime_stats = regimes.value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\n  ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†å¸ƒ:\")\n",
    "        for regime, percentage in regime_stats.items():\n",
    "            print(f\"    {regime}: {percentage:.1f}%\")\n",
    "        \n",
    "        # ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›ã®åˆ†æ\n",
    "        regime_changes = (regimes != regimes.shift(1)).sum()\n",
    "        avg_regime_duration = len(regimes) / regime_changes if regime_changes > 0 else len(regimes)\n",
    "        \n",
    "        print(f\"\\n  ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›:\")\n",
    "        print(f\"    è»¢æ›å›æ•°: {regime_changes}\")\n",
    "        print(f\"    å¹³å‡ç¶™ç¶šæœŸé–“: {avg_regime_duration:.1f}æ—¥\")\n",
    "        \n",
    "        # å„ãƒ¬ã‚¸ãƒ¼ãƒ ã§ã®ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ\n",
    "        return_col = f\"{name.replace('-', '_')}_return\"\n",
    "        if return_col in spreads_data.columns:\n",
    "            returns = spreads_data[return_col] * 100\n",
    "            \n",
    "            print(f\"\\n  ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ãƒªã‚¿ãƒ¼ãƒ³çµ±è¨ˆ:\")\n",
    "            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n",
    "                regime_returns = returns[regimes == regime]\n",
    "                if len(regime_returns) > 0:\n",
    "                    print(f\"    {regime}:\")\n",
    "                    print(f\"      å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: {regime_returns.mean():.4f}%\")\n",
    "                    print(f\"      ãƒªã‚¿ãƒ¼ãƒ³æ¨™æº–åå·®: {regime_returns.std():.4f}%\")\n",
    "                    print(f\"      æœ€å¤§æå¤±: {regime_returns.min():.4f}%\")\n",
    "        \n",
    "        regime_analysis[name] = {\n",
    "            'conditional_volatility': cond_vol,\n",
    "            'regimes': regimes,\n",
    "            'quantiles': {\n",
    "                '25%': vol_25,\n",
    "                '50%': vol_50,\n",
    "                '75%': vol_75,\n",
    "                '95%': vol_95\n",
    "            },\n",
    "            'regime_distribution': regime_stats,\n",
    "            'regime_changes': regime_changes,\n",
    "            'avg_duration': avg_regime_duration\n",
    "        }\n",
    "    \n",
    "    return regime_analysis\n",
    "\n",
    "# ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æå®Ÿè¡Œ\n",
    "regime_analysis = analyze_volatility_regimes(risk_metrics, spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_volatility_regimes(regime_analysis):\n    \"\"\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã®å¯è¦–åŒ–\"\"\"\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=(\n            'M1-M2: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ',\n            'ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†å¸ƒ',\n            'M2-M3: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ',\n            'ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›é »åº¦'\n        )\n    )\n    \n    regime_colors = {\n        'Low': 'green',\n        'Medium-Low': 'yellow',\n        'Medium-High': 'orange', \n        'High': 'red'\n    }\n    \n    # 1. M1-M2 volatility regimes\n    if 'M1-M2' in regime_analysis:\n        m1m2_data = regime_analysis['M1-M2']\n        cond_vol = m1m2_data['conditional_volatility']\n        regimes = m1m2_data['regimes']\n        \n        # Volatility line\n        fig.add_trace(\n            go.Scatter(\n                x=cond_vol.index,\n                y=cond_vol,\n                name='M1-M2 ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n                line=dict(color='blue', width=1),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n        \n        # Regime boundary lines\n        quantiles = m1m2_data['quantiles']\n        for label, value in quantiles.items():\n            fig.add_hline(y=value, line_dash=\"dash\", line_color=\"gray\", \n                         line_width=1, row=1, col=1)\n        \n        # Regime color mapping\n        for regime, color in regime_colors.items():\n            regime_mask = regimes == regime\n            if regime_mask.any():\n                regime_vol = cond_vol[regime_mask]\n                fig.add_trace(\n                    go.Scatter(\n                        x=regime_vol.index,\n                        y=regime_vol,\n                        mode='markers',\n                        name=f'{regime} ãƒ¬ã‚¸ãƒ¼ãƒ ',\n                        marker=dict(color=color, size=3, opacity=0.7)\n                    ),\n                    row=1, col=1\n                )\n    \n    # 2. Regime distribution\n    all_regime_dist = []\n    spread_labels = []\n    \n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_dist = regime_analysis[name]['regime_distribution']\n            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n                if regime in regime_dist:\n                    all_regime_dist.append(regime_dist[regime])\n                    spread_labels.append(f'{name}<br>{regime}')\n                else:\n                    all_regime_dist.append(0)\n                    spread_labels.append(f'{name}<br>{regime}')\n    \n    # Grouped bar chart for regime distribution\n    x_pos = 0\n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_dist = regime_analysis[name]['regime_distribution']\n            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n                value = regime_dist.get(regime, 0)\n                fig.add_trace(\n                    go.Bar(\n                        x=[f'{name}<br>{regime}'],\n                        y=[value],\n                        name=regime,\n                        marker_color=regime_colors[regime],\n                        showlegend=(name == 'M1-M2')  # Show legend only for first group\n                    ),\n                    row=1, col=2\n                )\n    \n    # 3. M2-M3 volatility regimes\n    if 'M2-M3' in regime_analysis:\n        m2m3_data = regime_analysis['M2-M3']\n        cond_vol = m2m3_data['conditional_volatility']\n        regimes = m2m3_data['regimes']\n        \n        fig.add_trace(\n            go.Scatter(\n                x=cond_vol.index,\n                y=cond_vol,\n                name='M2-M3 ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£',\n                line=dict(color='red', width=1),\n                showlegend=False\n            ),\n            row=2, col=1\n        )\n        \n        # Regime boundary lines\n        quantiles = m2m3_data['quantiles']\n        for label, value in quantiles.items():\n            fig.add_hline(y=value, line_dash=\"dash\", line_color=\"gray\", \n                         line_width=1, row=2, col=1)\n    \n    # 4. Regime transition frequency\n    regime_changes = []\n    avg_durations = []\n    spread_names = []\n    \n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_changes.append(regime_analysis[name]['regime_changes'])\n            avg_durations.append(regime_analysis[name]['avg_duration'])\n            spread_names.append(name)\n    \n    fig.add_trace(\n        go.Bar(\n            x=spread_names,\n            y=regime_changes,\n            name='ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›å›æ•°',\n            marker_color='lightblue',\n            yaxis='y',\n            offsetgroup=1\n        ),\n        row=2, col=2\n    )\n    \n    fig.add_trace(\n        go.Scatter(\n            x=spread_names,\n            y=avg_durations,\n            name='å¹³å‡ç¶™ç¶šæœŸé–“ï¼ˆæ—¥ï¼‰',\n            mode='lines+markers',\n            line=dict(color='red', width=2),\n            marker=dict(size=8),\n            yaxis='y2'\n        ),\n        row=2, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=800,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"æ¯”ç‡ (%)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (%)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"è»¢æ›å›æ•°\", row=2, col=2)\n    \n    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=1, col=1)\n    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=2, col=1)\n    \n    # Secondary axis settings\n    fig.update_layout(\n        yaxis4=dict(\n            title=\"å¹³å‡ç¶™ç¶šæœŸé–“ï¼ˆæ—¥ï¼‰\",\n            overlaying=\"y3\",\n            side=\"right\"\n        )\n    )\n    \n    return fig\n\n# Visualization of volatility regimes\nregime_chart = plot_volatility_regimes(regime_analysis)\nregime_chart.show()\n\n# Save image\nregime_chart.write_image('../generated_images/adjacent_spreads_volatility_regimes.png', \n                        width=1200, height=800, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. åˆ†æçµæœã‚µãƒãƒªãƒ¼",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°åˆ†æã‚µãƒãƒªãƒ¼\n",
    "def generate_volatility_modeling_summary(vol_stats, arch_results, garch_models, \n",
    "                                        risk_metrics, regime_analysis):\n",
    "    \"\"\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°åˆ†æã®åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°åˆ†æã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åŸºæœ¬ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹æ€§:\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in vol_stats.columns:\n",
    "            daily_vol = vol_stats.loc['æ—¥æ¬¡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£(%)', spread]\n",
    "            annual_vol = vol_stats.loc['å¹´ç‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£(%)', spread]\n",
    "            skewness = vol_stats.loc['æ­ªåº¦', spread]\n",
    "            kurtosis = vol_stats.loc['å°–åº¦', spread]\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    æ—¥æ¬¡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {daily_vol:.3f}%\")\n",
    "            print(f\"    å¹´ç‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {annual_vol:.1f}%\")\n",
    "            print(f\"    æ­ªåº¦: {skewness:.3f} ({'å³æ­ªã¿' if skewness > 0 else 'å·¦æ­ªã¿' if skewness < 0 else 'å¯¾ç§°'})\")\n",
    "            print(f\"    å°–åº¦: {kurtosis:.3f} ({'å°–é‹­' if kurtosis > 0 else 'å¹³å¦'})\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¬ ARCHåŠ¹æœæ¤œå®š:\")\n",
    "    arch_detected = 0\n",
    "    vol_clustering_detected = 0\n",
    "    \n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in arch_results:\n",
    "            has_arch = arch_results[spread]['has_arch_effects']\n",
    "            has_clustering = arch_results[spread]['has_volatility_clustering']\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    ARCHåŠ¹æœ: {'ã‚ã‚Š' if has_arch else 'ãªã—'}\")\n",
    "            print(f\"    ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {'ã‚ã‚Š' if has_clustering else 'ãªã—'}\")\n",
    "            \n",
    "            if has_arch:\n",
    "                arch_detected += 1\n",
    "            if has_clustering:\n",
    "                vol_clustering_detected += 1\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ GARCHãƒ¢ãƒ‡ãƒ«é¸æŠ:\")\n",
    "    best_models = {}\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in garch_models:\n",
    "            best_model_name = garch_models[spread]['best_model_name']\n",
    "            best_aic = garch_models[spread]['best_model']['aic']\n",
    "            \n",
    "            print(f\"  {spread}: {best_model_name} (AIC: {best_aic:.2f})\")\n",
    "            best_models[spread] = best_model_name\n",
    "    \n",
    "    print(f\"\\nâš ï¸ ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆ95%ä¿¡é ¼æ°´æº–ï¼‰:\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in risk_metrics:\n",
    "            current_vol = risk_metrics[spread]['current_volatility']\n",
    "            var_95 = abs(risk_metrics[spread]['var']['95.0%'])\n",
    "            es_95 = abs(risk_metrics[spread]['expected_shortfall']['95.0%'])\n",
    "            max_dd = risk_metrics[spread]['max_drawdown']\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    ç¾åœ¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {current_vol:.3f}%\")\n",
    "            print(f\"    95% VaR: {var_95:.3f}%\")\n",
    "            print(f\"    95% ES: {es_95:.3f}%\")\n",
    "            print(f\"    æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³: {max_dd:.3f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸ­ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ :\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in regime_analysis:\n",
    "            regime_changes = regime_analysis[spread]['regime_changes']\n",
    "            avg_duration = regime_analysis[spread]['avg_duration']\n",
    "            high_vol_ratio = regime_analysis[spread]['regime_distribution'].get('High', 0)\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›: {regime_changes}å›\")\n",
    "            print(f\"    å¹³å‡ç¶™ç¶šæœŸé–“: {avg_duration:.1f}æ—¥\")\n",
    "            print(f\"    é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“: {high_vol_ratio:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æŠ•è³‡ãƒ»ãƒªã‚¹ã‚¯ç®¡ç†ã¸ã®ç¤ºå”†:\")\n",
    "    \n",
    "    # ARCHåŠ¹æœã«åŸºã¥ãç¤ºå”†\n",
    "    if arch_detected >= 2:\n",
    "        print(f\"  â€¢ {arch_detected}/3ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã§ARCHåŠ¹æœæ¤œå‡º â†’ GARCHãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å‹•çš„ãƒªã‚¹ã‚¯ç®¡ç†ãŒæœ‰åŠ¹\")\n",
    "    \n",
    "    if vol_clustering_detected >= 2:\n",
    "        print(f\"  â€¢ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç¢ºèª â†’ é«˜ãƒœãƒ©æœŸé–“ã§ã®æ…é‡ãªãƒã‚¸ã‚·ãƒ§ãƒ³ç®¡ç†å¿…è¦\")\n",
    "    \n",
    "    # æœ€ã‚‚ãƒªã‚¹ã‚­ãƒ¼ãªã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚’ç‰¹å®š\n",
    "    if risk_metrics:\n",
    "        highest_var_spread = max(risk_metrics.keys(), \n",
    "                               key=lambda x: abs(risk_metrics[x]['var']['95.0%']))\n",
    "        highest_var = abs(risk_metrics[highest_var_spread]['var']['95.0%'])\n",
    "        print(f\"  â€¢ æœ€é«˜ãƒªã‚¹ã‚¯ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰: {highest_var_spread} (95% VaR: {highest_var:.3f}%)\")\n",
    "    \n",
    "    # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æã«åŸºã¥ãç¤ºå”†\n",
    "    if regime_analysis:\n",
    "        unstable_spreads = []\n",
    "        for spread, data in regime_analysis.items():\n",
    "            if data['regime_changes'] > len(data['conditional_volatility']) / 50:  # é »ç¹ãªè»¢æ›\n",
    "                unstable_spreads.append(spread)\n",
    "        \n",
    "        if unstable_spreads:\n",
    "            print(f\"  â€¢ ãƒ¬ã‚¸ãƒ¼ãƒ ä¸å®‰å®š: {', '.join(unstable_spreads)} â†’ å‹•çš„ãƒ˜ãƒƒã‚¸æˆ¦ç•¥æ¨å¥¨\")\n",
    "    \n",
    "    print(f\"\\nğŸ›¡ï¸ ãƒªã‚¹ã‚¯ç®¡ç†æ¨å¥¨äº‹é …:\")\n",
    "    print(f\"  â€¢ GARCHãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®å‹•çš„VaRç®¡ç†\")\n",
    "    print(f\"  â€¢ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ å¿œç­”å‹ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚¸ãƒ³ã‚°\")\n",
    "    print(f\"  â€¢ é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“ã§ã®æåˆ‡ã‚ŠåŸºæº–å³æ ¼åŒ–\")\n",
    "    print(f\"  â€¢ Expected Shortfallã«ã‚ˆã‚‹æ¥µç«¯ãƒªã‚¹ã‚¯ã‚·ãƒŠãƒªã‚ªç®¡ç†\")\n",
    "    \n",
    "    return {\n",
    "        'arch_effects_detected': arch_detected,\n",
    "        'volatility_clustering_detected': vol_clustering_detected,\n",
    "        'best_models': best_models,\n",
    "        'highest_risk_spread': highest_var_spread if risk_metrics else None,\n",
    "        'regime_instability': unstable_spreads if regime_analysis else []\n",
    "    }\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼ç”Ÿæˆ\n",
    "volatility_summary = generate_volatility_modeling_summary(\n",
    "    vol_stats, arch_results, garch_models, risk_metrics, regime_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçµæœã®ä¿å­˜\n",
    "def save_volatility_analysis_results(vol_stats, garch_models, risk_metrics, \n",
    "                                    regime_analysis, volatility_summary):\n",
    "    \"\"\"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\"\"\"\n",
    "    \n",
    "    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    os.makedirs('../analysis_results/adjacent_spreads', exist_ok=True)\n",
    "    \n",
    "    # 1. åŸºæœ¬ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çµ±è¨ˆ\n",
    "    vol_stats.to_csv('../analysis_results/adjacent_spreads/volatility_statistics.csv', \n",
    "                     encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. GARCHæ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "    garch_volatilities = pd.DataFrame()\n",
    "    for spread, models in garch_models.items():\n",
    "        best_model = models['best_model']\n",
    "        garch_volatilities[f'{spread}_volatility'] = best_model['conditional_volatility']\n",
    "    \n",
    "    if not garch_volatilities.empty:\n",
    "        garch_volatilities.to_csv('../analysis_results/adjacent_spreads/garch_conditional_volatility.csv', \n",
    "                                 encoding='utf-8-sig')\n",
    "    \n",
    "    # 3. VaRã¨ãƒªã‚¹ã‚¯æŒ‡æ¨™\n",
    "    risk_summary = pd.DataFrame()\n",
    "    for spread, metrics in risk_metrics.items():\n",
    "        risk_summary[f'{spread}_current_vol'] = [metrics['current_volatility']]\n",
    "        risk_summary[f'{spread}_95_var'] = [abs(metrics['var']['95.0%'])]\n",
    "        risk_summary[f'{spread}_95_es'] = [abs(metrics['expected_shortfall']['95.0%'])]\n",
    "        risk_summary[f'{spread}_max_drawdown'] = [metrics['max_drawdown']]\n",
    "    \n",
    "    if not risk_summary.empty:\n",
    "        risk_summary.to_csv('../analysis_results/adjacent_spreads/risk_metrics_summary.csv', \n",
    "                           encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # 4. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ \n",
    "    regime_data = pd.DataFrame()\n",
    "    for spread, analysis in regime_analysis.items():\n",
    "        regime_data[f'{spread}_regime'] = analysis['regimes']\n",
    "        regime_data[f'{spread}_volatility'] = analysis['conditional_volatility']\n",
    "    \n",
    "    if not regime_data.empty:\n",
    "        regime_data.to_csv('../analysis_results/adjacent_spreads/volatility_regimes.csv', \n",
    "                          encoding='utf-8-sig')\n",
    "    \n",
    "    # 5. åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆJSONï¼‰\n",
    "    import json\n",
    "    \n",
    "    with open('../analysis_results/adjacent_spreads/volatility_modeling_summary.json', \n",
    "              'w', encoding='utf-8') as f:\n",
    "        json.dump(volatility_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
    "    print(f\"  ğŸ“Š åŸºæœ¬çµ±è¨ˆ: ../analysis_results/adjacent_spreads/volatility_statistics.csv\")\n",
    "    print(f\"  ğŸ“ˆ GARCHæ¡ä»¶ä»˜ããƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: ../analysis_results/adjacent_spreads/garch_conditional_volatility.csv\")\n",
    "    print(f\"  âš ï¸ ãƒªã‚¹ã‚¯æŒ‡æ¨™: ../analysis_results/adjacent_spreads/risk_metrics_summary.csv\")\n",
    "    print(f\"  ğŸ­ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ : ../analysis_results/adjacent_spreads/volatility_regimes.csv\")\n",
    "    print(f\"  ğŸ“ åˆ†æã‚µãƒãƒªãƒ¼: ../analysis_results/adjacent_spreads/volatility_modeling_summary.json\")\n",
    "\n",
    "# åˆ†æçµæœä¿å­˜\n",
    "save_volatility_analysis_results(\n",
    "    vol_stats, garch_models, risk_metrics, regime_analysis, volatility_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n\nã“ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°åˆ†æã«ã‚ˆã‚Šã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã®ãƒªã‚¹ã‚¯ç‰¹æ€§ã«ã¤ã„ã¦è©³ç´°ãªæ´å¯Ÿã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚\n\n### ä¸»è¦ãªç™ºè¦‹äº‹é …\n1. **ARCHåŠ¹æœ**: è¤‡æ•°ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã§æ¡ä»¶ä»˜ãåˆ†æ•£ä¸å‡ä¸€æ€§ã‚’ç¢ºèª\n2. **ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°**: é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“ã¨ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“ã®æ˜ç¢ºãªåŒºåˆ†\n3. **GARCHãƒ¢ãƒ‡ãƒ«**: å„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã«æœ€é©ãªãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š\n4. **ãƒªã‚¹ã‚¯æŒ‡æ¨™**: VaR/ESã«ã‚ˆã‚‹å®šé‡çš„ãƒªã‚¹ã‚¯è©•ä¾¡\n5. **ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ**: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç’°å¢ƒã®å‘¨æœŸçš„å¤‰åŒ–\n\n### ä»Šå¾Œã®åˆ†æã‚¹ãƒ†ãƒƒãƒ—\n1. **æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬**: é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«\n2. **ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥é–‹ç™º**: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®å–å¼•æˆ¦ç•¥\n3. **ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ˜ãƒƒã‚¸**: ãƒ¬ã‚¸ãƒ¼ãƒ å¿œç­”å‹ãƒªã‚¹ã‚¯ç®¡ç†\n4. **ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ**: å–å¼•æˆ¦ç•¥ã®æ­´å²çš„æ¤œè¨¼\n5. **ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–**: ãƒªã‚¹ã‚¯èª¿æ•´å¾Œãƒªã‚¿ãƒ¼ãƒ³ã®æœ€å¤§åŒ–\n\næ¬¡å›ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ `4_adjacent_spreads_trading_strategies.ipynb` ã§ã¯ã€å…·ä½“çš„ãªå–å¼•æˆ¦ç•¥ã¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}