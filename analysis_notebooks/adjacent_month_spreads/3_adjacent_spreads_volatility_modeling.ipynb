{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LME銅先物隣月間スプレッド ボラティリティモデリング\n\n## 分析概要\nこのノートブックでは、隣月間スプレッド（M1-M2, M2-M3, M3-M4）のボラティリティ特性を詳細に分析し、GARCH系モデルによるリスク特性のモデリングを行います。\n\n### 分析目標\n- ボラティリティクラスタリングの検出\n- GARCHモデルによるボラティリティ予測\n- リスク指標（VaR、ES）の算出\n- ボラティリティレジームの識別\n\n### 期待される成果\n- 各スプレッドのボラティリティ動向の理解\n- 動的リスク管理のためのモデル構築\n- ボラティリティベース取引戦略の基盤\n- ストレステスト用リスクシナリオ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psycopg2\nfrom sqlalchemy import create_engine\nimport warnings\nfrom datetime import datetime, timedelta\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nimport os\n\n# Volatility modeling\nfrom arch import arch_model\nfrom arch.unitroot import ADF\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.preprocessing import StandardScaler\n\n# Risk calculation\nfrom scipy.stats import norm, t\nfrom scipy.optimize import minimize\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Data retrieval and preprocessing\ndef get_db_connection():\n    \"\"\"Get PostgreSQL database connection\"\"\"\n    try:\n        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n        return engine\n    except Exception as e:\n        print(f\"Database connection error: {e}\")\n        return None\n\ndef load_and_prepare_data():\n    \"\"\"Retrieve spread data and calculate daily returns\"\"\"\n    engine = get_db_connection()\n    \n    query = \"\"\"\n    SELECT \n        trade_date,\n        contract_month,\n        close_price\n    FROM lme_copper_futures \n    WHERE contract_month IN (1, 2, 3, 4)\n        AND close_price IS NOT NULL\n        AND close_price > 0\n    ORDER BY trade_date, contract_month\n    \"\"\"\n    \n    df = pd.read_sql(query, engine)\n    df['trade_date'] = pd.to_datetime(df['trade_date'])\n    \n    # Pivot and calculate spreads\n    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n    \n    # Calculate spreads\n    spreads_df = pd.DataFrame(index=pivot_df.index)\n    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n    \n    # Calculate daily returns\n    spreads_df['M1_M2_return'] = spreads_df['M1_M2_spread'].pct_change()\n    spreads_df['M2_M3_return'] = spreads_df['M2_M3_spread'].pct_change()\n    spreads_df['M3_M4_return'] = spreads_df['M3_M4_spread'].pct_change()\n    \n    # Also calculate log differences (log returns)\n    spreads_df['M1_M2_log_return'] = np.log(spreads_df['M1_M2_spread'].abs()).diff()\n    spreads_df['M2_M3_log_return'] = np.log(spreads_df['M2_M3_spread'].abs()).diff()\n    spreads_df['M3_M4_log_return'] = np.log(spreads_df['M3_M4_spread'].abs()).diff()\n    \n    return spreads_df.dropna()\n\n# Retrieve data\nspreads_data = load_and_prepare_data()\nprint(f\"✅ Data retrieval complete: {len(spreads_data):,} records\")\nprint(f\"📅 Analysis period: {spreads_data.index.min()} to {spreads_data.index.max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. 基本ボラティリティ分析",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_basic_volatility(df):\n    \"\"\"Basic volatility analysis\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    \n    print(\"📊 Basic Volatility Statistics:\")\n    print(\"=\" * 50)\n    \n    volatility_stats = pd.DataFrame()\n    \n    for col in return_columns:\n        spread_name = col.replace('_return', '').replace('_', '-')\n        returns = df[col].dropna()\n        \n        # Basic statistics\n        volatility_stats[spread_name] = [\n            returns.std() * 100,                    # Daily volatility (%)\n            returns.std() * np.sqrt(252) * 100,     # Annualized volatility (%)\n            returns.skew(),                         # Skewness\n            returns.kurtosis(),                     # Kurtosis (excess kurtosis)\n            (returns.abs() > 2*returns.std()).sum(), # Number of 2σ exceedances\n            (returns.abs() > 3*returns.std()).sum(), # Number of 3σ exceedances\n            len(returns)                            # Valid data count\n        ]\n    \n    volatility_stats.index = [\n        'Daily Volatility (%)', 'Annualized Volatility (%)', 'Skewness', 'Kurtosis',\n        '2σ Exceedances', '3σ Exceedances', 'Data Count'\n    ]\n    \n    print(volatility_stats.round(4))\n    \n    # Normality tests\n    print(\"\\n🔬 Normality Test Results:\")\n    print(\"-\" * 40)\n    \n    for col in return_columns:\n        spread_name = col.replace('_return', '').replace('_', '-')\n        returns = df[col].dropna()\n        \n        # Jarque-Bera test\n        jb_stat, jb_p = stats.jarque_bera(returns)\n        \n        # Shapiro-Wilk test (with sample size limit)\n        if len(returns) <= 5000:\n            sw_stat, sw_p = stats.shapiro(returns)\n        else:\n            sw_stat, sw_p = stats.shapiro(returns.sample(5000, random_state=42))\n        \n        print(f\"{spread_name}:\")\n        print(f\"  Jarque-Bera: Statistic={jb_stat:.4f}, p-value={jb_p:.4f}\")\n        print(f\"  Shapiro-Wilk: Statistic={sw_stat:.4f}, p-value={sw_p:.4f}\")\n        print(f\"  Result: {'Non-normal distribution' if jb_p < 0.05 else 'Possibly normal distribution'}\")\n    \n    return volatility_stats\n\n# Execute basic volatility analysis\nvol_stats = analyze_basic_volatility(spreads_data)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_volatility_analysis(df):\n    \"\"\"ボラティリティ分析の可視化\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=(\n            'スプレッドリターン時系列',\n            'リターン分布（ヒストグラム）',\n            'ローリングボラティリティ（30日）',\n            'Q-Qプロット（対正規分布）',\n            '絶対リターン自己相関',\n            'ボラティリティクラスタリング'\n        ),\n        vertical_spacing=0.08\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    # 1. Return time series\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        fig.add_trace(\n            go.Scatter(\n                x=df.index,\n                y=df[col] * 100,  # Convert to percentage\n                name=f'{name} リターン',\n                line=dict(color=color, width=0.8),\n                opacity=0.7\n            ),\n            row=1, col=1\n        )\n    \n    # 2. Return distribution\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        fig.add_trace(\n            go.Histogram(\n                x=df[col] * 100,\n                name=f'{name} 分布',\n                nbinsx=50,\n                opacity=0.7,\n                marker_color=color\n            ),\n            row=1, col=2\n        )\n    \n    # 3. Rolling volatility\n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        rolling_vol = df[col].rolling(window=30).std() * np.sqrt(252) * 100\n        fig.add_trace(\n            go.Scatter(\n                x=df.index,\n                y=rolling_vol,\n                name=f'{name} ボラティリティ',\n                line=dict(color=color, width=1.5)\n            ),\n            row=2, col=1\n        )\n    \n    # 4. Q-Q plot (M1-M2 only)\n    from scipy.stats import probplot\n    returns_clean = df['M1_M2_return'].dropna()\n    theoretical_q, sample_q = probplot(returns_clean, dist=\"norm\")\n    \n    fig.add_trace(\n        go.Scatter(\n            x=theoretical_q,\n            y=sample_q,\n            mode='markers',\n            name='M1-M2 Q-Q',\n            marker=dict(color='blue', size=3)\n        ),\n        row=2, col=2\n    )\n    \n    # Theoretical line\n    fig.add_trace(\n        go.Scatter(\n            x=[theoretical_q.min(), theoretical_q.max()],\n            y=[sample_q.min(), sample_q.max()],\n            mode='lines',\n            name='理論線',\n            line=dict(color='red', dash='dash')\n        ),\n        row=2, col=2\n    )\n    \n    # 5. Absolute return autocorrelation (volatility clustering detection)\n    abs_returns = df['M1_M2_return'].abs().dropna()\n    lags = range(1, 21)\n    autocorrs = [abs_returns.autocorr(lag=lag) for lag in lags]\n    \n    fig.add_trace(\n        go.Bar(\n            x=list(lags),\n            y=autocorrs,\n            name='M1-M2 絶対リターン自己相関',\n            marker_color='lightblue'\n        ),\n        row=3, col=1\n    )\n    \n    # Significance threshold lines (5% level)\n    n = len(abs_returns)\n    threshold = 1.96 / np.sqrt(n)\n    fig.add_hline(y=threshold, line_dash=\"dash\", line_color=\"red\", \n                 line_width=1, row=3, col=1)\n    fig.add_hline(y=-threshold, line_dash=\"dash\", line_color=\"red\", \n                 line_width=1, row=3, col=1)\n    \n    # 6. Volatility clustering visualization\n    squared_returns = (df['M1_M2_return'] ** 2).dropna()\n    fig.add_trace(\n        go.Scatter(\n            x=df.index[:len(squared_returns)],\n            y=squared_returns * 10000,  # basis points\n            name='M1-M2 二乗リターン',\n            line=dict(color='purple', width=1),\n            fill='tonexty',\n            fillcolor='rgba(128,0,128,0.3)'\n        ),\n        row=3, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"隣月間スプレッド ボラティリティ分析\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=1000,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"リターン (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"頻度\", row=1, col=2)\n    fig.update_xaxes(title_text=\"リターン (%)\", row=1, col=2)\n    \n    fig.update_yaxes(title_text=\"年率ボラティリティ (%)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"標本分位数\", row=2, col=2)\n    fig.update_xaxes(title_text=\"理論分位数\", row=2, col=2)\n    \n    fig.update_yaxes(title_text=\"自己相関\", row=3, col=1)\n    fig.update_xaxes(title_text=\"ラグ\", row=3, col=1)\n    \n    fig.update_yaxes(title_text=\"二乗リターン (bp)\", row=3, col=2)\n    fig.update_xaxes(title_text=\"日付\", row=3, col=2)\n    \n    return fig\n\n# Volatility analysis chart\nvol_chart = plot_volatility_analysis(spreads_data)\nvol_chart.show()\n\n# Save image\nos.makedirs('../generated_images', exist_ok=True)\nvol_chart.write_image('../generated_images/adjacent_spreads_volatility_analysis.png', \n                     width=1200, height=1000, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. ARCH効果検定",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_arch_effects(df):\n",
    "    \"\"\"ARCH効果（条件付き分散不均一性）の検定\"\"\"\n",
    "    from arch.unitroot import DFGLS\n",
    "    from statsmodels.stats.diagnostic import het_arch\n",
    "    \n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"🔬 ARCH効果検定結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    arch_test_results = {}\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        returns = df[col].dropna() * 100  # パーセント変換\n",
    "        \n",
    "        print(f\"\\n{name}スプレッド:\")\n",
    "        \n",
    "        # 1. 基本統計量\n",
    "        print(f\"  データ数: {len(returns):,}\")\n",
    "        print(f\"  平均: {returns.mean():.4f}%\")\n",
    "        print(f\"  標準偏差: {returns.std():.4f}%\")\n",
    "        \n",
    "        # 2. 定常性検定（ADF）\n",
    "        adf_result = adfuller(returns)\n",
    "        print(f\"  ADF検定: 統計量={adf_result[0]:.4f}, p値={adf_result[1]:.4f}\")\n",
    "        print(f\"    結果: {'定常' if adf_result[1] < 0.05 else '非定常'}\")\n",
    "        \n",
    "        # 3. Ljung-Box検定（系列相関）\n",
    "        try:\n",
    "            lb_result = acorr_ljungbox(returns, lags=10, return_df=True)\n",
    "            lb_pvalue = lb_result['lb_pvalue'].iloc[-1]  # 10ラグの結果\n",
    "            print(f\"  Ljung-Box検定(10ラグ): p値={lb_pvalue:.4f}\")\n",
    "            print(f\"    結果: {'系列相関あり' if lb_pvalue < 0.05 else '系列相関なし'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Ljung-Box検定: エラー - {e}\")\n",
    "            lb_pvalue = np.nan\n",
    "        \n",
    "        # 4. ARCH-LM検定（条件付き分散不均一性）\n",
    "        try:\n",
    "            # 複数のラグで検定\n",
    "            for lag in [5, 10, 15]:\n",
    "                lm_stat, lm_pvalue, _, _ = het_arch(returns, nlags=lag)\n",
    "                print(f\"  ARCH-LM検定({lag}ラグ): 統計量={lm_stat:.4f}, p値={lm_pvalue:.4f}\")\n",
    "                if lag == 10:  # 10ラグの結果を保存\n",
    "                    main_arch_pvalue = lm_pvalue\n",
    "            \n",
    "            print(f\"    結果: {'ARCH効果あり' if main_arch_pvalue < 0.05 else 'ARCH効果なし'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ARCH-LM検定: エラー - {e}\")\n",
    "            main_arch_pvalue = np.nan\n",
    "        \n",
    "        # 5. 絶対リターンの自己相関検定\n",
    "        abs_returns = returns.abs()\n",
    "        try:\n",
    "            abs_lb_result = acorr_ljungbox(abs_returns, lags=10, return_df=True)\n",
    "            abs_lb_pvalue = abs_lb_result['lb_pvalue'].iloc[-1]\n",
    "            print(f\"  絶対リターンLjung-Box検定: p値={abs_lb_pvalue:.4f}\")\n",
    "            print(f\"    結果: {'ボラティリティクラスタリングあり' if abs_lb_pvalue < 0.05 else 'クラスタリングなし'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  絶対リターンLjung-Box検定: エラー - {e}\")\n",
    "            abs_lb_pvalue = np.nan\n",
    "        \n",
    "        # 結果を保存\n",
    "        arch_test_results[name] = {\n",
    "            'data_count': len(returns),\n",
    "            'mean': returns.mean(),\n",
    "            'std': returns.std(),\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'is_stationary': adf_result[1] < 0.05,\n",
    "            'ljungbox_pvalue': lb_pvalue,\n",
    "            'has_serial_correlation': lb_pvalue < 0.05 if not np.isnan(lb_pvalue) else None,\n",
    "            'arch_lm_pvalue': main_arch_pvalue,\n",
    "            'has_arch_effects': main_arch_pvalue < 0.05 if not np.isnan(main_arch_pvalue) else None,\n",
    "            'abs_ljungbox_pvalue': abs_lb_pvalue,\n",
    "            'has_volatility_clustering': abs_lb_pvalue < 0.05 if not np.isnan(abs_lb_pvalue) else None\n",
    "        }\n",
    "    \n",
    "    return arch_test_results\n",
    "\n",
    "# ARCH効果検定実行\n",
    "arch_results = test_arch_effects(spreads_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. GARCHモデル推定",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_garch_models(df):\n",
    "    \"\"\"GARCH系モデルの推定\"\"\"\n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    garch_models = {}\n",
    "    \n",
    "    print(\"📈 GARCHモデル推定結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        returns = df[col].dropna() * 100  # パーセント変換\n",
    "        \n",
    "        print(f\"\\n{name}スプレッド:\")\n",
    "        \n",
    "        models_to_estimate = {\n",
    "            'GARCH(1,1)': arch_model(returns, vol='Garch', p=1, q=1),\n",
    "            'EGARCH(1,1)': arch_model(returns, vol='EGARCH', p=1, q=1),\n",
    "            'GJR-GARCH(1,1)': arch_model(returns, vol='GARCH', p=1, o=1, q=1)\n",
    "        }\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        for model_name, model in models_to_estimate.items():\n",
    "            try:\n",
    "                # モデル推定\n",
    "                result = model.fit(disp='off', show_warning=False)\n",
    "                \n",
    "                # AIC/BIC計算\n",
    "                aic = result.aic\n",
    "                bic = result.bic\n",
    "                log_likelihood = result.loglikelihood\n",
    "                \n",
    "                print(f\"  {model_name}:\")\n",
    "                print(f\"    対数尤度: {log_likelihood:.4f}\")\n",
    "                print(f\"    AIC: {aic:.4f}\")\n",
    "                print(f\"    BIC: {bic:.4f}\")\n",
    "                \n",
    "                # 条件付きボラティリティ\n",
    "                conditional_volatility = result.conditional_volatility\n",
    "                \n",
    "                model_results[model_name] = {\n",
    "                    'model': result,\n",
    "                    'aic': aic,\n",
    "                    'bic': bic,\n",
    "                    'log_likelihood': log_likelihood,\n",
    "                    'conditional_volatility': conditional_volatility,\n",
    "                    'residuals': result.resid,\n",
    "                    'standardized_residuals': result.std_resid\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name}: 推定エラー - {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 最適モデル選択（AIC基準）\n",
    "        if model_results:\n",
    "            best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['aic'])\n",
    "            print(f\"  \\n  📊 最適モデル（AIC基準）: {best_model_name}\")\n",
    "            \n",
    "            garch_models[name] = {\n",
    "                'all_models': model_results,\n",
    "                'best_model_name': best_model_name,\n",
    "                'best_model': model_results[best_model_name]\n",
    "            }\n",
    "    \n",
    "    return garch_models\n",
    "\n",
    "# GARCHモデル推定\n",
    "garch_models = estimate_garch_models(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_garch_results(garch_models, spreads_data):\n    \"\"\"GARCHモデル結果の可視化\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=(\n            'M1-M2: リターンと条件付きボラティリティ',\n            'M1-M2: 標準化残差',\n            'M2-M3: リターンと条件付きボラティリティ',\n            'M2-M3: 標準化残差',\n            'M3-M4: リターンと条件付きボラティリティ',\n            'M3-M4: 標準化残差'\n        ),\n        vertical_spacing=0.08\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    for i, (col, name, color) in enumerate(zip(return_columns, spread_names, colors)):\n        if name in garch_models:\n            best_model = garch_models[name]['best_model']\n            \n            returns = spreads_data[col].dropna() * 100\n            cond_vol = best_model['conditional_volatility']\n            std_resid = best_model['standardized_residuals']\n            \n            # Returns and conditional volatility\n            fig.add_trace(\n                go.Scatter(\n                    x=returns.index,\n                    y=returns,\n                    name=f'{name} リターン',\n                    line=dict(color=color, width=0.8),\n                    opacity=0.7\n                ),\n                row=i+1, col=1\n            )\n            \n            # ±2σ bands of conditional volatility\n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=2 * cond_vol,\n                    name=f'{name} +2σ',\n                    line=dict(color='red', dash='dash', width=1),\n                    showlegend=False\n                ),\n                row=i+1, col=1\n            )\n            \n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=-2 * cond_vol,\n                    name=f'{name} -2σ',\n                    line=dict(color='red', dash='dash', width=1),\n                    fill='tonexty',\n                    fillcolor='rgba(255,0,0,0.1)',\n                    showlegend=False\n                ),\n                row=i+1, col=1\n            )\n            \n            # Standardized residuals\n            fig.add_trace(\n                go.Scatter(\n                    x=std_resid.index,\n                    y=std_resid,\n                    name=f'{name} 標準化残差',\n                    line=dict(color='purple', width=0.8),\n                    mode='markers',\n                    marker=dict(size=2)\n                ),\n                row=i+1, col=2\n            )\n            \n            # ±2σ lines\n            fig.add_hline(y=2, line_dash=\"dash\", line_color=\"red\", \n                         line_width=1, row=i+1, col=2)\n            fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"red\", \n                         line_width=1, row=i+1, col=2)\n            fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n                         line_width=1, row=i+1, col=2)\n    \n    fig.update_layout(\n        title=dict(\n            text=\"GARCHモデル推定結果\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=1000,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    for i in range(3):\n        fig.update_yaxes(title_text=\"リターン (%)\", row=i+1, col=1)\n        fig.update_yaxes(title_text=\"標準化残差\", row=i+1, col=2)\n        if i == 2:\n            fig.update_xaxes(title_text=\"日付\", row=i+1, col=1)\n            fig.update_xaxes(title_text=\"日付\", row=i+1, col=2)\n    \n    return fig\n\n# Visualization of GARCH model results\ngarch_chart = plot_garch_results(garch_models, spreads_data)\ngarch_chart.show()\n\n# Save image\ngarch_chart.write_image('../generated_images/adjacent_spreads_garch_models.png', \n                       width=1200, height=1000, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. リスク指標計算",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_metrics(garch_models, spreads_data):\n",
    "    \"\"\"VaR、Expected Shortfall等のリスク指標計算\"\"\"\n",
    "    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    confidence_levels = [0.95, 0.99, 0.995]\n",
    "    \n",
    "    print(\"⚠️ リスク指標計算結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    risk_metrics = {}\n",
    "    \n",
    "    for col, name in zip(return_columns, spread_names):\n",
    "        if name not in garch_models:\n",
    "            continue\n",
    "            \n",
    "        returns = spreads_data[col].dropna() * 100\n",
    "        best_model = garch_models[name]['best_model']\n",
    "        cond_vol = best_model['conditional_volatility']\n",
    "        std_resid = best_model['standardized_residuals']\n",
    "        \n",
    "        print(f\"\\n{name}スプレッド:\")\n",
    "        \n",
    "        current_volatility = cond_vol.iloc[-1]  # 最新の条件付きボラティリティ\n",
    "        print(f\"  最新条件付きボラティリティ: {current_volatility:.4f}%\")\n",
    "        \n",
    "        spread_risk_metrics = {\n",
    "            'current_volatility': current_volatility,\n",
    "            'conditional_volatility': cond_vol,\n",
    "            'var': {},\n",
    "            'expected_shortfall': {},\n",
    "            'parametric_var': {},\n",
    "            'historical_var': {}\n",
    "        }\n",
    "        \n",
    "        # 各信頼水準でリスク指標を計算\n",
    "        for alpha in confidence_levels:\n",
    "            print(f\"\\n  信頼水準 {alpha*100:.1f}%:\")\n",
    "            \n",
    "            # 1. ヒストリカルVaR\n",
    "            historical_var = np.percentile(returns, (1-alpha)*100)\n",
    "            \n",
    "            # 2. パラメトリックVaR（正規分布仮定）\n",
    "            parametric_var = norm.ppf(1-alpha) * current_volatility\n",
    "            \n",
    "            # 3. GARCH-VaR（t分布仮定）\n",
    "            # 標準化残差の分布をt分布でフィット\n",
    "            try:\n",
    "                clean_std_resid = std_resid.dropna()\n",
    "                df_fitted, loc_fitted, scale_fitted = stats.t.fit(clean_std_resid)\n",
    "                garch_var = t.ppf(1-alpha, df=df_fitted, loc=loc_fitted, scale=scale_fitted) * current_volatility\n",
    "            except:\n",
    "                garch_var = parametric_var  # フィットに失敗した場合は正規分布を使用\n",
    "            \n",
    "            # 4. Expected Shortfall (CVaR)\n",
    "            var_threshold = historical_var\n",
    "            tail_returns = returns[returns <= var_threshold]\n",
    "            if len(tail_returns) > 0:\n",
    "                expected_shortfall = tail_returns.mean()\n",
    "            else:\n",
    "                expected_shortfall = historical_var\n",
    "            \n",
    "            print(f\"    ヒストリカルVaR: {historical_var:.4f}%\")\n",
    "            print(f\"    パラメトリックVaR: {parametric_var:.4f}%\")\n",
    "            print(f\"    GARCH-VaR: {garch_var:.4f}%\")\n",
    "            print(f\"    Expected Shortfall: {expected_shortfall:.4f}%\")\n",
    "            \n",
    "            # 結果を保存\n",
    "            alpha_key = f'{alpha*100:.1f}%'\n",
    "            spread_risk_metrics['historical_var'][alpha_key] = historical_var\n",
    "            spread_risk_metrics['parametric_var'][alpha_key] = parametric_var\n",
    "            spread_risk_metrics['var'][alpha_key] = garch_var\n",
    "            spread_risk_metrics['expected_shortfall'][alpha_key] = expected_shortfall\n",
    "        \n",
    "        # 追加のリスク指標\n",
    "        max_drawdown = calculate_max_drawdown(returns)\n",
    "        avg_vol_30d = cond_vol.rolling(window=30).mean().iloc[-1]\n",
    "        vol_of_vol = cond_vol.rolling(window=30).std().iloc[-1]\n",
    "        \n",
    "        print(f\"\\n  追加リスク指標:\")\n",
    "        print(f\"    最大ドローダウン: {max_drawdown:.4f}%\")\n",
    "        print(f\"    30日平均ボラティリティ: {avg_vol_30d:.4f}%\")\n",
    "        print(f\"    ボラティリティのボラティリティ: {vol_of_vol:.4f}%\")\n",
    "        \n",
    "        spread_risk_metrics.update({\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'avg_volatility_30d': avg_vol_30d,\n",
    "            'volatility_of_volatility': vol_of_vol\n",
    "        })\n",
    "        \n",
    "        risk_metrics[name] = spread_risk_metrics\n",
    "    \n",
    "    return risk_metrics\n",
    "\n",
    "def calculate_max_drawdown(returns):\n",
    "    \"\"\"最大ドローダウンの計算\"\"\"\n",
    "    cumulative = (1 + returns/100).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max * 100\n",
    "    return drawdown.min()\n",
    "\n",
    "# リスク指標計算\n",
    "risk_metrics = calculate_risk_metrics(garch_models, spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_risk_metrics(risk_metrics, spreads_data):\n    \"\"\"リスク指標の可視化\"\"\"\n    return_columns = ['M1_M2_return', 'M2_M3_return', 'M3_M4_return']\n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=(\n            'VaR比較（95%信頼水準）',\n            '条件付きボラティリティ推移',\n            'VaRバックテスト（ヒストリカルVaR vs 実績）',\n            'リスク指標サマリー'\n        )\n    )\n    \n    colors = ['blue', 'red', 'green']\n    \n    # 1. VaR comparison\n    var_types = ['ヒストリカルVaR', 'パラメトリックVaR', 'GARCH-VaR']\n    var_keys = ['historical_var', 'parametric_var', 'var']\n    \n    x_pos = np.arange(len(spread_names))\n    bar_width = 0.25\n    \n    for i, (var_type, var_key) in enumerate(zip(var_types, var_keys)):\n        var_values = []\n        for name in spread_names:\n            if name in risk_metrics:\n                var_values.append(abs(risk_metrics[name][var_key]['95.0%']))\n            else:\n                var_values.append(0)\n        \n        fig.add_trace(\n            go.Bar(\n                x=[f\"{name}<br>({var_type})\" for name in spread_names],\n                y=var_values,\n                name=var_type,\n                marker_color=colors[i],\n                opacity=0.8\n            ),\n            row=1, col=1\n        )\n    \n    # 2. Conditional volatility evolution\n    for i, (name, color) in enumerate(zip(spread_names, colors)):\n        if name in risk_metrics:\n            cond_vol = risk_metrics[name]['conditional_volatility']\n            fig.add_trace(\n                go.Scatter(\n                    x=cond_vol.index,\n                    y=cond_vol,\n                    name=f'{name} 条件付きボラティリティ',\n                    line=dict(color=color, width=1.5)\n                ),\n                row=1, col=2\n            )\n    \n    # 3. VaR backtest (M1-M2 only)\n    if 'M1-M2' in risk_metrics:\n        returns = spreads_data['M1_M2_return'].dropna() * 100\n        historical_var_95 = risk_metrics['M1-M2']['historical_var']['95.0%']\n        \n        fig.add_trace(\n            go.Scatter(\n                x=returns.index,\n                y=returns,\n                name='M1-M2 リターン',\n                line=dict(color='blue', width=0.8),\n                opacity=0.7\n            ),\n            row=2, col=1\n        )\n        \n        # VaR line\n        fig.add_hline(y=historical_var_95, line_dash=\"dash\", line_color=\"red\", \n                     line_width=2, row=2, col=1)\n        \n        # Highlight VaR violations\n        var_violations = returns[returns < historical_var_95]\n        if len(var_violations) > 0:\n            fig.add_trace(\n                go.Scatter(\n                    x=var_violations.index,\n                    y=var_violations,\n                    mode='markers',\n                    name='VaR違反',\n                    marker=dict(color='red', size=6, symbol='x')\n                ),\n                row=2, col=1\n            )\n        \n        # VaR violation rate\n        violation_rate = len(var_violations) / len(returns) * 100\n        expected_rate = 5.0  # Expected violation rate for 95% VaR\n        \n    # 4. Risk metrics summary (table)\n    summary_data = []\n    for name in spread_names:\n        if name in risk_metrics:\n            metrics = risk_metrics[name]\n            summary_data.append([\n                name,\n                f\"{metrics['current_volatility']:.3f}%\",\n                f\"{abs(metrics['var']['95.0%']):.3f}%\",\n                f\"{abs(metrics['expected_shortfall']['95.0%']):.3f}%\",\n                f\"{metrics['max_drawdown']:.3f}%\"\n            ])\n    \n    fig.add_trace(\n        go.Table(\n            header=dict(\n                values=['スプレッド', '現在ボラティリティ', '95% VaR', '95% ES', '最大DD'],\n                fill_color='lightblue',\n                font=dict(size=12)\n            ),\n            cells=dict(\n                values=list(zip(*summary_data)) if summary_data else [[], [], [], [], []],\n                fill_color='white',\n                font=dict(size=11)\n            )\n        ),\n        row=2, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"リスク指標分析\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=800,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"VaR (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"ボラティリティ (%)\", row=1, col=2)\n    fig.update_xaxes(title_text=\"日付\", row=1, col=2)\n    \n    fig.update_yaxes(title_text=\"リターン (%)\", row=2, col=1)\n    fig.update_xaxes(title_text=\"日付\", row=2, col=1)\n    \n    return fig\n\n# Visualization of risk metrics\nrisk_chart = plot_risk_metrics(risk_metrics, spreads_data)\nrisk_chart.show()\n\n# Save image\nrisk_chart.write_image('../generated_images/adjacent_spreads_risk_metrics.png', \n                      width=1200, height=800, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. ボラティリティレジーム分析",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_volatility_regimes(risk_metrics, spreads_data):\n",
    "    \"\"\"ボラティリティレジーム分析\"\"\"\n",
    "    print(\"🎭 ボラティリティレジーム分析:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    regime_analysis = {}\n",
    "    \n",
    "    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if name not in risk_metrics:\n",
    "            continue\n",
    "            \n",
    "        cond_vol = risk_metrics[name]['conditional_volatility']\n",
    "        \n",
    "        print(f\"\\n{name}スプレッド:\")\n",
    "        \n",
    "        # ボラティリティの分位数でレジーム分類\n",
    "        vol_25 = cond_vol.quantile(0.25)\n",
    "        vol_50 = cond_vol.quantile(0.50)\n",
    "        vol_75 = cond_vol.quantile(0.75)\n",
    "        vol_95 = cond_vol.quantile(0.95)\n",
    "        \n",
    "        print(f\"  ボラティリティ分位数:\")\n",
    "        print(f\"    25%分位: {vol_25:.4f}%\")\n",
    "        print(f\"    50%分位: {vol_50:.4f}%\")\n",
    "        print(f\"    75%分位: {vol_75:.4f}%\")\n",
    "        print(f\"    95%分位: {vol_95:.4f}%\")\n",
    "        \n",
    "        # レジーム分類\n",
    "        regimes = pd.Series(index=cond_vol.index, dtype='object')\n",
    "        regimes[cond_vol <= vol_25] = 'Low'\n",
    "        regimes[(cond_vol > vol_25) & (cond_vol <= vol_50)] = 'Medium-Low'\n",
    "        regimes[(cond_vol > vol_50) & (cond_vol <= vol_75)] = 'Medium-High'\n",
    "        regimes[cond_vol > vol_75] = 'High'\n",
    "        \n",
    "        # レジーム統計\n",
    "        regime_stats = regimes.value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"\\n  レジーム分布:\")\n",
    "        for regime, percentage in regime_stats.items():\n",
    "            print(f\"    {regime}: {percentage:.1f}%\")\n",
    "        \n",
    "        # レジーム転換の分析\n",
    "        regime_changes = (regimes != regimes.shift(1)).sum()\n",
    "        avg_regime_duration = len(regimes) / regime_changes if regime_changes > 0 else len(regimes)\n",
    "        \n",
    "        print(f\"\\n  レジーム転換:\")\n",
    "        print(f\"    転換回数: {regime_changes}\")\n",
    "        print(f\"    平均継続期間: {avg_regime_duration:.1f}日\")\n",
    "        \n",
    "        # 各レジームでのリターン統計\n",
    "        return_col = f\"{name.replace('-', '_')}_return\"\n",
    "        if return_col in spreads_data.columns:\n",
    "            returns = spreads_data[return_col] * 100\n",
    "            \n",
    "            print(f\"\\n  レジーム別リターン統計:\")\n",
    "            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n",
    "                regime_returns = returns[regimes == regime]\n",
    "                if len(regime_returns) > 0:\n",
    "                    print(f\"    {regime}:\")\n",
    "                    print(f\"      平均リターン: {regime_returns.mean():.4f}%\")\n",
    "                    print(f\"      リターン標準偏差: {regime_returns.std():.4f}%\")\n",
    "                    print(f\"      最大損失: {regime_returns.min():.4f}%\")\n",
    "        \n",
    "        regime_analysis[name] = {\n",
    "            'conditional_volatility': cond_vol,\n",
    "            'regimes': regimes,\n",
    "            'quantiles': {\n",
    "                '25%': vol_25,\n",
    "                '50%': vol_50,\n",
    "                '75%': vol_75,\n",
    "                '95%': vol_95\n",
    "            },\n",
    "            'regime_distribution': regime_stats,\n",
    "            'regime_changes': regime_changes,\n",
    "            'avg_duration': avg_regime_duration\n",
    "        }\n",
    "    \n",
    "    return regime_analysis\n",
    "\n",
    "# ボラティリティレジーム分析実行\n",
    "regime_analysis = analyze_volatility_regimes(risk_metrics, spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_volatility_regimes(regime_analysis):\n    \"\"\"ボラティリティレジームの可視化\"\"\"\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=(\n            'M1-M2: ボラティリティレジーム',\n            'レジーム分布',\n            'M2-M3: ボラティリティレジーム',\n            'レジーム転換頻度'\n        )\n    )\n    \n    regime_colors = {\n        'Low': 'green',\n        'Medium-Low': 'yellow',\n        'Medium-High': 'orange', \n        'High': 'red'\n    }\n    \n    # 1. M1-M2 volatility regimes\n    if 'M1-M2' in regime_analysis:\n        m1m2_data = regime_analysis['M1-M2']\n        cond_vol = m1m2_data['conditional_volatility']\n        regimes = m1m2_data['regimes']\n        \n        # Volatility line\n        fig.add_trace(\n            go.Scatter(\n                x=cond_vol.index,\n                y=cond_vol,\n                name='M1-M2 ボラティリティ',\n                line=dict(color='blue', width=1),\n                showlegend=False\n            ),\n            row=1, col=1\n        )\n        \n        # Regime boundary lines\n        quantiles = m1m2_data['quantiles']\n        for label, value in quantiles.items():\n            fig.add_hline(y=value, line_dash=\"dash\", line_color=\"gray\", \n                         line_width=1, row=1, col=1)\n        \n        # Regime color mapping\n        for regime, color in regime_colors.items():\n            regime_mask = regimes == regime\n            if regime_mask.any():\n                regime_vol = cond_vol[regime_mask]\n                fig.add_trace(\n                    go.Scatter(\n                        x=regime_vol.index,\n                        y=regime_vol,\n                        mode='markers',\n                        name=f'{regime} レジーム',\n                        marker=dict(color=color, size=3, opacity=0.7)\n                    ),\n                    row=1, col=1\n                )\n    \n    # 2. Regime distribution\n    all_regime_dist = []\n    spread_labels = []\n    \n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_dist = regime_analysis[name]['regime_distribution']\n            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n                if regime in regime_dist:\n                    all_regime_dist.append(regime_dist[regime])\n                    spread_labels.append(f'{name}<br>{regime}')\n                else:\n                    all_regime_dist.append(0)\n                    spread_labels.append(f'{name}<br>{regime}')\n    \n    # Grouped bar chart for regime distribution\n    x_pos = 0\n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_dist = regime_analysis[name]['regime_distribution']\n            for regime in ['Low', 'Medium-Low', 'Medium-High', 'High']:\n                value = regime_dist.get(regime, 0)\n                fig.add_trace(\n                    go.Bar(\n                        x=[f'{name}<br>{regime}'],\n                        y=[value],\n                        name=regime,\n                        marker_color=regime_colors[regime],\n                        showlegend=(name == 'M1-M2')  # Show legend only for first group\n                    ),\n                    row=1, col=2\n                )\n    \n    # 3. M2-M3 volatility regimes\n    if 'M2-M3' in regime_analysis:\n        m2m3_data = regime_analysis['M2-M3']\n        cond_vol = m2m3_data['conditional_volatility']\n        regimes = m2m3_data['regimes']\n        \n        fig.add_trace(\n            go.Scatter(\n                x=cond_vol.index,\n                y=cond_vol,\n                name='M2-M3 ボラティリティ',\n                line=dict(color='red', width=1),\n                showlegend=False\n            ),\n            row=2, col=1\n        )\n        \n        # Regime boundary lines\n        quantiles = m2m3_data['quantiles']\n        for label, value in quantiles.items():\n            fig.add_hline(y=value, line_dash=\"dash\", line_color=\"gray\", \n                         line_width=1, row=2, col=1)\n    \n    # 4. Regime transition frequency\n    regime_changes = []\n    avg_durations = []\n    spread_names = []\n    \n    for name in ['M1-M2', 'M2-M3', 'M3-M4']:\n        if name in regime_analysis:\n            regime_changes.append(regime_analysis[name]['regime_changes'])\n            avg_durations.append(regime_analysis[name]['avg_duration'])\n            spread_names.append(name)\n    \n    fig.add_trace(\n        go.Bar(\n            x=spread_names,\n            y=regime_changes,\n            name='レジーム転換回数',\n            marker_color='lightblue',\n            yaxis='y',\n            offsetgroup=1\n        ),\n        row=2, col=2\n    )\n    \n    fig.add_trace(\n        go.Scatter(\n            x=spread_names,\n            y=avg_durations,\n            name='平均継続期間（日）',\n            mode='lines+markers',\n            line=dict(color='red', width=2),\n            marker=dict(size=8),\n            yaxis='y2'\n        ),\n        row=2, col=2\n    )\n    \n    fig.update_layout(\n        title=dict(\n            text=\"ボラティリティレジーム分析\",\n            x=0.5,\n            font=dict(size=16)\n        ),\n        height=800,\n        showlegend=True\n    )\n    \n    # Update axis labels\n    fig.update_yaxes(title_text=\"ボラティリティ (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"比率 (%)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"ボラティリティ (%)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"転換回数\", row=2, col=2)\n    \n    fig.update_xaxes(title_text=\"日付\", row=1, col=1)\n    fig.update_xaxes(title_text=\"日付\", row=2, col=1)\n    \n    # Secondary axis settings\n    fig.update_layout(\n        yaxis4=dict(\n            title=\"平均継続期間（日）\",\n            overlaying=\"y3\",\n            side=\"right\"\n        )\n    )\n    \n    return fig\n\n# Visualization of volatility regimes\nregime_chart = plot_volatility_regimes(regime_analysis)\nregime_chart.show()\n\n# Save image\nregime_chart.write_image('../generated_images/adjacent_spreads_volatility_regimes.png', \n                        width=1200, height=800, scale=2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. 分析結果サマリー",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ボラティリティモデリング分析サマリー\n",
    "def generate_volatility_modeling_summary(vol_stats, arch_results, garch_models, \n",
    "                                        risk_metrics, regime_analysis):\n",
    "    \"\"\"ボラティリティモデリング分析の包括的サマリー\"\"\"\n",
    "    \n",
    "    print(\"📋 隣月間スプレッド ボラティリティモデリング分析サマリー\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n📊 基本ボラティリティ特性:\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in vol_stats.columns:\n",
    "            daily_vol = vol_stats.loc['日次ボラティリティ(%)', spread]\n",
    "            annual_vol = vol_stats.loc['年率ボラティリティ(%)', spread]\n",
    "            skewness = vol_stats.loc['歪度', spread]\n",
    "            kurtosis = vol_stats.loc['尖度', spread]\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    日次ボラティリティ: {daily_vol:.3f}%\")\n",
    "            print(f\"    年率ボラティリティ: {annual_vol:.1f}%\")\n",
    "            print(f\"    歪度: {skewness:.3f} ({'右歪み' if skewness > 0 else '左歪み' if skewness < 0 else '対称'})\")\n",
    "            print(f\"    尖度: {kurtosis:.3f} ({'尖鋭' if kurtosis > 0 else '平坦'})\")\n",
    "    \n",
    "    print(f\"\\n🔬 ARCH効果検定:\")\n",
    "    arch_detected = 0\n",
    "    vol_clustering_detected = 0\n",
    "    \n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in arch_results:\n",
    "            has_arch = arch_results[spread]['has_arch_effects']\n",
    "            has_clustering = arch_results[spread]['has_volatility_clustering']\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    ARCH効果: {'あり' if has_arch else 'なし'}\")\n",
    "            print(f\"    ボラティリティクラスタリング: {'あり' if has_clustering else 'なし'}\")\n",
    "            \n",
    "            if has_arch:\n",
    "                arch_detected += 1\n",
    "            if has_clustering:\n",
    "                vol_clustering_detected += 1\n",
    "    \n",
    "    print(f\"\\n📈 GARCHモデル選択:\")\n",
    "    best_models = {}\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in garch_models:\n",
    "            best_model_name = garch_models[spread]['best_model_name']\n",
    "            best_aic = garch_models[spread]['best_model']['aic']\n",
    "            \n",
    "            print(f\"  {spread}: {best_model_name} (AIC: {best_aic:.2f})\")\n",
    "            best_models[spread] = best_model_name\n",
    "    \n",
    "    print(f\"\\n⚠️ リスク指標（95%信頼水準）:\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in risk_metrics:\n",
    "            current_vol = risk_metrics[spread]['current_volatility']\n",
    "            var_95 = abs(risk_metrics[spread]['var']['95.0%'])\n",
    "            es_95 = abs(risk_metrics[spread]['expected_shortfall']['95.0%'])\n",
    "            max_dd = risk_metrics[spread]['max_drawdown']\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    現在ボラティリティ: {current_vol:.3f}%\")\n",
    "            print(f\"    95% VaR: {var_95:.3f}%\")\n",
    "            print(f\"    95% ES: {es_95:.3f}%\")\n",
    "            print(f\"    最大ドローダウン: {max_dd:.3f}%\")\n",
    "    \n",
    "    print(f\"\\n🎭 ボラティリティレジーム:\")\n",
    "    for spread in ['M1-M2', 'M2-M3', 'M3-M4']:\n",
    "        if spread in regime_analysis:\n",
    "            regime_changes = regime_analysis[spread]['regime_changes']\n",
    "            avg_duration = regime_analysis[spread]['avg_duration']\n",
    "            high_vol_ratio = regime_analysis[spread]['regime_distribution'].get('High', 0)\n",
    "            \n",
    "            print(f\"  {spread}:\")\n",
    "            print(f\"    レジーム転換: {regime_changes}回\")\n",
    "            print(f\"    平均継続期間: {avg_duration:.1f}日\")\n",
    "            print(f\"    高ボラティリティ期間: {high_vol_ratio:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n💡 投資・リスク管理への示唆:\")\n",
    "    \n",
    "    # ARCH効果に基づく示唆\n",
    "    if arch_detected >= 2:\n",
    "        print(f\"  • {arch_detected}/3のスプレッドでARCH効果検出 → GARCHモデルによる動的リスク管理が有効\")\n",
    "    \n",
    "    if vol_clustering_detected >= 2:\n",
    "        print(f\"  • ボラティリティクラスタリング確認 → 高ボラ期間での慎重なポジション管理必要\")\n",
    "    \n",
    "    # 最もリスキーなスプレッドを特定\n",
    "    if risk_metrics:\n",
    "        highest_var_spread = max(risk_metrics.keys(), \n",
    "                               key=lambda x: abs(risk_metrics[x]['var']['95.0%']))\n",
    "        highest_var = abs(risk_metrics[highest_var_spread]['var']['95.0%'])\n",
    "        print(f\"  • 最高リスクスプレッド: {highest_var_spread} (95% VaR: {highest_var:.3f}%)\")\n",
    "    \n",
    "    # レジーム分析に基づく示唆\n",
    "    if regime_analysis:\n",
    "        unstable_spreads = []\n",
    "        for spread, data in regime_analysis.items():\n",
    "            if data['regime_changes'] > len(data['conditional_volatility']) / 50:  # 頻繁な転換\n",
    "                unstable_spreads.append(spread)\n",
    "        \n",
    "        if unstable_spreads:\n",
    "            print(f\"  • レジーム不安定: {', '.join(unstable_spreads)} → 動的ヘッジ戦略推奨\")\n",
    "    \n",
    "    print(f\"\\n🛡️ リスク管理推奨事項:\")\n",
    "    print(f\"  • GARCHモデルベースの動的VaR管理\")\n",
    "    print(f\"  • ボラティリティレジーム応答型ポジションサイジング\")\n",
    "    print(f\"  • 高ボラティリティ期間での損切り基準厳格化\")\n",
    "    print(f\"  • Expected Shortfallによる極端リスクシナリオ管理\")\n",
    "    \n",
    "    return {\n",
    "        'arch_effects_detected': arch_detected,\n",
    "        'volatility_clustering_detected': vol_clustering_detected,\n",
    "        'best_models': best_models,\n",
    "        'highest_risk_spread': highest_var_spread if risk_metrics else None,\n",
    "        'regime_instability': unstable_spreads if regime_analysis else []\n",
    "    }\n",
    "\n",
    "# サマリー生成\n",
    "volatility_summary = generate_volatility_modeling_summary(\n",
    "    vol_stats, arch_results, garch_models, risk_metrics, regime_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析結果の保存\n",
    "def save_volatility_analysis_results(vol_stats, garch_models, risk_metrics, \n",
    "                                    regime_analysis, volatility_summary):\n",
    "    \"\"\"ボラティリティ分析結果をファイルに保存\"\"\"\n",
    "    \n",
    "    # 出力ディレクトリ作成\n",
    "    os.makedirs('../analysis_results/adjacent_spreads', exist_ok=True)\n",
    "    \n",
    "    # 1. 基本ボラティリティ統計\n",
    "    vol_stats.to_csv('../analysis_results/adjacent_spreads/volatility_statistics.csv', \n",
    "                     encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. GARCH条件付きボラティリティ\n",
    "    garch_volatilities = pd.DataFrame()\n",
    "    for spread, models in garch_models.items():\n",
    "        best_model = models['best_model']\n",
    "        garch_volatilities[f'{spread}_volatility'] = best_model['conditional_volatility']\n",
    "    \n",
    "    if not garch_volatilities.empty:\n",
    "        garch_volatilities.to_csv('../analysis_results/adjacent_spreads/garch_conditional_volatility.csv', \n",
    "                                 encoding='utf-8-sig')\n",
    "    \n",
    "    # 3. VaRとリスク指標\n",
    "    risk_summary = pd.DataFrame()\n",
    "    for spread, metrics in risk_metrics.items():\n",
    "        risk_summary[f'{spread}_current_vol'] = [metrics['current_volatility']]\n",
    "        risk_summary[f'{spread}_95_var'] = [abs(metrics['var']['95.0%'])]\n",
    "        risk_summary[f'{spread}_95_es'] = [abs(metrics['expected_shortfall']['95.0%'])]\n",
    "        risk_summary[f'{spread}_max_drawdown'] = [metrics['max_drawdown']]\n",
    "    \n",
    "    if not risk_summary.empty:\n",
    "        risk_summary.to_csv('../analysis_results/adjacent_spreads/risk_metrics_summary.csv', \n",
    "                           encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # 4. ボラティリティレジーム\n",
    "    regime_data = pd.DataFrame()\n",
    "    for spread, analysis in regime_analysis.items():\n",
    "        regime_data[f'{spread}_regime'] = analysis['regimes']\n",
    "        regime_data[f'{spread}_volatility'] = analysis['conditional_volatility']\n",
    "    \n",
    "    if not regime_data.empty:\n",
    "        regime_data.to_csv('../analysis_results/adjacent_spreads/volatility_regimes.csv', \n",
    "                          encoding='utf-8-sig')\n",
    "    \n",
    "    # 5. 分析サマリー（JSON）\n",
    "    import json\n",
    "    \n",
    "    with open('../analysis_results/adjacent_spreads/volatility_modeling_summary.json', \n",
    "              'w', encoding='utf-8') as f:\n",
    "        json.dump(volatility_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 ボラティリティ分析結果を保存しました:\")\n",
    "    print(f\"  📊 基本統計: ../analysis_results/adjacent_spreads/volatility_statistics.csv\")\n",
    "    print(f\"  📈 GARCH条件付きボラティリティ: ../analysis_results/adjacent_spreads/garch_conditional_volatility.csv\")\n",
    "    print(f\"  ⚠️ リスク指標: ../analysis_results/adjacent_spreads/risk_metrics_summary.csv\")\n",
    "    print(f\"  🎭 ボラティリティレジーム: ../analysis_results/adjacent_spreads/volatility_regimes.csv\")\n",
    "    print(f\"  📝 分析サマリー: ../analysis_results/adjacent_spreads/volatility_modeling_summary.json\")\n",
    "\n",
    "# 分析結果保存\n",
    "save_volatility_analysis_results(\n",
    "    vol_stats, garch_models, risk_metrics, regime_analysis, volatility_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 次のステップ\n\nこのボラティリティモデリング分析により、隣月間スプレッドのリスク特性について詳細な洞察を得ることができました。\n\n### 主要な発見事項\n1. **ARCH効果**: 複数のスプレッドで条件付き分散不均一性を確認\n2. **ボラティリティクラスタリング**: 高ボラティリティ期間と低ボラティリティ期間の明確な区分\n3. **GARCHモデル**: 各スプレッドに最適なボラティリティモデルを特定\n4. **リスク指標**: VaR/ESによる定量的リスク評価\n5. **レジーム分析**: ボラティリティ環境の周期的変化\n\n### 今後の分析ステップ\n1. **機械学習予測**: 高度なパターン認識と予測モデル\n2. **トレーディング戦略開発**: ボラティリティベースの取引戦略\n3. **ダイナミックヘッジ**: レジーム応答型リスク管理\n4. **バックテスト**: 取引戦略の歴史的検証\n5. **ポートフォリオ最適化**: リスク調整後リターンの最大化\n\n次回のノートブック `4_adjacent_spreads_trading_strategies.ipynb` では、具体的な取引戦略とバックテストを実装します。",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}