{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMEéŠ…å…ˆç‰© éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æ\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆM1-M2ã€M2-M3ã€M3-M4ï¼‰ã®ç›¸é–¢é–¢ä¿‚ã¨å…±å’Œåˆ†é–¢ä¿‚ã‚’è©³ç´°ã«åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "### åˆ†æç›®æ¨™\n",
    "- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã®ç›¸é–¢æ§‹é€ ã®ç†è§£\n",
    "- å…±å’Œåˆ†é–¢ä¿‚ï¼ˆé•·æœŸå‡è¡¡é–¢ä¿‚ï¼‰ã®æ¤œå‡º\n",
    "- ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ã®æ©Ÿä¼šç‰¹å®š\n",
    "- ãƒªã‚¹ã‚¯åˆ†æ•£åŠ¹æœã®è©•ä¾¡\n",
    "\n",
    "### æœŸå¾…ã•ã‚Œã‚‹æˆæœ\n",
    "- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã®ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãã®æ™‚é–“å¤‰å‹•\n",
    "- å…±å’Œåˆ†é–¢ä¿‚ã«ã‚ˆã‚‹é•·æœŸå‡è¡¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç™ºè¦‹\n",
    "- çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š\n",
    "- ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ã¸ã®ç¤ºå”†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# çµ±è¨ˆãƒ»æ™‚ç³»åˆ—åˆ†æ\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¨ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã‚’å–å¾—\"\"\"\n",
    "    try:\n",
    "        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_calculate_spreads():\n",
    "    \"\"\"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨è¨ˆç®—\"\"\"\n",
    "    engine = get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        trade_date,\n",
    "        contract_month,\n",
    "        close_price,\n",
    "        volume,\n",
    "        open_interest\n",
    "    FROM lme_copper_futures \n",
    "    WHERE contract_month IN (1, 2, 3, 4)\n",
    "        AND close_price IS NOT NULL\n",
    "        AND close_price > 0\n",
    "    ORDER BY trade_date, contract_month\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "    \n",
    "    # ãƒ”ãƒœãƒƒãƒˆã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰è¨ˆç®—\n",
    "    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n",
    "    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n",
    "    \n",
    "    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰è¨ˆç®—\n",
    "    spreads_df = pd.DataFrame(index=pivot_df.index)\n",
    "    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n",
    "    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n",
    "    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n",
    "    \n",
    "    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒ\n",
    "    spreads_df['M1_price'] = pivot_df['M1']\n",
    "    spreads_df['M2_price'] = pivot_df['M2']\n",
    "    spreads_df['M3_price'] = pivot_df['M3']\n",
    "    spreads_df['M4_price'] = pivot_df['M4']\n",
    "    \n",
    "    return spreads_df.dropna()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "spreads_data = load_and_calculate_spreads()\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(spreads_data):,} ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n",
    "print(f\"ğŸ“… åˆ†ææœŸé–“: {spreads_data.index.min()} ï½ {spreads_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŸºæœ¬ç›¸é–¢åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_basic_correlations(df):\n",
    "    \"\"\"åŸºæœ¬ç›¸é–¢åˆ†æ\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    \n",
    "    # ç›¸é–¢è¡Œåˆ—è¨ˆç®—\n",
    "    correlation_matrix = df[spread_columns].corr()\n",
    "    \n",
    "    print(\"ğŸ“Š éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç›¸é–¢è¡Œåˆ—:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(correlation_matrix.round(4))\n",
    "    \n",
    "    # Spearmané †ä½ç›¸é–¢ã‚‚è¨ˆç®—\n",
    "    spearman_corr = df[spread_columns].corr(method='spearman')\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Spearmané †ä½ç›¸é–¢è¡Œåˆ—:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(spearman_corr.round(4))\n",
    "    \n",
    "    return correlation_matrix, spearman_corr\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix, spearman_corr):\n",
    "    \"\"\"ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Pearsonç›¸é–¢', 'Spearmané †ä½ç›¸é–¢'),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    # Pearsonç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=spread_names,\n",
    "            y=spread_names,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            text=np.round(corr_matrix.values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 14},\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=0.45)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Spearmanç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=spearman_corr.values,\n",
    "            x=spread_names,\n",
    "            y=spread_names,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            text=np.round(spearman_corr.values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 14},\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=1.02)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç›¸é–¢åˆ†æ\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=500,\n",
    "        width=900\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# åŸºæœ¬ç›¸é–¢åˆ†æå®Ÿè¡Œ\n",
    "pearson_corr, spearman_corr = analyze_basic_correlations(spreads_data)\n",
    "\n",
    "# ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º\n",
    "corr_chart = plot_correlation_heatmap(pearson_corr, spearman_corr)\n",
    "corr_chart.show()\n",
    "\n",
    "# ç”»åƒä¿å­˜\n",
    "os.makedirs('../generated_images', exist_ok=True)\n",
    "corr_chart.write_image('../generated_images/adjacent_spreads_correlation_heatmap.png', \n",
    "                      width=900, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ™‚é–“å¤‰å‹•ç›¸é–¢åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_correlations(df, window=60):\n",
    "    \"\"\"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ã®è¨ˆç®—\"\"\"\n",
    "    rolling_corr_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # M1-M2 vs M2-M3ã®ç›¸é–¢\n",
    "    rolling_corr_df['M1M2_vs_M2M3'] = df['M1_M2_spread'].rolling(window=window).corr(\n",
    "        df['M2_M3_spread']\n",
    "    )\n",
    "    \n",
    "    # M2-M3 vs M3-M4ã®ç›¸é–¢\n",
    "    rolling_corr_df['M2M3_vs_M3M4'] = df['M2_M3_spread'].rolling(window=window).corr(\n",
    "        df['M3_M4_spread']\n",
    "    )\n",
    "    \n",
    "    # M1-M2 vs M3-M4ã®ç›¸é–¢\n",
    "    rolling_corr_df['M1M2_vs_M3M4'] = df['M1_M2_spread'].rolling(window=window).corr(\n",
    "        df['M3_M4_spread']\n",
    "    )\n",
    "    \n",
    "    return rolling_corr_df.dropna()\n",
    "\n",
    "def plot_rolling_correlations(rolling_corr_df):\n",
    "    \"\"\"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ã®ãƒ—ãƒ­ãƒƒãƒˆ\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    correlations = [\n",
    "        ('M1M2_vs_M2M3', 'M1-M2 vs M2-M3', 'blue'),\n",
    "        ('M2M3_vs_M3M4', 'M2-M3 vs M3-M4', 'red'),\n",
    "        ('M1M2_vs_M3M4', 'M1-M2 vs M3-M4', 'green')\n",
    "    ]\n",
    "    \n",
    "    for col, name, color in correlations:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_corr_df.index,\n",
    "                y=rolling_corr_df[col],\n",
    "                name=name,\n",
    "                line=dict(color=color, width=2),\n",
    "                mode='lines'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # åŸºæº–ç·šã‚’è¿½åŠ \n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1)\n",
    "    fig.add_hline(y=0.5, line_dash=\"dot\", line_color=\"gray\", line_width=1)\n",
    "    fig.add_hline(y=-0.5, line_dash=\"dot\", line_color=\"gray\", line_width=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ï¼ˆ60æ—¥ï¼‰\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        xaxis_title=\"æ—¥ä»˜\",\n",
    "        yaxis_title=\"ç›¸é–¢ä¿‚æ•°\",\n",
    "        height=500,\n",
    "        showlegend=True,\n",
    "        yaxis=dict(range=[-1, 1])\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢è¨ˆç®—\n",
    "rolling_corr_data = calculate_rolling_correlations(spreads_data, window=60)\n",
    "\n",
    "print(f\"ğŸ“ˆ ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢çµ±è¨ˆï¼ˆ60æ—¥çª“ï¼‰:\")\n",
    "print(\"=\" * 50)\n",
    "print(rolling_corr_data.describe().round(4))\n",
    "\n",
    "# ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ãƒãƒ£ãƒ¼ãƒˆ\n",
    "rolling_corr_chart = plot_rolling_correlations(rolling_corr_data)\n",
    "rolling_corr_chart.show()\n",
    "\n",
    "# ç”»åƒä¿å­˜\n",
    "rolling_corr_chart.write_image('../generated_images/adjacent_spreads_rolling_correlation.png', \n",
    "                              width=1200, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å…±å’Œåˆ†åˆ†æï¼ˆé•·æœŸå‡è¡¡é–¢ä¿‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cointegration(df):\n",
    "    \"\"\"å…±å’Œåˆ†æ¤œå®šã®å®Ÿè¡Œ\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"ğŸ”¬ å…±å’Œåˆ†æ¤œå®šçµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    coint_results = {}\n",
    "    \n",
    "    # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºå…±å’Œåˆ†æ¤œå®š\n",
    "    pairs = [\n",
    "        (('M1_M2_spread', 'M2_M3_spread'), ('M1-M2', 'M2-M3')),\n",
    "        (('M2_M3_spread', 'M3_M4_spread'), ('M2-M3', 'M3-M4')),\n",
    "        (('M1_M2_spread', 'M3_M4_spread'), ('M1-M2', 'M3-M4'))\n",
    "    ]\n",
    "    \n",
    "    for (col1, col2), (name1, name2) in pairs:\n",
    "        # Engle-Grangerå…±å’Œåˆ†æ¤œå®š\n",
    "        coint_t, p_value, critical_values = coint(df[col1], df[col2])\n",
    "        \n",
    "        coint_results[f'{name1}_vs_{name2}'] = {\n",
    "            'test_statistic': coint_t,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name1} vs {name2}:\")\n",
    "        print(f\"  æ¤œå®šçµ±è¨ˆé‡: {coint_t:.4f}\")\n",
    "        print(f\"  på€¤: {p_value:.4f}\")\n",
    "        print(f\"  çµæœ: {'å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Š' if p_value < 0.05 else 'å…±å’Œåˆ†é–¢ä¿‚ãªã—'}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 1%: {critical_values[0]:.4f}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 5%: {critical_values[1]:.4f}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 10%: {critical_values[2]:.4f}\")\n",
    "    \n",
    "    return coint_results\n",
    "\n",
    "def johansen_cointegration_test(df):\n",
    "    \"\"\"Johansenå…±å’Œåˆ†æ¤œå®šï¼ˆå¤šå¤‰é‡ï¼‰\"\"\"\n",
    "    spread_data = df[['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']].dropna()\n",
    "    \n",
    "    print(f\"\\nğŸ” Johansenå…±å’Œåˆ†æ¤œå®šï¼ˆå¤šå¤‰é‡ï¼‰:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Johansenæ¤œå®šå®Ÿè¡Œ\n",
    "    johansen_result = coint_johansen(spread_data, det_order=0, k_ar_diff=1)\n",
    "    \n",
    "    print(f\"Traceçµ±è¨ˆé‡:\")\n",
    "    for i, (trace_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr1, johansen_result.cvt[:, 0], \n",
    "            johansen_result.cvt[:, 1], johansen_result.cvt[:, 2])\n",
    "    ):\n",
    "        print(f\"  râ‰¤{i}: {trace_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "        \n",
    "        if trace_stat > cv_95:\n",
    "            print(f\"    â†’ r>{i}ã®å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Šï¼ˆ95%æ°´æº–ï¼‰\")\n",
    "        else:\n",
    "            print(f\"    â†’ râ‰¤{i}ã®å…±å’Œåˆ†é–¢ä¿‚\")\n",
    "    \n",
    "    print(f\"\\nMaximum Eigenvalueçµ±è¨ˆé‡:\")\n",
    "    for i, (max_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr2, johansen_result.cvm[:, 0], \n",
    "            johansen_result.cvm[:, 1], johansen_result.cvm[:, 2])\n",
    "    ):\n",
    "        print(f\"  r={i}: {max_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "    \n",
    "    return johansen_result\n",
    "\n",
    "# å…±å’Œåˆ†æ¤œå®šå®Ÿè¡Œ\n",
    "coint_results = test_cointegration(spreads_data)\n",
    "johansen_result = johansen_cointegration_test(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cointegration_relationships(df, coint_results):\n",
    "    \"\"\"å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æœ‰æ„ãªå…±å’Œåˆ†é–¢ä¿‚ã‚’æŒã¤ãƒšã‚¢ã‚’ç‰¹å®š\n",
    "    significant_pairs = []\n",
    "    \n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            significant_pairs.append(pair_name)\n",
    "            print(f\"âœ… {pair_name}: å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Šï¼ˆp={result['p_value']:.4f}ï¼‰\")\n",
    "    \n",
    "    if not significant_pairs:\n",
    "        print(\"âŒ æœ‰æ„ãªå…±å’Œåˆ†é–¢ä¿‚ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "        return None\n",
    "    \n",
    "    # æœ€ã‚‚å¼·ã„å…±å’Œåˆ†é–¢ä¿‚ã®ãƒšã‚¢ã§è©³ç´°åˆ†æ\n",
    "    best_pair_name = min(coint_results.keys(), key=lambda x: coint_results[x]['p_value'])\n",
    "    print(f\"\\nğŸ¯ æœ€å¼·å…±å’Œåˆ†é–¢ä¿‚: {best_pair_name}\")\n",
    "    \n",
    "    # ãƒšã‚¢åã‹ã‚‰å®Ÿéš›ã®ã‚«ãƒ©ãƒ åã‚’ç‰¹å®š\n",
    "    if 'M1-M2_vs_M2-M3' in best_pair_name:\n",
    "        col1, col2 = 'M1_M2_spread', 'M2_M3_spread'\n",
    "        name1, name2 = 'M1-M2', 'M2-M3'\n",
    "    elif 'M2-M3_vs_M3-M4' in best_pair_name:\n",
    "        col1, col2 = 'M2_M3_spread', 'M3_M4_spread'\n",
    "        name1, name2 = 'M2-M3', 'M3-M4'\n",
    "    else:\n",
    "        col1, col2 = 'M1_M2_spread', 'M3_M4_spread'\n",
    "        name1, name2 = 'M1-M2', 'M3-M4'\n",
    "    \n",
    "    # å…±å’Œåˆ†å›å¸°\n",
    "    y = df[col1]\n",
    "    x = sm.add_constant(df[col2])\n",
    "    \n",
    "    model = OLS(y, x).fit()\n",
    "    \n",
    "    print(f\"\\nå›å¸°å¼: {name1} = {model.params[0]:.4f} + {model.params[1]:.4f} * {name2}\")\n",
    "    print(f\"RÂ²: {model.rsquared:.4f}\")\n",
    "    print(f\"å›å¸°ä¿‚æ•°ã®tå€¤: {model.tvalues[1]:.4f}\")\n",
    "    \n",
    "    # æ®‹å·®ï¼ˆèª¤å·®ä¿®æ­£é …ï¼‰ã®è¨ˆç®—\n",
    "    residuals = model.resid\n",
    "    \n",
    "    # æ®‹å·®ã®å®šå¸¸æ€§æ¤œå®š\n",
    "    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n",
    "    \n",
    "    print(f\"\\næ®‹å·®ã®å®šå¸¸æ€§æ¤œå®šï¼ˆADFï¼‰:\")\n",
    "    print(f\"  çµ±è¨ˆé‡: {adf_stat:.4f}\")\n",
    "    print(f\"  på€¤: {adf_p:.4f}\")\n",
    "    print(f\"  çµæœ: {'å®šå¸¸' if adf_p < 0.05 else 'éå®šå¸¸'}\")\n",
    "    \n",
    "    return {\n",
    "        'best_pair': (col1, col2),\n",
    "        'regression_model': model,\n",
    "        'residuals': residuals,\n",
    "        'adf_test': (adf_stat, adf_p)\n",
    "    }\n",
    "\n",
    "# å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ\n",
    "coint_analysis = analyze_cointegration_relationships(spreads_data, coint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cointegration_analysis(df, coint_analysis):\n",
    "    \"\"\"å…±å’Œåˆ†åˆ†æã®å¯è¦–åŒ–\"\"\"\n",
    "    if coint_analysis is None:\n",
    "        print(\"å…±å’Œåˆ†é–¢ä¿‚ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãŸã‚ã€ãƒ—ãƒ­ãƒƒãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "        return None\n",
    "    \n",
    "    col1, col2 = coint_analysis['best_pair']\n",
    "    residuals = coint_analysis['residuals']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒšã‚¢ã®æ™‚ç³»åˆ—',\n",
    "            'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰æ•£å¸ƒå›³ã¨å›å¸°ç·š',\n",
    "            'èª¤å·®ä¿®æ­£é …ï¼ˆæ®‹å·®ï¼‰ã®æ¨ç§»',\n",
    "            'æ®‹å·®ã®åˆ†å¸ƒ'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[col1],\n",
    "            name=col1.replace('_spread', '').replace('_', '-'),\n",
    "            line=dict(color='blue', width=1)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[col2],\n",
    "            name=col2.replace('_spread', '').replace('_', '-'),\n",
    "            line=dict(color='red', width=1)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. æ•£å¸ƒå›³ã¨å›å¸°ç·š\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[col2],\n",
    "            y=df[col1],\n",
    "            mode='markers',\n",
    "            name='ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ',\n",
    "            marker=dict(color='lightblue', size=3, opacity=0.6)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # å›å¸°ç·š\n",
    "    model = coint_analysis['regression_model']\n",
    "    x_range = np.linspace(df[col2].min(), df[col2].max(), 100)\n",
    "    y_pred = model.params[0] + model.params[1] * x_range\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            name='å›å¸°ç·š',\n",
    "            line=dict(color='red', width=2)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. æ®‹å·®ã®æ™‚ç³»åˆ—\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=residuals,\n",
    "            name='èª¤å·®ä¿®æ­£é …',\n",
    "            line=dict(color='green', width=1)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1, row=2, col=1)\n",
    "    \n",
    "    # 4. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=residuals,\n",
    "            name='æ®‹å·®åˆ†å¸ƒ',\n",
    "            nbinsx=50,\n",
    "            marker_color='purple',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"å…±å’Œåˆ†åˆ†æ - é•·æœŸå‡è¡¡é–¢ä¿‚ã®å¯è¦–åŒ–\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # è»¸ãƒ©ãƒ™ãƒ«æ›´æ–°\n",
    "    fig.update_yaxes(title_text=\"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ (USD/t)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=f\"{col2.replace('_spread', '').replace('_', '-')} ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=f\"{col1.replace('_spread', '').replace('_', '-')} ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰\", row=1, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"æ®‹å·®\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"æ®‹å·®\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"é »åº¦\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# å…±å’Œåˆ†åˆ†æã®å¯è¦–åŒ–\n",
    "if coint_analysis:\n",
    "    coint_chart = plot_cointegration_analysis(spreads_data, coint_analysis)\n",
    "    if coint_chart:\n",
    "        coint_chart.show()\n",
    "        \n",
    "        # ç”»åƒä¿å­˜\n",
    "        coint_chart.write_image('../generated_images/adjacent_spreads_cointegration_analysis.png', \n",
    "                               width=1200, height=800, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(df):\n",
    "    \"\"\"ä¸»æˆåˆ†åˆ†æã®å®Ÿè¡Œ\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_data = df[spread_columns].dropna()\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(spread_data)\n",
    "    \n",
    "    # PCAå®Ÿè¡Œ\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    print(f\"ğŸ“Š ä¸»æˆåˆ†åˆ†æçµæœ:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # å¯„ä¸ç‡ã¨ç´¯ç©å¯„ä¸ç‡\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    for i, (var_ratio, cum_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n",
    "        print(f\"PC{i+1}: å¯„ä¸ç‡ {var_ratio:.4f} ({var_ratio*100:.2f}%), ç´¯ç©å¯„ä¸ç‡ {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n",
    "    \n",
    "    # ä¸»æˆåˆ†è² è·é‡\n",
    "    print(f\"\\nğŸ“ˆ ä¸»æˆåˆ†è² è·é‡:\")\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    \n",
    "    loadings_df = pd.DataFrame(\n",
    "        loadings,\n",
    "        index=['M1-M2', 'M2-M3', 'M3-M4'],\n",
    "        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n",
    "    )\n",
    "    \n",
    "    print(loadings_df.round(4))\n",
    "    \n",
    "    # ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢\n",
    "    pca_scores_df = pd.DataFrame(\n",
    "        pca_result,\n",
    "        index=spread_data.index,\n",
    "        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'pca_model': pca,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance_ratio': cumulative_variance_ratio,\n",
    "        'loadings': loadings_df,\n",
    "        'scores': pca_scores_df,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "def plot_pca_analysis(pca_results):\n",
    "    \"\"\"PCAåˆ†æã®å¯è¦–åŒ–\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'å¯„ä¸ç‡ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼‰',\n",
    "            'ä¸»æˆåˆ†è² è·é‡',\n",
    "            'ç¬¬1ãƒ»ç¬¬2ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢',\n",
    "            'ç¬¬1ä¸»æˆåˆ†ã®æ™‚ç³»åˆ—'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    pcs = [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=pcs,\n",
    "            y=pca_results['explained_variance_ratio'] * 100,\n",
    "            name='å¯„ä¸ç‡',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pcs,\n",
    "            y=pca_results['cumulative_variance_ratio'] * 100,\n",
    "            name='ç´¯ç©å¯„ä¸ç‡',\n",
    "            line=dict(color='red', width=2),\n",
    "            yaxis='y2'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. ä¸»æˆåˆ†è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=pca_results['loadings'].values,\n",
    "            x=pca_results['loadings'].columns,\n",
    "            y=pca_results['loadings'].index,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            text=np.round(pca_results['loadings'].values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 12},\n",
    "            showscale=True\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢æ•£å¸ƒå›³\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_results['scores']['PC1'],\n",
    "            y=pca_results['scores']['PC2'],\n",
    "            mode='markers',\n",
    "            name='PC1 vs PC2',\n",
    "            marker=dict(\n",
    "                color=pca_results['scores'].index.map(lambda x: x.year),\n",
    "                colorscale='Viridis',\n",
    "                size=4,\n",
    "                opacity=0.7,\n",
    "                colorbar=dict(title=\"å¹´\")\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. ç¬¬1ä¸»æˆåˆ†ã®æ™‚ç³»åˆ—\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_results['scores'].index,\n",
    "            y=pca_results['scores']['PC1'],\n",
    "            name='ç¬¬1ä¸»æˆåˆ†',\n",
    "            line=dict(color='blue', width=1)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1, row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ä¸»æˆåˆ†åˆ†æ\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # è»¸ãƒ©ãƒ™ãƒ«æ›´æ–°\n",
    "    fig.update_yaxes(title_text=\"å¯„ä¸ç‡ (%)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"ä¸»æˆåˆ†\", row=1, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"ç¬¬2ä¸»æˆåˆ†\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"ç¬¬1ä¸»æˆåˆ†\", row=2, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"ç¬¬1ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# PCAåˆ†æå®Ÿè¡Œ\n",
    "pca_results = perform_pca_analysis(spreads_data)\n",
    "\n",
    "# PCAå¯è¦–åŒ–\n",
    "pca_chart = plot_pca_analysis(pca_results)\n",
    "pca_chart.show()\n",
    "\n",
    "# ç”»åƒä¿å­˜\n",
    "pca_chart.write_image('../generated_images/adjacent_spreads_pca_analysis.png', \n",
    "                     width=1200, height=800, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_statistical_arbitrage_opportunities(df, pca_results, coint_analysis):\n",
    "    \"\"\"çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ¯ çµ±è¨ˆçš„è£å®šæ©Ÿä¼šåˆ†æ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    arbitrage_opportunities = {}\n",
    "    \n",
    "    # 1. å…±å’Œåˆ†ãƒ™ãƒ¼ã‚¹ã®è£å®š\n",
    "    if coint_analysis and coint_analysis['adf_test'][1] < 0.05:\n",
    "        residuals = coint_analysis['residuals']\n",
    "        \n",
    "        # æ®‹å·®ã®çµ±è¨ˆé‡\n",
    "        residual_mean = residuals.mean()\n",
    "        residual_std = residuals.std()\n",
    "        \n",
    "        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ»ã‚¨ã‚°ã‚¸ãƒƒãƒˆã‚·ã‚°ãƒŠãƒ«\n",
    "        upper_threshold = residual_mean + 2 * residual_std\n",
    "        lower_threshold = residual_mean - 2 * residual_std\n",
    "        \n",
    "        # ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ\n",
    "        signals = pd.Series(index=residuals.index, dtype=float)\n",
    "        signals[residuals > upper_threshold] = -1  # ã‚·ãƒ§ãƒ¼ãƒˆã‚·ã‚°ãƒŠãƒ«\n",
    "        signals[residuals < lower_threshold] = 1   # ãƒ­ãƒ³ã‚°ã‚·ã‚°ãƒŠãƒ«\n",
    "        signals[abs(residuals - residual_mean) < 0.5 * residual_std] = 0  # ã‚¨ã‚°ã‚¸ãƒƒãƒˆ\n",
    "        \n",
    "        # å‰å€¤ã§åŸ‹ã‚ã‚‹\n",
    "        signals = signals.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        arbitrage_opportunities['cointegration_pairs_trading'] = {\n",
    "            'pair': coint_analysis['best_pair'],\n",
    "            'residuals': residuals,\n",
    "            'signals': signals,\n",
    "            'thresholds': (lower_threshold, upper_threshold),\n",
    "            'signal_frequency': (signals != 0).sum() / len(signals) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰æ©Ÿä¼š:\")\n",
    "        print(f\"   å¯¾è±¡ãƒšã‚¢: {coint_analysis['best_pair']}\")\n",
    "        print(f\"   ã‚·ã‚°ãƒŠãƒ«é »åº¦: {arbitrage_opportunities['cointegration_pairs_trading']['signal_frequency']:.2f}%\")\n",
    "        print(f\"   ã‚¨ãƒ³ãƒˆãƒªãƒ¼é–¾å€¤: Â±{2:.1f}Ïƒ ({lower_threshold:.4f}, {upper_threshold:.4f})\")\n",
    "    \n",
    "    # 2. ä¸»æˆåˆ†ãƒ™ãƒ¼ã‚¹ã®è£å®š\n",
    "    pc1_scores = pca_results['scores']['PC1']\n",
    "    pc1_mean = pc1_scores.mean()\n",
    "    pc1_std = pc1_scores.std()\n",
    "    \n",
    "    # ç¬¬1ä¸»æˆåˆ†ã®æ¥µå€¤æ¤œå‡º\n",
    "    pc1_upper = pc1_mean + 2 * pc1_std\n",
    "    pc1_lower = pc1_mean - 2 * pc1_std\n",
    "    \n",
    "    pc1_signals = pd.Series(index=pc1_scores.index, dtype=float)\n",
    "    pc1_signals[pc1_scores > pc1_upper] = -1\n",
    "    pc1_signals[pc1_scores < pc1_lower] = 1\n",
    "    pc1_signals[abs(pc1_scores - pc1_mean) < 0.5 * pc1_std] = 0\n",
    "    pc1_signals = pc1_signals.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    arbitrage_opportunities['pca_factor_trading'] = {\n",
    "        'pc1_scores': pc1_scores,\n",
    "        'signals': pc1_signals,\n",
    "        'thresholds': (pc1_lower, pc1_upper),\n",
    "        'signal_frequency': (pc1_signals != 0).sum() / len(pc1_signals) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•æ©Ÿä¼š:\")\n",
    "    print(f\"   ç¬¬1ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"   ã‚·ã‚°ãƒŠãƒ«é »åº¦: {arbitrage_opportunities['pca_factor_trading']['signal_frequency']:.2f}%\")\n",
    "    print(f\"   ã‚¨ãƒ³ãƒˆãƒªãƒ¼é–¾å€¤: Â±{2:.1f}Ïƒ ({pc1_lower:.4f}, {pc1_upper:.4f})\")\n",
    "    \n",
    "    # 3. ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ©Ÿä¼š\n",
    "    rolling_corr_60d = spreads_data['M1_M2_spread'].rolling(window=60).corr(\n",
    "        spreads_data['M2_M3_spread']\n",
    "    ).dropna()\n",
    "    \n",
    "    corr_mean = rolling_corr_60d.mean()\n",
    "    corr_std = rolling_corr_60d.std()\n",
    "    \n",
    "    # ç›¸é–¢ã®ç•°å¸¸å€¤ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³ï¼‰ã‚’æ¤œå‡º\n",
    "    corr_breakdown_threshold = corr_mean - 2 * corr_std\n",
    "    correlation_breakdowns = rolling_corr_60d < corr_breakdown_threshold\n",
    "    \n",
    "    arbitrage_opportunities['correlation_breakdown'] = {\n",
    "        'rolling_correlation': rolling_corr_60d,\n",
    "        'breakdown_threshold': corr_breakdown_threshold,\n",
    "        'breakdown_periods': correlation_breakdowns,\n",
    "        'breakdown_frequency': correlation_breakdowns.sum() / len(correlation_breakdowns) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ©Ÿä¼š:\")\n",
    "    print(f\"   å¹³å‡ç›¸é–¢: {corr_mean:.4f}\")\n",
    "    print(f\"   ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é–¾å€¤: {corr_breakdown_threshold:.4f}\")\n",
    "    print(f\"   ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é »åº¦: {arbitrage_opportunities['correlation_breakdown']['breakdown_frequency']:.2f}%\")\n",
    "    \n",
    "    return arbitrage_opportunities\n",
    "\n",
    "# çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®åˆ†æ\n",
    "arbitrage_opps = identify_statistical_arbitrage_opportunities(\n",
    "    spreads_data, pca_results, coint_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arbitrage_opportunities(arbitrage_opps):\n",
    "    \"\"\"çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®å¯è¦–åŒ–\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=(\n",
    "            'å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚·ã‚°ãƒŠãƒ«',\n",
    "            'ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•ã‚·ã‚°ãƒŠãƒ«',\n",
    "            'ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ©Ÿä¼š'\n",
    "        ),\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    # 1. å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰\n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_data = arbitrage_opps['cointegration_pairs_trading']\n",
    "        residuals = coint_data['residuals']\n",
    "        signals = coint_data['signals']\n",
    "        lower_thresh, upper_thresh = coint_data['thresholds']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=residuals.index,\n",
    "                y=residuals,\n",
    "                name='èª¤å·®ä¿®æ­£é …',\n",
    "                line=dict(color='blue', width=1)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # é–¾å€¤ç·š\n",
    "        fig.add_hline(y=upper_thresh, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        fig.add_hline(y=lower_thresh, line_dash=\"dash\", line_color=\"green\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        \n",
    "        # ã‚·ã‚°ãƒŠãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n",
    "        buy_signals = signals[signals == 1]\n",
    "        sell_signals = signals[signals == -1]\n",
    "        \n",
    "        if len(buy_signals) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=buy_signals.index,\n",
    "                    y=residuals.loc[buy_signals.index],\n",
    "                    mode='markers',\n",
    "                    name='è²·ã„ã‚·ã‚°ãƒŠãƒ«',\n",
    "                    marker=dict(color='green', size=8, symbol='triangle-up')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        if len(sell_signals) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sell_signals.index,\n",
    "                    y=residuals.loc[sell_signals.index],\n",
    "                    mode='markers',\n",
    "                    name='å£²ã‚Šã‚·ã‚°ãƒŠãƒ«',\n",
    "                    marker=dict(color='red', size=8, symbol='triangle-down')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # 2. ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•\n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_data = arbitrage_opps['pca_factor_trading']\n",
    "        pc1_scores = pca_data['pc1_scores']\n",
    "        pc1_signals = pca_data['signals']\n",
    "        pc1_lower, pc1_upper = pca_data['thresholds']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pc1_scores.index,\n",
    "                y=pc1_scores,\n",
    "                name='ç¬¬1ä¸»æˆåˆ†',\n",
    "                line=dict(color='purple', width=1)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # é–¾å€¤ç·š\n",
    "        fig.add_hline(y=pc1_upper, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "        fig.add_hline(y=pc1_lower, line_dash=\"dash\", line_color=\"green\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "    \n",
    "    # 3. ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³\n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        corr_data = arbitrage_opps['correlation_breakdown']\n",
    "        rolling_corr = corr_data['rolling_correlation']\n",
    "        breakdown_threshold = corr_data['breakdown_threshold']\n",
    "        breakdowns = corr_data['breakdown_periods']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_corr.index,\n",
    "                y=rolling_corr,\n",
    "                name='60æ—¥ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢',\n",
    "                line=dict(color='orange', width=1)\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é–¾å€¤\n",
    "        fig.add_hline(y=breakdown_threshold, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=2, row=3, col=1)\n",
    "        \n",
    "        # ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æœŸé–“ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n",
    "        breakdown_dates = breakdowns[breakdowns].index\n",
    "        if len(breakdown_dates) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=breakdown_dates,\n",
    "                    y=rolling_corr.loc[breakdown_dates],\n",
    "                    mode='markers',\n",
    "                    name='ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³',\n",
    "                    marker=dict(color='red', size=6)\n",
    "                ),\n",
    "                row=3, col=1\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=1000,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # è»¸ãƒ©ãƒ™ãƒ«æ›´æ–°\n",
    "    fig.update_yaxes(title_text=\"æ®‹å·®\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"PC1ã‚¹ã‚³ã‚¢\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"ç›¸é–¢ä¿‚æ•°\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"æ—¥ä»˜\", row=3, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# è£å®šæ©Ÿä¼šã®å¯è¦–åŒ–\n",
    "arbitrage_chart = plot_arbitrage_opportunities(arbitrage_opps)\n",
    "arbitrage_chart.show()\n",
    "\n",
    "# ç”»åƒä¿å­˜\n",
    "arbitrage_chart.write_image('../generated_images/adjacent_spreads_arbitrage_opportunities.png', \n",
    "                           width=1200, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆ†æçµæœã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ…æ‹¬çš„åˆ†æã‚µãƒãƒªãƒ¼\n",
    "def generate_correlation_analysis_summary(pearson_corr, coint_results, pca_results, arbitrage_opps):\n",
    "    \"\"\"ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã®åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nğŸ”— ç›¸é–¢æ§‹é€ :\")\n",
    "    print(f\"  M1-M2 vs M2-M3: {pearson_corr.loc['M1_M2_spread', 'M2_M3_spread']:.3f}\")\n",
    "    print(f\"  M2-M3 vs M3-M4: {pearson_corr.loc['M2_M3_spread', 'M3_M4_spread']:.3f}\")\n",
    "    print(f\"  M1-M2 vs M3-M4: {pearson_corr.loc['M1_M2_spread', 'M3_M4_spread']:.3f}\")\n",
    "    \n",
    "    # æœ€å¼·ç›¸é–¢ãƒšã‚¢\n",
    "    max_corr = 0\n",
    "    max_pair = \"\"\n",
    "    for i in range(len(pearson_corr)):\n",
    "        for j in range(i+1, len(pearson_corr)):\n",
    "            corr_val = abs(pearson_corr.iloc[i, j])\n",
    "            if corr_val > max_corr:\n",
    "                max_corr = corr_val\n",
    "                max_pair = f\"{pearson_corr.index[i]} vs {pearson_corr.columns[j]}\"\n",
    "    \n",
    "    print(f\"  æœ€å¼·ç›¸é–¢ãƒšã‚¢: {max_pair} ({max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å…±å’Œåˆ†åˆ†æ:\")\n",
    "    coint_pairs = 0\n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            coint_pairs += 1\n",
    "            print(f\"  âœ… {pair_name}: p={result['p_value']:.4f} (å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Š)\")\n",
    "        else:\n",
    "            print(f\"  âŒ {pair_name}: p={result['p_value']:.4f} (å…±å’Œåˆ†é–¢ä¿‚ãªã—)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ä¸»æˆåˆ†åˆ†æ:\")\n",
    "    print(f\"  ç¬¬1ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"  ç¬¬2ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][1]*100:.2f}%\")\n",
    "    print(f\"  ç´¯ç©å¯„ä¸ç‡ï¼ˆPC1+PC2ï¼‰: {pca_results['cumulative_variance_ratio'][1]*100:.2f}%\")\n",
    "    \n",
    "    # ç¬¬1ä¸»æˆåˆ†ã®æ§‹æˆ\n",
    "    pc1_loadings = pca_results['loadings']['PC1']\n",
    "    dominant_component = pc1_loadings.abs().idxmax()\n",
    "    print(f\"  ç¬¬1ä¸»æˆåˆ†ã®æ”¯é…çš„è¦ç´ : {dominant_component} ({pc1_loadings[dominant_component]:.3f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ’° çµ±è¨ˆçš„è£å®šæ©Ÿä¼š:\")\n",
    "    total_opportunities = 0\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_freq = arbitrage_opps['cointegration_pairs_trading']['signal_frequency']\n",
    "        print(f\"  å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰: {coint_freq:.2f}% ã®ã‚·ã‚°ãƒŠãƒ«é »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_freq = arbitrage_opps['pca_factor_trading']['signal_frequency']\n",
    "        print(f\"  ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•: {pca_freq:.2f}% ã®ã‚·ã‚°ãƒŠãƒ«é »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        breakdown_freq = arbitrage_opps['correlation_breakdown']['breakdown_frequency']\n",
    "        print(f\"  ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³: {breakdown_freq:.2f}% ã®ç™ºç”Ÿé »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æŠ•è³‡æˆ¦ç•¥ã¸ã®ç¤ºå”†:\")\n",
    "    \n",
    "    if coint_pairs > 0:\n",
    "        print(f\"  â€¢ å…±å’Œåˆ†é–¢ä¿‚ã‚’åˆ©ç”¨ã—ãŸå¹³å‡å›å¸°æˆ¦ç•¥ãŒæœ‰åŠ¹\")\n",
    "        print(f\"  â€¢ é•·æœŸå‡è¡¡ã‹ã‚‰ã®ä¹–é›¢ã‚’ç‹™ã£ãŸãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒå¯èƒ½\")\n",
    "    \n",
    "    if pca_results['explained_variance_ratio'][0] > 0.6:\n",
    "        print(f\"  â€¢ ç¬¬1ä¸»æˆåˆ†ãŒé«˜å¯„ä¸ç‡ï¼ˆ{pca_results['explained_variance_ratio'][0]*100:.1f}%ï¼‰\")\n",
    "        print(f\"  â€¢ ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æ´»ç”¨å¯èƒ½\")\n",
    "    \n",
    "    print(f\"  â€¢ {total_opportunities}ç¨®é¡ã®çµ±è¨ˆçš„è£å®šæˆ¦ç•¥ãŒå®Ÿè£…å¯èƒ½\")\n",
    "    print(f\"  â€¢ ç›¸é–¢æ§‹é€ ã®æ™‚é–“å¤‰å‹•ã‚’æ´»ç”¨ã—ãŸå‹•çš„ãƒ˜ãƒƒã‚¸æˆ¦ç•¥\")\n",
    "    \n",
    "    # ãƒªã‚¹ã‚¯ç®¡ç†ã®æè¨€\n",
    "    avg_corr = np.mean([abs(pearson_corr.iloc[i, j]) for i in range(len(pearson_corr)) \n",
    "                       for j in range(i+1, len(pearson_corr))])\n",
    "    \n",
    "    print(f\"\\nâš ï¸ ãƒªã‚¹ã‚¯ç®¡ç†:\")\n",
    "    print(f\"  â€¢ å¹³å‡ç›¸é–¢: {avg_corr:.3f} - {'é«˜ã„' if avg_corr > 0.5 else 'ä¸­ç¨‹åº¦ã®'}åˆ†æ•£åŠ¹æœ\")\n",
    "    \n",
    "    if max_corr > 0.8:\n",
    "        print(f\"  â€¢ ä¸€éƒ¨ãƒšã‚¢ã§é«˜ç›¸é–¢ï¼ˆ{max_corr:.3f}ï¼‰ - é›†ä¸­ãƒªã‚¹ã‚¯ã«æ³¨æ„\")\n",
    "    \n",
    "    print(f\"  â€¢ ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ™‚ã®æå¤±æ‹¡å¤§ãƒªã‚¹ã‚¯ã‚’è€ƒæ…®\")\n",
    "    print(f\"  â€¢ è¤‡æ•°æˆ¦ç•¥ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯åˆ†æ•£æ¨å¥¨\")\n",
    "    \n",
    "    return {\n",
    "        'max_correlation': (max_pair, max_corr),\n",
    "        'cointegrated_pairs': coint_pairs,\n",
    "        'pca_pc1_contribution': pca_results['explained_variance_ratio'][0],\n",
    "        'arbitrage_opportunities': total_opportunities,\n",
    "        'average_correlation': avg_corr\n",
    "    }\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼ç”Ÿæˆ\n",
    "correlation_summary = generate_correlation_analysis_summary(\n",
    "    pearson_corr, coint_results, pca_results, arbitrage_opps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçµæœã®ä¿å­˜\n",
    "def save_correlation_analysis_results(pearson_corr, spearman_corr, pca_results, \n",
    "                                     arbitrage_opps, correlation_summary):\n",
    "    \"\"\"ç›¸é–¢åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\"\"\"\n",
    "    \n",
    "    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    os.makedirs('../analysis_results/adjacent_spreads', exist_ok=True)\n",
    "    \n",
    "    # 1. ç›¸é–¢è¡Œåˆ—\n",
    "    pearson_corr.to_csv('../analysis_results/adjacent_spreads/pearson_correlation.csv', \n",
    "                       encoding='utf-8-sig')\n",
    "    spearman_corr.to_csv('../analysis_results/adjacent_spreads/spearman_correlation.csv', \n",
    "                        encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. PCAçµæœ\n",
    "    pca_results['loadings'].to_csv('../analysis_results/adjacent_spreads/pca_loadings.csv', \n",
    "                                  encoding='utf-8-sig')\n",
    "    pca_results['scores'].to_csv('../analysis_results/adjacent_spreads/pca_scores.csv', \n",
    "                                encoding='utf-8-sig')\n",
    "    \n",
    "    # 3. å¯„ä¸ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    variance_df = pd.DataFrame({\n",
    "        'Component': [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))],\n",
    "        'Explained_Variance_Ratio': pca_results['explained_variance_ratio'],\n",
    "        'Cumulative_Variance_Ratio': pca_results['cumulative_variance_ratio']\n",
    "    })\n",
    "    variance_df.to_csv('../analysis_results/adjacent_spreads/pca_variance_explained.csv', \n",
    "                      encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # 4. çµ±è¨ˆçš„è£å®šã‚·ã‚°ãƒŠãƒ«\n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_signals = arbitrage_opps['cointegration_pairs_trading']['signals']\n",
    "        coint_signals.to_csv('../analysis_results/adjacent_spreads/cointegration_signals.csv', \n",
    "                            encoding='utf-8-sig')\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_signals = arbitrage_opps['pca_factor_trading']['signals']\n",
    "        pca_signals.to_csv('../analysis_results/adjacent_spreads/pca_factor_signals.csv', \n",
    "                          encoding='utf-8-sig')\n",
    "    \n",
    "    # 5. åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆJSONï¼‰\n",
    "    import json\n",
    "    \n",
    "    with open('../analysis_results/adjacent_spreads/correlation_analysis_summary.json', \n",
    "              'w', encoding='utf-8') as f:\n",
    "        json.dump(correlation_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ç›¸é–¢åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
    "    print(f\"  ğŸ“Š Pearsonç›¸é–¢: ../analysis_results/adjacent_spreads/pearson_correlation.csv\")\n",
    "    print(f\"  ğŸ“ˆ Spearmanç›¸é–¢: ../analysis_results/adjacent_spreads/spearman_correlation.csv\")\n",
    "    print(f\"  ğŸ¯ PCAè² è·é‡: ../analysis_results/adjacent_spreads/pca_loadings.csv\")\n",
    "    print(f\"  ğŸ“‰ PCAã‚¹ã‚³ã‚¢: ../analysis_results/adjacent_spreads/pca_scores.csv\")\n",
    "    print(f\"  ğŸ“‹ å¯„ä¸ç‡: ../analysis_results/adjacent_spreads/pca_variance_explained.csv\")\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        print(f\"  ğŸ’° å…±å’Œåˆ†ã‚·ã‚°ãƒŠãƒ«: ../analysis_results/adjacent_spreads/cointegration_signals.csv\")\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        print(f\"  ğŸ”§ PCAã‚·ã‚°ãƒŠãƒ«: ../analysis_results/adjacent_spreads/pca_factor_signals.csv\")\n",
    "    \n",
    "    print(f\"  ğŸ“ åˆ†æã‚µãƒãƒªãƒ¼: ../analysis_results/adjacent_spreads/correlation_analysis_summary.json\")\n",
    "\n",
    "# åˆ†æçµæœä¿å­˜\n",
    "save_correlation_analysis_results(\n",
    "    pearson_corr, spearman_corr, pca_results, arbitrage_opps, correlation_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "ã“ã®ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã«ã‚ˆã‚Šã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æŠŠæ¡ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ä¸»è¦ç™ºè¦‹äº‹é …\n",
    "1. **ç›¸é–¢æ§‹é€ **: éš£æ¥ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã«ä¸­ç¨‹åº¦ã‹ã‚‰å¼·ã„ç›¸é–¢é–¢ä¿‚\n",
    "2. **å…±å’Œåˆ†é–¢ä¿‚**: ä¸€éƒ¨ãƒšã‚¢ã§é•·æœŸå‡è¡¡é–¢ä¿‚ã‚’ç¢ºèª\n",
    "3. **ä¸»æˆåˆ†æ§‹é€ **: ç¬¬1ä¸»æˆåˆ†ãŒå…¨å¤‰å‹•ã®å¤§éƒ¨åˆ†ã‚’èª¬æ˜\n",
    "4. **è£å®šæ©Ÿä¼š**: è¤‡æ•°ã®çµ±è¨ˆçš„è£å®šæˆ¦ç•¥ãŒå®Ÿè£…å¯èƒ½\n",
    "\n",
    "### æ¬¡ã®åˆ†æã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°**: GARCHç³»ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯åˆ†æ\n",
    "2. **æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬**: ã‚ˆã‚Šé«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨äºˆæ¸¬\n",
    "3. **å–å¼•æˆ¦ç•¥æ§‹ç¯‰**: å®Ÿéš›ã®å£²è²·ãƒ«ãƒ¼ãƒ«ã¨ãƒªã‚¹ã‚¯ç®¡ç†\n",
    "4. **ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã§ã®æˆ¦ç•¥æ¤œè¨¼\n",
    "5. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡**: ãƒªã‚¹ã‚¯èª¿æ•´å¾Œãƒªã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡\n",
    "\n",
    "æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ `3_adjacent_spreads_volatility_modeling.ipynb` ã§ã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒªã‚¹ã‚¯ç‰¹æ€§ã®è©³ç´°åˆ†æã‚’å®Ÿæ–½ã—ã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}