{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMEéŠ…å…ˆç‰© éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æ\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆM1-M2ã€M2-M3ã€M3-M4ï¼‰ã®ç›¸é–¢é–¢ä¿‚ã¨å…±å’Œåˆ†é–¢ä¿‚ã‚’è©³ç´°ã«åˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "### åˆ†æç›®æ¨™\n",
    "- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã®ç›¸é–¢æ§‹é€ ã®ç†è§£\n",
    "- å…±å’Œåˆ†é–¢ä¿‚ï¼ˆé•·æœŸå‡è¡¡é–¢ä¿‚ï¼‰ã®æ¤œå‡º\n",
    "- ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ã®æ©Ÿä¼šç‰¹å®š\n",
    "- ãƒªã‚¹ã‚¯åˆ†æ•£åŠ¹æœã®è©•ä¾¡\n",
    "\n",
    "### æœŸå¾…ã•ã‚Œã‚‹æˆæœ\n",
    "- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã®ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãã®æ™‚é–“å¤‰å‹•\n",
    "- å…±å’Œåˆ†é–¢ä¿‚ã«ã‚ˆã‚‹é•·æœŸå‡è¡¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç™ºè¦‹\n",
    "- çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š\n",
    "- ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ã¸ã®ç¤ºå”†"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psycopg2\nfrom sqlalchemy import create_engine\nimport warnings\nfrom datetime import datetime, timedelta\n# import plotly.graph_objects as go\n# import plotly.express as px\n# from plotly.subplots import make_subplots\n# Plotly replaced with matplotlib for compatibility\nfrom scipy import stats\nimport os\n\n# çµ±è¨ˆãƒ»æ™‚ç³»åˆ—åˆ†æ - å•é¡Œã®ã‚ã‚‹é–¢æ•°ã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\ntry:\n    from statsmodels.tsa.stattools import coint, adfuller\n    from statsmodels.stats.diagnostic import acorr_ljungbox\n    from statsmodels.tsa.vector_ar.vecm import coint_johansen\n    from statsmodels.regression.linear_model import OLS\n    # statsmodels.apiã®ä»£ã‚ã‚Šã«ã€å¿…è¦ãªé–¢æ•°ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n    from statsmodels.tools import add_constant\n    print(\"âœ… Statsmodels ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Statsmodels ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n    print(\"ä¸€éƒ¨ã®é«˜åº¦ãªçµ±è¨ˆåˆ†ææ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã¾ã™\")\n    # åŸºæœ¬çš„ãªä»£æ›¿é–¢æ•°ã‚’å®šç¾©\n    from scipy.stats import linregress\n    def add_constant(x):\n        \"\"\"statsmodels.add_constantã®ä»£æ›¿\"\"\"\n        return np.column_stack([np.ones(len(x)), x])\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã¨ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã‚’å–å¾—\"\"\"\n",
    "    try:\n",
    "        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_calculate_spreads():\n",
    "    \"\"\"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨è¨ˆç®—\"\"\"\n",
    "    engine = get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        trade_date,\n",
    "        contract_month,\n",
    "        close_price,\n",
    "        volume,\n",
    "        open_interest\n",
    "    FROM lme_copper_futures \n",
    "    WHERE contract_month IN (1, 2, 3, 4)\n",
    "        AND close_price IS NOT NULL\n",
    "        AND close_price > 0\n",
    "    ORDER BY trade_date, contract_month\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "    \n",
    "    # ãƒ”ãƒœãƒƒãƒˆã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰è¨ˆç®—\n",
    "    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n",
    "    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n",
    "    \n",
    "    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰è¨ˆç®—\n",
    "    spreads_df = pd.DataFrame(index=pivot_df.index)\n",
    "    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n",
    "    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n",
    "    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n",
    "    \n",
    "    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒ\n",
    "    spreads_df['M1_price'] = pivot_df['M1']\n",
    "    spreads_df['M2_price'] = pivot_df['M2']\n",
    "    spreads_df['M3_price'] = pivot_df['M3']\n",
    "    spreads_df['M4_price'] = pivot_df['M4']\n",
    "    \n",
    "    return spreads_df.dropna()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "spreads_data = load_and_calculate_spreads()\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(spreads_data):,} ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n",
    "print(f\"ğŸ“… åˆ†ææœŸé–“: {spreads_data.index.min()} ï½ {spreads_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åŸºæœ¬ç›¸é–¢åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_basic_correlations(df):\n    \"\"\"åŸºæœ¬ç›¸é–¢åˆ†æ\"\"\"\n    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n    \n    # ç›¸é–¢è¡Œåˆ—è¨ˆç®—\n    correlation_matrix = df[spread_columns].corr()\n    \n    print(\"ğŸ“Š éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç›¸é–¢è¡Œåˆ—:\")\n    print(\"=\" * 40)\n    print(correlation_matrix.round(4))\n    \n    # Spearmané †ä½ç›¸é–¢ã‚‚è¨ˆç®—\n    spearman_corr = df[spread_columns].corr(method='spearman')\n    \n    print(\"\\nğŸ“ˆ Spearmané †ä½ç›¸é–¢è¡Œåˆ—:\")\n    print(\"=\" * 40)\n    print(spearman_corr.round(4))\n    \n    return correlation_matrix, spearman_corr\n\ndef plot_correlation_heatmap(corr_matrix, spearman_corr):\n    \"\"\"ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆï¼ˆmatplotlibç‰ˆï¼‰\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    # Pearsonç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n    im1 = ax1.imshow(corr_matrix.values, cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n    ax1.set_xticks(range(len(spread_names)))\n    ax1.set_yticks(range(len(spread_names)))\n    ax1.set_xticklabels(spread_names)\n    ax1.set_yticklabels(spread_names)\n    ax1.set_title('Pearson Correlation', fontsize=14)\n    \n    # æ•°å€¤ã‚’ã‚»ãƒ«ã«è¡¨ç¤º\n    for i in range(len(spread_names)):\n        for j in range(len(spread_names)):\n            ax1.text(j, i, f'{corr_matrix.iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=12)\n    \n    # Spearmanç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n    im2 = ax2.imshow(spearman_corr.values, cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n    ax2.set_xticks(range(len(spread_names)))\n    ax2.set_yticks(range(len(spread_names)))\n    ax2.set_xticklabels(spread_names)\n    ax2.set_yticklabels(spread_names)\n    ax2.set_title('Spearman Rank Correlation', fontsize=14)\n    \n    # æ•°å€¤ã‚’ã‚»ãƒ«ã«è¡¨ç¤º\n    for i in range(len(spread_names)):\n        for j in range(len(spread_names)):\n            ax2.text(j, i, f'{spearman_corr.iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=12)\n    \n    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼è¿½åŠ \n    plt.colorbar(im1, ax=ax1, shrink=0.6)\n    plt.colorbar(im2, ax=ax2, shrink=0.6)\n    \n    plt.suptitle('Adjacent Month Spreads Correlation Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# åŸºæœ¬ç›¸é–¢åˆ†æå®Ÿè¡Œ\npearson_corr, spearman_corr = analyze_basic_correlations(spreads_data)\n\n# ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º\ncorr_chart = plot_correlation_heatmap(pearson_corr, spearman_corr)\nplt.show()\n\n# ç”»åƒä¿å­˜\nos.makedirs('../../generated_images', exist_ok=True)\ncorr_chart.savefig('../../generated_images/adjacent_spreads_correlation_heatmap.png', \n                   dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ™‚é–“å¤‰å‹•ç›¸é–¢åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def calculate_rolling_correlations(df, window=60):\n    \"\"\"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ã®è¨ˆç®—\"\"\"\n    rolling_corr_df = pd.DataFrame(index=df.index)\n    \n    # M1-M2 vs M2-M3ã®ç›¸é–¢\n    rolling_corr_df['M1M2_vs_M2M3'] = df['M1_M2_spread'].rolling(window=window).corr(\n        df['M2_M3_spread']\n    )\n    \n    # M2-M3 vs M3-M4ã®ç›¸é–¢\n    rolling_corr_df['M2M3_vs_M3M4'] = df['M2_M3_spread'].rolling(window=window).corr(\n        df['M3_M4_spread']\n    )\n    \n    # M1-M2 vs M3-M4ã®ç›¸é–¢\n    rolling_corr_df['M1M2_vs_M3M4'] = df['M1_M2_spread'].rolling(window=window).corr(\n        df['M3_M4_spread']\n    )\n    \n    return rolling_corr_df.dropna()\n\ndef plot_rolling_correlations(rolling_corr_df):\n    \"\"\"ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆmatplotlibç‰ˆï¼‰\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    correlations = [\n        ('M1M2_vs_M2M3', 'M1-M2 vs M2-M3', 'blue'),\n        ('M2M3_vs_M3M4', 'M2-M3 vs M3-M4', 'red'),\n        ('M1M2_vs_M3M4', 'M1-M2 vs M3-M4', 'green')\n    ]\n    \n    for col, name, color in correlations:\n        ax.plot(rolling_corr_df.index, rolling_corr_df[col], \n               label=name, color=color, linewidth=2)\n    \n    # åŸºæº–ç·šã‚’è¿½åŠ \n    ax.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax.axhline(y=0.5, linestyle=':', color='gray', linewidth=1)\n    ax.axhline(y=-0.5, linestyle=':', color='gray', linewidth=1)\n    \n    ax.set_title('Adjacent Month Spreads Rolling Correlation (60 days)', fontsize=16)\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Correlation Coefficient')\n    ax.set_ylim(-1, 1)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return fig\n\n# ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢è¨ˆç®—\nrolling_corr_data = calculate_rolling_correlations(spreads_data, window=60)\n\nprint(f\"ğŸ“ˆ ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢çµ±è¨ˆï¼ˆ60æ—¥çª“ï¼‰:\")\nprint(\"=\" * 50)\nprint(rolling_corr_data.describe().round(4))\n\n# ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç›¸é–¢ãƒãƒ£ãƒ¼ãƒˆ\nrolling_corr_chart = plot_rolling_correlations(rolling_corr_data)\nplt.show()\n\n# ç”»åƒä¿å­˜\nrolling_corr_chart.savefig('../../generated_images/adjacent_spreads_rolling_correlation.png', \n                           dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å…±å’Œåˆ†åˆ†æï¼ˆé•·æœŸå‡è¡¡é–¢ä¿‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cointegration(df):\n",
    "    \"\"\"å…±å’Œåˆ†æ¤œå®šã®å®Ÿè¡Œ\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"ğŸ”¬ å…±å’Œåˆ†æ¤œå®šçµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    coint_results = {}\n",
    "    \n",
    "    # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºå…±å’Œåˆ†æ¤œå®š\n",
    "    pairs = [\n",
    "        (('M1_M2_spread', 'M2_M3_spread'), ('M1-M2', 'M2-M3')),\n",
    "        (('M2_M3_spread', 'M3_M4_spread'), ('M2-M3', 'M3-M4')),\n",
    "        (('M1_M2_spread', 'M3_M4_spread'), ('M1-M2', 'M3-M4'))\n",
    "    ]\n",
    "    \n",
    "    for (col1, col2), (name1, name2) in pairs:\n",
    "        # Engle-Grangerå…±å’Œåˆ†æ¤œå®š\n",
    "        coint_t, p_value, critical_values = coint(df[col1], df[col2])\n",
    "        \n",
    "        coint_results[f'{name1}_vs_{name2}'] = {\n",
    "            'test_statistic': coint_t,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name1} vs {name2}:\")\n",
    "        print(f\"  æ¤œå®šçµ±è¨ˆé‡: {coint_t:.4f}\")\n",
    "        print(f\"  på€¤: {p_value:.4f}\")\n",
    "        print(f\"  çµæœ: {'å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Š' if p_value < 0.05 else 'å…±å’Œåˆ†é–¢ä¿‚ãªã—'}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 1%: {critical_values[0]:.4f}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 5%: {critical_values[1]:.4f}\")\n",
    "        print(f\"  è‡¨ç•Œå€¤ 10%: {critical_values[2]:.4f}\")\n",
    "    \n",
    "    return coint_results\n",
    "\n",
    "def johansen_cointegration_test(df):\n",
    "    \"\"\"Johansenå…±å’Œåˆ†æ¤œå®šï¼ˆå¤šå¤‰é‡ï¼‰\"\"\"\n",
    "    spread_data = df[['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']].dropna()\n",
    "    \n",
    "    print(f\"\\nğŸ” Johansenå…±å’Œåˆ†æ¤œå®šï¼ˆå¤šå¤‰é‡ï¼‰:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Johansenæ¤œå®šå®Ÿè¡Œ\n",
    "    johansen_result = coint_johansen(spread_data, det_order=0, k_ar_diff=1)\n",
    "    \n",
    "    print(f\"Traceçµ±è¨ˆé‡:\")\n",
    "    for i, (trace_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr1, johansen_result.cvt[:, 0], \n",
    "            johansen_result.cvt[:, 1], johansen_result.cvt[:, 2])\n",
    "    ):\n",
    "        print(f\"  râ‰¤{i}: {trace_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "        \n",
    "        if trace_stat > cv_95:\n",
    "            print(f\"    â†’ r>{i}ã®å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Šï¼ˆ95%æ°´æº–ï¼‰\")\n",
    "        else:\n",
    "            print(f\"    â†’ râ‰¤{i}ã®å…±å’Œåˆ†é–¢ä¿‚\")\n",
    "    \n",
    "    print(f\"\\nMaximum Eigenvalueçµ±è¨ˆé‡:\")\n",
    "    for i, (max_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr2, johansen_result.cvm[:, 0], \n",
    "            johansen_result.cvm[:, 1], johansen_result.cvm[:, 2])\n",
    "    ):\n",
    "        print(f\"  r={i}: {max_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "    \n",
    "    return johansen_result\n",
    "\n",
    "# å…±å’Œåˆ†æ¤œå®šå®Ÿè¡Œ\n",
    "coint_results = test_cointegration(spreads_data)\n",
    "johansen_result = johansen_cointegration_test(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_cointegration_relationships(df, coint_results):\n    \"\"\"å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ\"\"\"\n    \n    print(f\"\\nğŸ“Š å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ:\")\n    print(\"=\" * 60)\n    \n    # æœ‰æ„ãªå…±å’Œåˆ†é–¢ä¿‚ã‚’æŒã¤ãƒšã‚¢ã‚’ç‰¹å®š\n    significant_pairs = []\n    \n    for pair_name, result in coint_results.items():\n        if result['p_value'] < 0.05:\n            significant_pairs.append(pair_name)\n            print(f\"âœ… {pair_name}: å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Šï¼ˆp={result['p_value']:.4f}ï¼‰\")\n    \n    if not significant_pairs:\n        print(\"âŒ æœ‰æ„ãªå…±å’Œåˆ†é–¢ä¿‚ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")\n        return None\n    \n    # æœ€ã‚‚å¼·ã„å…±å’Œåˆ†é–¢ä¿‚ã®ãƒšã‚¢ã§è©³ç´°åˆ†æ\n    best_pair_name = min(coint_results.keys(), key=lambda x: coint_results[x]['p_value'])\n    print(f\"\\nğŸ¯ æœ€å¼·å…±å’Œåˆ†é–¢ä¿‚: {best_pair_name}\")\n    \n    # ãƒšã‚¢åã‹ã‚‰å®Ÿéš›ã®ã‚«ãƒ©ãƒ åã‚’ç‰¹å®š\n    if 'M1-M2_vs_M2-M3' in best_pair_name:\n        col1, col2 = 'M1_M2_spread', 'M2_M3_spread'\n        name1, name2 = 'M1-M2', 'M2-M3'\n    elif 'M2-M3_vs_M3-M4' in best_pair_name:\n        col1, col2 = 'M2_M3_spread', 'M3_M4_spread'\n        name1, name2 = 'M2-M3', 'M3-M4'\n    else:\n        col1, col2 = 'M1_M2_spread', 'M3_M4_spread'\n        name1, name2 = 'M1-M2', 'M3-M4'\n    \n    try:\n        # å…±å’Œåˆ†å›å¸°\n        y = df[col1]\n        x = add_constant(df[col2])\n        \n        model = OLS(y, x).fit()\n        \n        print(f\"\\nå›å¸°å¼: {name1} = {model.params[0]:.4f} + {model.params[1]:.4f} * {name2}\")\n        print(f\"RÂ²: {model.rsquared:.4f}\")\n        print(f\"å›å¸°ä¿‚æ•°ã®tå€¤: {model.tvalues[1]:.4f}\")\n        \n        # æ®‹å·®ï¼ˆèª¤å·®ä¿®æ­£é …ï¼‰ã®è¨ˆç®—\n        residuals = model.resid\n        \n        # æ®‹å·®ã®å®šå¸¸æ€§æ¤œå®š\n        adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n        \n        print(f\"\\næ®‹å·®ã®å®šå¸¸æ€§æ¤œå®šï¼ˆADFï¼‰:\")\n        print(f\"  çµ±è¨ˆé‡: {adf_stat:.4f}\")\n        print(f\"  på€¤: {adf_p:.4f}\")\n        print(f\"  çµæœ: {'å®šå¸¸' if adf_p < 0.05 else 'éå®šå¸¸'}\")\n        \n        return {\n            'best_pair': (col1, col2),\n            'regression_model': model,\n            'residuals': residuals,\n            'adf_test': (adf_stat, adf_p)\n        }\n        \n    except Exception as e:\n        print(f\"âš ï¸ å…±å’Œåˆ†å›å¸°åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}\")\n        print(\"scipy.statsã‚’ä½¿ç”¨ã—ãŸç°¡æ˜“å›å¸°åˆ†æã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n        \n        # ä»£æ›¿åˆ†æï¼ˆscipyã‚’ä½¿ç”¨ï¼‰\n        from scipy.stats import linregress\n        \n        slope, intercept, r_value, p_value, std_err = linregress(df[col2], df[col1])\n        \n        print(f\"\\nå›å¸°å¼: {name1} = {intercept:.4f} + {slope:.4f} * {name2}\")\n        print(f\"RÂ²: {r_value**2:.4f}\")\n        print(f\"på€¤: {p_value:.4f}\")\n        \n        # æ®‹å·®è¨ˆç®—\n        predicted = intercept + slope * df[col2]\n        residuals = df[col1] - predicted\n        \n        # æ®‹å·®ã®å®šå¸¸æ€§æ¤œå®š\n        adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n        \n        print(f\"\\næ®‹å·®ã®å®šå¸¸æ€§æ¤œå®šï¼ˆADFï¼‰:\")\n        print(f\"  çµ±è¨ˆé‡: {adf_stat:.4f}\")\n        print(f\"  på€¤: {adf_p:.4f}\")\n        print(f\"  çµæœ: {'å®šå¸¸' if adf_p < 0.05 else 'éå®šå¸¸'}\")\n        \n        # ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ\n        class SimpleModel:\n            def __init__(self, intercept, slope, r_squared):\n                self.params = [intercept, slope]\n                self.rsquared = r_squared\n        \n        model = SimpleModel(intercept, slope, r_value**2)\n        \n        return {\n            'best_pair': (col1, col2),\n            'regression_model': model,\n            'residuals': residuals,\n            'adf_test': (adf_stat, adf_p)\n        }\n\n# å…±å’Œåˆ†é–¢ä¿‚ã®è©³ç´°åˆ†æ\ncoint_analysis = analyze_cointegration_relationships(spreads_data, coint_results)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_cointegration_analysis(df, coint_analysis):\n    \"\"\"å…±å’Œåˆ†åˆ†æã®å¯è¦–åŒ–ï¼ˆmatplotlibç‰ˆï¼‰\"\"\"\n    if coint_analysis is None:\n        print(\"å…±å’Œåˆ†é–¢ä¿‚ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãŸã‚ã€ãƒ—ãƒ­ãƒƒãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n        return None\n    \n    col1, col2 = coint_analysis['best_pair']\n    residuals = coint_analysis['residuals']\n    model = coint_analysis['regression_model']\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ\n    ax1.plot(df.index, df[col1], label=col1.replace('_spread', '').replace('_', '-'), color='blue')\n    ax1.plot(df.index, df[col2], label=col2.replace('_spread', '').replace('_', '-'), color='red')\n    ax1.set_title('Spread Pair Time Series')\n    ax1.set_ylabel('Spread (USD/t)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. æ•£å¸ƒå›³ã¨å›å¸°ç·š\n    ax2.scatter(df[col2], df[col1], alpha=0.6, s=10, color='lightblue', label='Data Points')\n    \n    # å›å¸°ç·š\n    x_range = np.linspace(df[col2].min(), df[col2].max(), 100)\n    y_pred = model.params[0] + model.params[1] * x_range\n    ax2.plot(x_range, y_pred, color='red', linewidth=2, label='Regression Line')\n    \n    ax2.set_title('Spread Scatter Plot with Regression Line')\n    ax2.set_xlabel(f\"{col2.replace('_spread', '').replace('_', '-')} Spread\")\n    ax2.set_ylabel(f\"{col1.replace('_spread', '').replace('_', '-')} Spread\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # 3. æ®‹å·®ã®æ™‚ç³»åˆ—\n    ax3.plot(df.index, residuals, color='green', linewidth=1, label='Error Correction Term')\n    ax3.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax3.set_title('Error Correction Term (Residuals) Over Time')\n    ax3.set_xlabel('Date')\n    ax3.set_ylabel('Residuals')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n    ax4.hist(residuals, bins=50, alpha=0.7, color='purple', label='Residual Distribution')\n    ax4.set_title('Residual Distribution')\n    ax4.set_xlabel('Residuals')\n    ax4.set_ylabel('Frequency')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.suptitle('Cointegration Analysis - Long-term Equilibrium Relationship', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# å…±å’Œåˆ†åˆ†æã®å¯è¦–åŒ–\nif coint_analysis:\n    coint_chart = plot_cointegration_analysis(spreads_data, coint_analysis)\n    if coint_chart:\n        plt.show()\n        \n        # ç”»åƒä¿å­˜\n        coint_chart.savefig('../../generated_images/adjacent_spreads_cointegration_analysis.png', \n                           dpi=300, bbox_inches='tight')\n        plt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def perform_pca_analysis(df):\n    \"\"\"ä¸»æˆåˆ†åˆ†æã®å®Ÿè¡Œ\"\"\"\n    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n    spread_data = df[spread_columns].dropna()\n    \n    # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(spread_data)\n    \n    # PCAå®Ÿè¡Œ\n    pca = PCA()\n    pca_result = pca.fit_transform(scaled_data)\n    \n    print(f\"ğŸ“Š ä¸»æˆåˆ†åˆ†æçµæœ:\")\n    print(\"=\" * 50)\n    \n    # å¯„ä¸ç‡ã¨ç´¯ç©å¯„ä¸ç‡\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n    \n    for i, (var_ratio, cum_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n        print(f\"PC{i+1}: å¯„ä¸ç‡ {var_ratio:.4f} ({var_ratio*100:.2f}%), ç´¯ç©å¯„ä¸ç‡ {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n    \n    # ä¸»æˆåˆ†è² è·é‡\n    print(f\"\\nğŸ“ˆ ä¸»æˆåˆ†è² è·é‡:\")\n    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n    \n    loadings_df = pd.DataFrame(\n        loadings,\n        index=['M1-M2', 'M2-M3', 'M3-M4'],\n        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n    )\n    \n    print(loadings_df.round(4))\n    \n    # ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢\n    pca_scores_df = pd.DataFrame(\n        pca_result,\n        index=spread_data.index,\n        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n    )\n    \n    return {\n        'pca_model': pca,\n        'explained_variance_ratio': explained_variance_ratio,\n        'cumulative_variance_ratio': cumulative_variance_ratio,\n        'loadings': loadings_df,\n        'scores': pca_scores_df,\n        'scaler': scaler\n    }\n\ndef plot_pca_analysis(pca_results):\n    \"\"\"PCAåˆ†æã®å¯è¦–åŒ–ï¼ˆmatplotlibç‰ˆï¼‰\"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ\n    pcs = [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))]\n    explained_var = pca_results['explained_variance_ratio'] * 100\n    cumulative_var = pca_results['cumulative_variance_ratio'] * 100\n    \n    bars = ax1.bar(pcs, explained_var, color='lightblue', alpha=0.7, label='Variance Explained')\n    ax1_twin = ax1.twinx()\n    ax1_twin.plot(pcs, cumulative_var, color='red', marker='o', linewidth=2, label='Cumulative Variance')\n    \n    ax1.set_title('Scree Plot')\n    ax1.set_xlabel('Principal Component')\n    ax1.set_ylabel('Variance Explained (%)', color='blue')\n    ax1_twin.set_ylabel('Cumulative Variance (%)', color='red')\n    ax1.legend(loc='upper left')\n    ax1_twin.legend(loc='upper right')\n    \n    # 2. ä¸»æˆåˆ†è² è·é‡ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n    im = ax2.imshow(pca_results['loadings'].values, cmap='RdBu', aspect='auto')\n    ax2.set_xticks(range(len(pca_results['loadings'].columns)))\n    ax2.set_yticks(range(len(pca_results['loadings'].index)))\n    ax2.set_xticklabels(pca_results['loadings'].columns)\n    ax2.set_yticklabels(pca_results['loadings'].index)\n    ax2.set_title('Principal Component Loadings')\n    \n    # æ•°å€¤ã‚’ã‚»ãƒ«ã«è¡¨ç¤º\n    for i in range(len(pca_results['loadings'].index)):\n        for j in range(len(pca_results['loadings'].columns)):\n            ax2.text(j, i, f'{pca_results[\"loadings\"].iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=10)\n    \n    plt.colorbar(im, ax=ax2, shrink=0.6)\n    \n    # 3. ä¸»æˆåˆ†ã‚¹ã‚³ã‚¢æ•£å¸ƒå›³\n    years = pca_results['scores'].index.year\n    scatter = ax3.scatter(pca_results['scores']['PC1'], pca_results['scores']['PC2'], \n                         c=years, cmap='viridis', alpha=0.7, s=20)\n    ax3.set_title('PC1 vs PC2 Scatter Plot')\n    ax3.set_xlabel('First Principal Component')\n    ax3.set_ylabel('Second Principal Component')\n    plt.colorbar(scatter, ax=ax3, label='Year')\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. ç¬¬1ä¸»æˆåˆ†ã®æ™‚ç³»åˆ—\n    ax4.plot(pca_results['scores'].index, pca_results['scores']['PC1'], \n            color='blue', linewidth=1, label='PC1 Score')\n    ax4.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax4.set_title('First Principal Component Time Series')\n    ax4.set_xlabel('Date')\n    ax4.set_ylabel('PC1 Score')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.suptitle('Adjacent Month Spreads Principal Component Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# PCAåˆ†æå®Ÿè¡Œ\npca_results = perform_pca_analysis(spreads_data)\n\n# PCAå¯è¦–åŒ–\npca_chart = plot_pca_analysis(pca_results)\nplt.show()\n\n# ç”»åƒä¿å­˜\npca_chart.savefig('../../generated_images/adjacent_spreads_pca_analysis.png', \n                  dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def identify_statistical_arbitrage_opportunities(df, pca_results, coint_analysis):\n    \"\"\"çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®ç‰¹å®š\"\"\"\n    \n    print(f\"ğŸ¯ çµ±è¨ˆçš„è£å®šæ©Ÿä¼šåˆ†æ:\")\n    print(\"=\" * 60)\n    \n    arbitrage_opportunities = {}\n    \n    # 1. å…±å’Œåˆ†ãƒ™ãƒ¼ã‚¹ã®è£å®š\n    if coint_analysis and coint_analysis['adf_test'][1] < 0.05:\n        residuals = coint_analysis['residuals']\n        \n        # æ®‹å·®ã®çµ±è¨ˆé‡\n        residual_mean = residuals.mean()\n        residual_std = residuals.std()\n        \n        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ»ã‚¨ã‚°ã‚¸ãƒƒãƒˆã‚·ã‚°ãƒŠãƒ«\n        upper_threshold = residual_mean + 2 * residual_std\n        lower_threshold = residual_mean - 2 * residual_std\n        \n        # ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ\n        signals = pd.Series(index=residuals.index, dtype=float)\n        signals[residuals > upper_threshold] = -1  # ã‚·ãƒ§ãƒ¼ãƒˆã‚·ã‚°ãƒŠãƒ«\n        signals[residuals < lower_threshold] = 1   # ãƒ­ãƒ³ã‚°ã‚·ã‚°ãƒŠãƒ«\n        signals[abs(residuals - residual_mean) < 0.5 * residual_std] = 0  # ã‚¨ã‚°ã‚¸ãƒƒãƒˆ\n        \n        # å‰å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼‰\n        signals = signals.ffill().fillna(0)\n        \n        arbitrage_opportunities['cointegration_pairs_trading'] = {\n            'pair': coint_analysis['best_pair'],\n            'residuals': residuals,\n            'signals': signals,\n            'thresholds': (lower_threshold, upper_threshold),\n            'signal_frequency': (signals != 0).sum() / len(signals) * 100\n        }\n        \n        print(f\"âœ… å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰æ©Ÿä¼š:\")\n        print(f\"   å¯¾è±¡ãƒšã‚¢: {coint_analysis['best_pair']}\")\n        print(f\"   ã‚·ã‚°ãƒŠãƒ«é »åº¦: {arbitrage_opportunities['cointegration_pairs_trading']['signal_frequency']:.2f}%\")\n        print(f\"   ã‚¨ãƒ³ãƒˆãƒªãƒ¼é–¾å€¤: Â±{2:.1f}Ïƒ ({lower_threshold:.4f}, {upper_threshold:.4f})\")\n    \n    # 2. ä¸»æˆåˆ†ãƒ™ãƒ¼ã‚¹ã®è£å®š\n    pc1_scores = pca_results['scores']['PC1']\n    pc1_mean = pc1_scores.mean()\n    pc1_std = pc1_scores.std()\n    \n    # ç¬¬1ä¸»æˆåˆ†ã®æ¥µå€¤æ¤œå‡º\n    pc1_upper = pc1_mean + 2 * pc1_std\n    pc1_lower = pc1_mean - 2 * pc1_std\n    \n    pc1_signals = pd.Series(index=pc1_scores.index, dtype=float)\n    pc1_signals[pc1_scores > pc1_upper] = -1\n    pc1_signals[pc1_scores < pc1_lower] = 1\n    pc1_signals[abs(pc1_scores - pc1_mean) < 0.5 * pc1_std] = 0\n    pc1_signals = pc1_signals.ffill().fillna(0)\n    \n    arbitrage_opportunities['pca_factor_trading'] = {\n        'pc1_scores': pc1_scores,\n        'signals': pc1_signals,\n        'thresholds': (pc1_lower, pc1_upper),\n        'signal_frequency': (pc1_signals != 0).sum() / len(pc1_signals) * 100\n    }\n    \n    print(f\"\\nâœ… ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•æ©Ÿä¼š:\")\n    print(f\"   ç¬¬1ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n    print(f\"   ã‚·ã‚°ãƒŠãƒ«é »åº¦: {arbitrage_opportunities['pca_factor_trading']['signal_frequency']:.2f}%\")\n    print(f\"   ã‚¨ãƒ³ãƒˆãƒªãƒ¼é–¾å€¤: Â±{2:.1f}Ïƒ ({pc1_lower:.4f}, {pc1_upper:.4f})\")\n    \n    # 3. ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ©Ÿä¼š\n    rolling_corr_60d = spreads_data['M1_M2_spread'].rolling(window=60).corr(\n        spreads_data['M2_M3_spread']\n    ).dropna()\n    \n    corr_mean = rolling_corr_60d.mean()\n    corr_std = rolling_corr_60d.std()\n    \n    # ç›¸é–¢ã®ç•°å¸¸å€¤ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³ï¼‰ã‚’æ¤œå‡º\n    corr_breakdown_threshold = corr_mean - 2 * corr_std\n    correlation_breakdowns = rolling_corr_60d < corr_breakdown_threshold\n    \n    arbitrage_opportunities['correlation_breakdown'] = {\n        'rolling_correlation': rolling_corr_60d,\n        'breakdown_threshold': corr_breakdown_threshold,\n        'breakdown_periods': correlation_breakdowns,\n        'breakdown_frequency': correlation_breakdowns.sum() / len(correlation_breakdowns) * 100\n    }\n    \n    print(f\"\\nâœ… ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ©Ÿä¼š:\")\n    print(f\"   å¹³å‡ç›¸é–¢: {corr_mean:.4f}\")\n    print(f\"   ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é–¾å€¤: {corr_breakdown_threshold:.4f}\")\n    print(f\"   ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é »åº¦: {arbitrage_opportunities['correlation_breakdown']['breakdown_frequency']:.2f}%\")\n    \n    return arbitrage_opportunities\n\n# çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®åˆ†æ\narbitrage_opps = identify_statistical_arbitrage_opportunities(\n    spreads_data, pca_results, coint_analysis\n)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_arbitrage_opportunities(arbitrage_opps):\n    \"\"\"çµ±è¨ˆçš„è£å®šæ©Ÿä¼šã®å¯è¦–åŒ–ï¼ˆmatplotlibç‰ˆï¼‰\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n    \n    # 1. å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰\n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        coint_data = arbitrage_opps['cointegration_pairs_trading']\n        residuals = coint_data['residuals']\n        signals = coint_data['signals']\n        lower_thresh, upper_thresh = coint_data['thresholds']\n        \n        ax1.plot(residuals.index, residuals, color='blue', linewidth=1, label='Error Correction Term')\n        \n        # é–¾å€¤ç·š\n        ax1.axhline(y=upper_thresh, linestyle='--', color='red', linewidth=1)\n        ax1.axhline(y=lower_thresh, linestyle='--', color='green', linewidth=1)\n        ax1.axhline(y=0, linestyle=':', color='black', linewidth=1)\n        \n        # ã‚·ã‚°ãƒŠãƒ«ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n        buy_signals = signals[signals == 1]\n        sell_signals = signals[signals == -1]\n        \n        if len(buy_signals) > 0:\n            ax1.scatter(buy_signals.index, residuals.loc[buy_signals.index], \n                       color='green', marker='^', s=50, label='Buy Signal', zorder=5)\n        \n        if len(sell_signals) > 0:\n            ax1.scatter(sell_signals.index, residuals.loc[sell_signals.index], \n                       color='red', marker='v', s=50, label='Sell Signal', zorder=5)\n        \n        ax1.set_title('Cointegration Pairs Trading Signals')\n        ax1.set_ylabel('Residuals')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n    \n    # 2. ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•\n    if 'pca_factor_trading' in arbitrage_opps:\n        pca_data = arbitrage_opps['pca_factor_trading']\n        pc1_scores = pca_data['pc1_scores']\n        pc1_signals = pca_data['signals']\n        pc1_lower, pc1_upper = pca_data['thresholds']\n        \n        ax2.plot(pc1_scores.index, pc1_scores, color='purple', linewidth=1, label='PC1 Score')\n        \n        # é–¾å€¤ç·š\n        ax2.axhline(y=pc1_upper, linestyle='--', color='red', linewidth=1)\n        ax2.axhline(y=pc1_lower, linestyle='--', color='green', linewidth=1)\n        ax2.axhline(y=0, linestyle=':', color='black', linewidth=1)\n        \n        ax2.set_title('PCA Factor Trading Signals')\n        ax2.set_ylabel('PC1 Score')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n    \n    # 3. ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³\n    if 'correlation_breakdown' in arbitrage_opps:\n        corr_data = arbitrage_opps['correlation_breakdown']\n        rolling_corr = corr_data['rolling_correlation']\n        breakdown_threshold = corr_data['breakdown_threshold']\n        breakdowns = corr_data['breakdown_periods']\n        \n        ax3.plot(rolling_corr.index, rolling_corr, color='orange', linewidth=1, label='60-day Rolling Correlation')\n        \n        # ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³é–¾å€¤\n        ax3.axhline(y=breakdown_threshold, linestyle='--', color='red', linewidth=2, label='Breakdown Threshold')\n        \n        # ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æœŸé–“ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n        breakdown_dates = breakdowns[breakdowns].index\n        if len(breakdown_dates) > 0:\n            ax3.scatter(breakdown_dates, rolling_corr.loc[breakdown_dates], \n                       color='red', s=30, label='Breakdown', zorder=5)\n        \n        ax3.set_title('Correlation Breakdown Opportunities')\n        ax3.set_ylabel('Correlation Coefficient')\n        ax3.set_xlabel('Date')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n    \n    plt.suptitle('Statistical Arbitrage Opportunities', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# è£å®šæ©Ÿä¼šã®å¯è¦–åŒ–\narbitrage_chart = plot_arbitrage_opportunities(arbitrage_opps)\nplt.show()\n\n# ç”»åƒä¿å­˜\narbitrage_chart.savefig('../../generated_images/adjacent_spreads_arbitrage_opportunities.png', \n                        dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆ†æçµæœã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ…æ‹¬çš„åˆ†æã‚µãƒãƒªãƒ¼\n",
    "def generate_correlation_analysis_summary(pearson_corr, coint_results, pca_results, arbitrage_opps):\n",
    "    \"\"\"ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã®åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nğŸ”— ç›¸é–¢æ§‹é€ :\")\n",
    "    print(f\"  M1-M2 vs M2-M3: {pearson_corr.loc['M1_M2_spread', 'M2_M3_spread']:.3f}\")\n",
    "    print(f\"  M2-M3 vs M3-M4: {pearson_corr.loc['M2_M3_spread', 'M3_M4_spread']:.3f}\")\n",
    "    print(f\"  M1-M2 vs M3-M4: {pearson_corr.loc['M1_M2_spread', 'M3_M4_spread']:.3f}\")\n",
    "    \n",
    "    # æœ€å¼·ç›¸é–¢ãƒšã‚¢\n",
    "    max_corr = 0\n",
    "    max_pair = \"\"\n",
    "    for i in range(len(pearson_corr)):\n",
    "        for j in range(i+1, len(pearson_corr)):\n",
    "            corr_val = abs(pearson_corr.iloc[i, j])\n",
    "            if corr_val > max_corr:\n",
    "                max_corr = corr_val\n",
    "                max_pair = f\"{pearson_corr.index[i]} vs {pearson_corr.columns[j]}\"\n",
    "    \n",
    "    print(f\"  æœ€å¼·ç›¸é–¢ãƒšã‚¢: {max_pair} ({max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å…±å’Œåˆ†åˆ†æ:\")\n",
    "    coint_pairs = 0\n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            coint_pairs += 1\n",
    "            print(f\"  âœ… {pair_name}: p={result['p_value']:.4f} (å…±å’Œåˆ†é–¢ä¿‚ã‚ã‚Š)\")\n",
    "        else:\n",
    "            print(f\"  âŒ {pair_name}: p={result['p_value']:.4f} (å…±å’Œåˆ†é–¢ä¿‚ãªã—)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ä¸»æˆåˆ†åˆ†æ:\")\n",
    "    print(f\"  ç¬¬1ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"  ç¬¬2ä¸»æˆåˆ†å¯„ä¸ç‡: {pca_results['explained_variance_ratio'][1]*100:.2f}%\")\n",
    "    print(f\"  ç´¯ç©å¯„ä¸ç‡ï¼ˆPC1+PC2ï¼‰: {pca_results['cumulative_variance_ratio'][1]*100:.2f}%\")\n",
    "    \n",
    "    # ç¬¬1ä¸»æˆåˆ†ã®æ§‹æˆ\n",
    "    pc1_loadings = pca_results['loadings']['PC1']\n",
    "    dominant_component = pc1_loadings.abs().idxmax()\n",
    "    print(f\"  ç¬¬1ä¸»æˆåˆ†ã®æ”¯é…çš„è¦ç´ : {dominant_component} ({pc1_loadings[dominant_component]:.3f})\")\n",
    "    \n",
    "    print(f\"\\nğŸ’° çµ±è¨ˆçš„è£å®šæ©Ÿä¼š:\")\n",
    "    total_opportunities = 0\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_freq = arbitrage_opps['cointegration_pairs_trading']['signal_frequency']\n",
    "        print(f\"  å…±å’Œåˆ†ãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰: {coint_freq:.2f}% ã®ã‚·ã‚°ãƒŠãƒ«é »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_freq = arbitrage_opps['pca_factor_trading']['signal_frequency']\n",
    "        print(f\"  ä¸»æˆåˆ†ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¼•: {pca_freq:.2f}% ã®ã‚·ã‚°ãƒŠãƒ«é »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        breakdown_freq = arbitrage_opps['correlation_breakdown']['breakdown_frequency']\n",
    "        print(f\"  ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³: {breakdown_freq:.2f}% ã®ç™ºç”Ÿé »åº¦\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æŠ•è³‡æˆ¦ç•¥ã¸ã®ç¤ºå”†:\")\n",
    "    \n",
    "    if coint_pairs > 0:\n",
    "        print(f\"  â€¢ å…±å’Œåˆ†é–¢ä¿‚ã‚’åˆ©ç”¨ã—ãŸå¹³å‡å›å¸°æˆ¦ç•¥ãŒæœ‰åŠ¹\")\n",
    "        print(f\"  â€¢ é•·æœŸå‡è¡¡ã‹ã‚‰ã®ä¹–é›¢ã‚’ç‹™ã£ãŸãƒšã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒå¯èƒ½\")\n",
    "    \n",
    "    if pca_results['explained_variance_ratio'][0] > 0.6:\n",
    "        print(f\"  â€¢ ç¬¬1ä¸»æˆåˆ†ãŒé«˜å¯„ä¸ç‡ï¼ˆ{pca_results['explained_variance_ratio'][0]*100:.1f}%ï¼‰\")\n",
    "        print(f\"  â€¢ ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æ´»ç”¨å¯èƒ½\")\n",
    "    \n",
    "    print(f\"  â€¢ {total_opportunities}ç¨®é¡ã®çµ±è¨ˆçš„è£å®šæˆ¦ç•¥ãŒå®Ÿè£…å¯èƒ½\")\n",
    "    print(f\"  â€¢ ç›¸é–¢æ§‹é€ ã®æ™‚é–“å¤‰å‹•ã‚’æ´»ç”¨ã—ãŸå‹•çš„ãƒ˜ãƒƒã‚¸æˆ¦ç•¥\")\n",
    "    \n",
    "    # ãƒªã‚¹ã‚¯ç®¡ç†ã®æè¨€\n",
    "    avg_corr = np.mean([abs(pearson_corr.iloc[i, j]) for i in range(len(pearson_corr)) \n",
    "                       for j in range(i+1, len(pearson_corr))])\n",
    "    \n",
    "    print(f\"\\nâš ï¸ ãƒªã‚¹ã‚¯ç®¡ç†:\")\n",
    "    print(f\"  â€¢ å¹³å‡ç›¸é–¢: {avg_corr:.3f} - {'é«˜ã„' if avg_corr > 0.5 else 'ä¸­ç¨‹åº¦ã®'}åˆ†æ•£åŠ¹æœ\")\n",
    "    \n",
    "    if max_corr > 0.8:\n",
    "        print(f\"  â€¢ ä¸€éƒ¨ãƒšã‚¢ã§é«˜ç›¸é–¢ï¼ˆ{max_corr:.3f}ï¼‰ - é›†ä¸­ãƒªã‚¹ã‚¯ã«æ³¨æ„\")\n",
    "    \n",
    "    print(f\"  â€¢ ç›¸é–¢ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³æ™‚ã®æå¤±æ‹¡å¤§ãƒªã‚¹ã‚¯ã‚’è€ƒæ…®\")\n",
    "    print(f\"  â€¢ è¤‡æ•°æˆ¦ç•¥ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯åˆ†æ•£æ¨å¥¨\")\n",
    "    \n",
    "    return {\n",
    "        'max_correlation': (max_pair, max_corr),\n",
    "        'cointegrated_pairs': coint_pairs,\n",
    "        'pca_pc1_contribution': pca_results['explained_variance_ratio'][0],\n",
    "        'arbitrage_opportunities': total_opportunities,\n",
    "        'average_correlation': avg_corr\n",
    "    }\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼ç”Ÿæˆ\n",
    "correlation_summary = generate_correlation_analysis_summary(\n",
    "    pearson_corr, coint_results, pca_results, arbitrage_opps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# åˆ†æçµæœã®ä¿å­˜\ndef save_correlation_analysis_results(pearson_corr, spearman_corr, pca_results, \n                                     arbitrage_opps, correlation_summary):\n    \"\"\"ç›¸é–¢åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\"\"\"\n    \n    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n    os.makedirs('../../analysis_results/adjacent_spreads', exist_ok=True)\n    \n    # 1. ç›¸é–¢è¡Œåˆ—\n    pearson_corr.to_csv('../../analysis_results/adjacent_spreads/pearson_correlation.csv', \n                       encoding='utf-8-sig')\n    spearman_corr.to_csv('../../analysis_results/adjacent_spreads/spearman_correlation.csv', \n                        encoding='utf-8-sig')\n    \n    # 2. PCAçµæœ\n    pca_results['loadings'].to_csv('../../analysis_results/adjacent_spreads/pca_loadings.csv', \n                                  encoding='utf-8-sig')\n    pca_results['scores'].to_csv('../../analysis_results/adjacent_spreads/pca_scores.csv', \n                                encoding='utf-8-sig')\n    \n    # 3. å¯„ä¸ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n    variance_df = pd.DataFrame({\n        'Component': [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))],\n        'Explained_Variance_Ratio': pca_results['explained_variance_ratio'],\n        'Cumulative_Variance_Ratio': pca_results['cumulative_variance_ratio']\n    })\n    variance_df.to_csv('../../analysis_results/adjacent_spreads/pca_variance_explained.csv', \n                      encoding='utf-8-sig', index=False)\n    \n    # 4. çµ±è¨ˆçš„è£å®šã‚·ã‚°ãƒŠãƒ«\n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        coint_signals = arbitrage_opps['cointegration_pairs_trading']['signals']\n        coint_signals.to_csv('../../analysis_results/adjacent_spreads/cointegration_signals.csv', \n                            encoding='utf-8-sig')\n    \n    if 'pca_factor_trading' in arbitrage_opps:\n        pca_signals = arbitrage_opps['pca_factor_trading']['signals']\n        pca_signals.to_csv('../../analysis_results/adjacent_spreads/pca_factor_signals.csv', \n                          encoding='utf-8-sig')\n    \n    # 5. åˆ†æã‚µãƒãƒªãƒ¼ï¼ˆJSONï¼‰\n    import json\n    \n    with open('../../analysis_results/adjacent_spreads/correlation_analysis_summary.json', \n              'w', encoding='utf-8') as f:\n        json.dump(correlation_summary, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\nğŸ’¾ ç›¸é–¢åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n    print(f\"  ğŸ“Š Pearsonç›¸é–¢: ../../analysis_results/adjacent_spreads/pearson_correlation.csv\")\n    print(f\"  ğŸ“ˆ Spearmanç›¸é–¢: ../../analysis_results/adjacent_spreads/spearman_correlation.csv\")\n    print(f\"  ğŸ¯ PCAè² è·é‡: ../../analysis_results/adjacent_spreads/pca_loadings.csv\")\n    print(f\"  ğŸ“‰ PCAã‚¹ã‚³ã‚¢: ../../analysis_results/adjacent_spreads/pca_scores.csv\")\n    print(f\"  ğŸ“‹ å¯„ä¸ç‡: ../../analysis_results/adjacent_spreads/pca_variance_explained.csv\")\n    \n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        print(f\"  ğŸ’° å…±å’Œåˆ†ã‚·ã‚°ãƒŠãƒ«: ../../analysis_results/adjacent_spreads/cointegration_signals.csv\")\n    \n    if 'pca_factor_trading' in arbitrage_opps:\n        print(f\"  ğŸ”§ PCAã‚·ã‚°ãƒŠãƒ«: ../../analysis_results/adjacent_spreads/pca_factor_signals.csv\")\n    \n    print(f\"  ğŸ“ åˆ†æã‚µãƒãƒªãƒ¼: ../../analysis_results/adjacent_spreads/correlation_analysis_summary.json\")\n\n# åˆ†æçµæœä¿å­˜\nsave_correlation_analysis_results(\n    pearson_corr, spearman_corr, pca_results, arbitrage_opps, correlation_summary\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "ã“ã®ç›¸é–¢ãƒ»å…±å’Œåˆ†åˆ†æã«ã‚ˆã‚Šã€éš£æœˆé–“ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æŠŠæ¡ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ä¸»è¦ç™ºè¦‹äº‹é …\n",
    "1. **ç›¸é–¢æ§‹é€ **: éš£æ¥ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰é–“ã«ä¸­ç¨‹åº¦ã‹ã‚‰å¼·ã„ç›¸é–¢é–¢ä¿‚\n",
    "2. **å…±å’Œåˆ†é–¢ä¿‚**: ä¸€éƒ¨ãƒšã‚¢ã§é•·æœŸå‡è¡¡é–¢ä¿‚ã‚’ç¢ºèª\n",
    "3. **ä¸»æˆåˆ†æ§‹é€ **: ç¬¬1ä¸»æˆåˆ†ãŒå…¨å¤‰å‹•ã®å¤§éƒ¨åˆ†ã‚’èª¬æ˜\n",
    "4. **è£å®šæ©Ÿä¼š**: è¤‡æ•°ã®çµ±è¨ˆçš„è£å®šæˆ¦ç•¥ãŒå®Ÿè£…å¯èƒ½\n",
    "\n",
    "### æ¬¡ã®åˆ†æã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°**: GARCHç³»ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯åˆ†æ\n",
    "2. **æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬**: ã‚ˆã‚Šé«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨äºˆæ¸¬\n",
    "3. **å–å¼•æˆ¦ç•¥æ§‹ç¯‰**: å®Ÿéš›ã®å£²è²·ãƒ«ãƒ¼ãƒ«ã¨ãƒªã‚¹ã‚¯ç®¡ç†\n",
    "4. **ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã§ã®æˆ¦ç•¥æ¤œè¨¼\n",
    "5. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡**: ãƒªã‚¹ã‚¯èª¿æ•´å¾Œãƒªã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡\n",
    "\n",
    "æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ `3_adjacent_spreads_volatility_modeling.ipynb` ã§ã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒªã‚¹ã‚¯ç‰¹æ€§ã®è©³ç´°åˆ†æã‚’å®Ÿæ–½ã—ã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}