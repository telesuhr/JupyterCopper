{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LME銅先物 隣月間スプレッド 相関・共和分分析\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、隣月間スプレッド（M1-M2、M2-M3、M3-M4）の相関関係と共和分関係を詳細に分析します。\n",
    "\n",
    "### 分析目標\n",
    "- スプレッド間の相関構造の理解\n",
    "- 共和分関係（長期均衡関係）の検出\n",
    "- ペアトレードの機会特定\n",
    "- リスク分散効果の評価\n",
    "\n",
    "### 期待される成果\n",
    "- スプレッド間の相関パターンとその時間変動\n",
    "- 共和分関係による長期均衡メカニズムの発見\n",
    "- 統計的裁定機会の特定\n",
    "- ポートフォリオ構築への示唆"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 必要ライブラリのインポート\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psycopg2\nfrom sqlalchemy import create_engine\nimport warnings\nfrom datetime import datetime, timedelta\n# import plotly.graph_objects as go\n# import plotly.express as px\n# from plotly.subplots import make_subplots\n# Plotly replaced with matplotlib for compatibility\nfrom scipy import stats\nimport os\n\n# 統計・時系列分析 - 問題のある関数を個別にインポート\ntry:\n    from statsmodels.tsa.stattools import coint, adfuller\n    from statsmodels.stats.diagnostic import acorr_ljungbox\n    from statsmodels.tsa.vector_ar.vecm import coint_johansen\n    from statsmodels.regression.linear_model import OLS\n    # statsmodels.apiの代わりに、必要な関数のみインポート\n    from statsmodels.tools import add_constant\n    print(\"✅ Statsmodels インポート成功\")\nexcept ImportError as e:\n    print(f\"⚠️ Statsmodels インポートエラー: {e}\")\n    print(\"一部の高度な統計分析機能が制限されます\")\n    # 基本的な代替関数を定義\n    from scipy.stats import linregress\n    def add_constant(x):\n        \"\"\"statsmodels.add_constantの代替\"\"\"\n        return np.column_stack([np.ones(len(x)), x])\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベース接続とデータ取得\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQLデータベースへの接続を取得\"\"\"\n",
    "    try:\n",
    "        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"データベース接続エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_calculate_spreads():\n",
    "    \"\"\"スプレッドデータの取得と計算\"\"\"\n",
    "    engine = get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        trade_date,\n",
    "        contract_month,\n",
    "        close_price,\n",
    "        volume,\n",
    "        open_interest\n",
    "    FROM lme_copper_futures \n",
    "    WHERE contract_month IN (1, 2, 3, 4)\n",
    "        AND close_price IS NOT NULL\n",
    "        AND close_price > 0\n",
    "    ORDER BY trade_date, contract_month\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "    \n",
    "    # ピボットしてスプレッド計算\n",
    "    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n",
    "    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n",
    "    \n",
    "    # スプレッド計算\n",
    "    spreads_df = pd.DataFrame(index=pivot_df.index)\n",
    "    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n",
    "    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n",
    "    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n",
    "    \n",
    "    # 価格データも保持\n",
    "    spreads_df['M1_price'] = pivot_df['M1']\n",
    "    spreads_df['M2_price'] = pivot_df['M2']\n",
    "    spreads_df['M3_price'] = pivot_df['M3']\n",
    "    spreads_df['M4_price'] = pivot_df['M4']\n",
    "    \n",
    "    return spreads_df.dropna()\n",
    "\n",
    "# データ取得\n",
    "spreads_data = load_and_calculate_spreads()\n",
    "print(f\"✅ データ取得完了: {len(spreads_data):,} レコード\")\n",
    "print(f\"📅 分析期間: {spreads_data.index.min()} ～ {spreads_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本相関分析"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_basic_correlations(df):\n    \"\"\"基本相関分析\"\"\"\n    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n    \n    # 相関行列計算\n    correlation_matrix = df[spread_columns].corr()\n    \n    print(\"📊 隣月間スプレッド相関行列:\")\n    print(\"=\" * 40)\n    print(correlation_matrix.round(4))\n    \n    # Spearman順位相関も計算\n    spearman_corr = df[spread_columns].corr(method='spearman')\n    \n    print(\"\\n📈 Spearman順位相関行列:\")\n    print(\"=\" * 40)\n    print(spearman_corr.round(4))\n    \n    return correlation_matrix, spearman_corr\n\ndef plot_correlation_heatmap(corr_matrix, spearman_corr):\n    \"\"\"相関ヒートマップの作成（matplotlib版）\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n    \n    # Pearson相関ヒートマップ\n    im1 = ax1.imshow(corr_matrix.values, cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n    ax1.set_xticks(range(len(spread_names)))\n    ax1.set_yticks(range(len(spread_names)))\n    ax1.set_xticklabels(spread_names)\n    ax1.set_yticklabels(spread_names)\n    ax1.set_title('Pearson Correlation', fontsize=14)\n    \n    # 数値をセルに表示\n    for i in range(len(spread_names)):\n        for j in range(len(spread_names)):\n            ax1.text(j, i, f'{corr_matrix.iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=12)\n    \n    # Spearman相関ヒートマップ\n    im2 = ax2.imshow(spearman_corr.values, cmap='RdBu', vmin=-1, vmax=1, aspect='auto')\n    ax2.set_xticks(range(len(spread_names)))\n    ax2.set_yticks(range(len(spread_names)))\n    ax2.set_xticklabels(spread_names)\n    ax2.set_yticklabels(spread_names)\n    ax2.set_title('Spearman Rank Correlation', fontsize=14)\n    \n    # 数値をセルに表示\n    for i in range(len(spread_names)):\n        for j in range(len(spread_names)):\n            ax2.text(j, i, f'{spearman_corr.iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=12)\n    \n    # カラーバー追加\n    plt.colorbar(im1, ax=ax1, shrink=0.6)\n    plt.colorbar(im2, ax=ax2, shrink=0.6)\n    \n    plt.suptitle('Adjacent Month Spreads Correlation Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# 基本相関分析実行\npearson_corr, spearman_corr = analyze_basic_correlations(spreads_data)\n\n# 相関ヒートマップ表示\ncorr_chart = plot_correlation_heatmap(pearson_corr, spearman_corr)\nplt.show()\n\n# 画像保存\nos.makedirs('../../generated_images', exist_ok=True)\ncorr_chart.savefig('../../generated_images/adjacent_spreads_correlation_heatmap.png', \n                   dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 時間変動相関分析"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def calculate_rolling_correlations(df, window=60):\n    \"\"\"ローリング相関の計算\"\"\"\n    rolling_corr_df = pd.DataFrame(index=df.index)\n    \n    # M1-M2 vs M2-M3の相関\n    rolling_corr_df['M1M2_vs_M2M3'] = df['M1_M2_spread'].rolling(window=window).corr(\n        df['M2_M3_spread']\n    )\n    \n    # M2-M3 vs M3-M4の相関\n    rolling_corr_df['M2M3_vs_M3M4'] = df['M2_M3_spread'].rolling(window=window).corr(\n        df['M3_M4_spread']\n    )\n    \n    # M1-M2 vs M3-M4の相関\n    rolling_corr_df['M1M2_vs_M3M4'] = df['M1_M2_spread'].rolling(window=window).corr(\n        df['M3_M4_spread']\n    )\n    \n    return rolling_corr_df.dropna()\n\ndef plot_rolling_correlations(rolling_corr_df):\n    \"\"\"ローリング相関のプロット（matplotlib版）\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    correlations = [\n        ('M1M2_vs_M2M3', 'M1-M2 vs M2-M3', 'blue'),\n        ('M2M3_vs_M3M4', 'M2-M3 vs M3-M4', 'red'),\n        ('M1M2_vs_M3M4', 'M1-M2 vs M3-M4', 'green')\n    ]\n    \n    for col, name, color in correlations:\n        ax.plot(rolling_corr_df.index, rolling_corr_df[col], \n               label=name, color=color, linewidth=2)\n    \n    # 基準線を追加\n    ax.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax.axhline(y=0.5, linestyle=':', color='gray', linewidth=1)\n    ax.axhline(y=-0.5, linestyle=':', color='gray', linewidth=1)\n    \n    ax.set_title('Adjacent Month Spreads Rolling Correlation (60 days)', fontsize=16)\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Correlation Coefficient')\n    ax.set_ylim(-1, 1)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return fig\n\n# ローリング相関計算\nrolling_corr_data = calculate_rolling_correlations(spreads_data, window=60)\n\nprint(f\"📈 ローリング相関統計（60日窓）:\")\nprint(\"=\" * 50)\nprint(rolling_corr_data.describe().round(4))\n\n# ローリング相関チャート\nrolling_corr_chart = plot_rolling_correlations(rolling_corr_data)\nplt.show()\n\n# 画像保存\nrolling_corr_chart.savefig('../../generated_images/adjacent_spreads_rolling_correlation.png', \n                           dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 共和分分析（長期均衡関係）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cointegration(df):\n",
    "    \"\"\"共和分検定の実行\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"🔬 共和分検定結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    coint_results = {}\n",
    "    \n",
    "    # ペアワイズ共和分検定\n",
    "    pairs = [\n",
    "        (('M1_M2_spread', 'M2_M3_spread'), ('M1-M2', 'M2-M3')),\n",
    "        (('M2_M3_spread', 'M3_M4_spread'), ('M2-M3', 'M3-M4')),\n",
    "        (('M1_M2_spread', 'M3_M4_spread'), ('M1-M2', 'M3-M4'))\n",
    "    ]\n",
    "    \n",
    "    for (col1, col2), (name1, name2) in pairs:\n",
    "        # Engle-Granger共和分検定\n",
    "        coint_t, p_value, critical_values = coint(df[col1], df[col2])\n",
    "        \n",
    "        coint_results[f'{name1}_vs_{name2}'] = {\n",
    "            'test_statistic': coint_t,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name1} vs {name2}:\")\n",
    "        print(f\"  検定統計量: {coint_t:.4f}\")\n",
    "        print(f\"  p値: {p_value:.4f}\")\n",
    "        print(f\"  結果: {'共和分関係あり' if p_value < 0.05 else '共和分関係なし'}\")\n",
    "        print(f\"  臨界値 1%: {critical_values[0]:.4f}\")\n",
    "        print(f\"  臨界値 5%: {critical_values[1]:.4f}\")\n",
    "        print(f\"  臨界値 10%: {critical_values[2]:.4f}\")\n",
    "    \n",
    "    return coint_results\n",
    "\n",
    "def johansen_cointegration_test(df):\n",
    "    \"\"\"Johansen共和分検定（多変量）\"\"\"\n",
    "    spread_data = df[['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']].dropna()\n",
    "    \n",
    "    print(f\"\\n🔍 Johansen共和分検定（多変量）:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Johansen検定実行\n",
    "    johansen_result = coint_johansen(spread_data, det_order=0, k_ar_diff=1)\n",
    "    \n",
    "    print(f\"Trace統計量:\")\n",
    "    for i, (trace_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr1, johansen_result.cvt[:, 0], \n",
    "            johansen_result.cvt[:, 1], johansen_result.cvt[:, 2])\n",
    "    ):\n",
    "        print(f\"  r≤{i}: {trace_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "        \n",
    "        if trace_stat > cv_95:\n",
    "            print(f\"    → r>{i}の共和分関係あり（95%水準）\")\n",
    "        else:\n",
    "            print(f\"    → r≤{i}の共和分関係\")\n",
    "    \n",
    "    print(f\"\\nMaximum Eigenvalue統計量:\")\n",
    "    for i, (max_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr2, johansen_result.cvm[:, 0], \n",
    "            johansen_result.cvm[:, 1], johansen_result.cvm[:, 2])\n",
    "    ):\n",
    "        print(f\"  r={i}: {max_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "    \n",
    "    return johansen_result\n",
    "\n",
    "# 共和分検定実行\n",
    "coint_results = test_cointegration(spreads_data)\n",
    "johansen_result = johansen_cointegration_test(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_cointegration_relationships(df, coint_results):\n    \"\"\"共和分関係の詳細分析\"\"\"\n    \n    print(f\"\\n📊 共和分関係の詳細分析:\")\n    print(\"=\" * 60)\n    \n    # 有意な共和分関係を持つペアを特定\n    significant_pairs = []\n    \n    for pair_name, result in coint_results.items():\n        if result['p_value'] < 0.05:\n            significant_pairs.append(pair_name)\n            print(f\"✅ {pair_name}: 共和分関係あり（p={result['p_value']:.4f}）\")\n    \n    if not significant_pairs:\n        print(\"❌ 有意な共和分関係は検出されませんでした\")\n        return None\n    \n    # 最も強い共和分関係のペアで詳細分析\n    best_pair_name = min(coint_results.keys(), key=lambda x: coint_results[x]['p_value'])\n    print(f\"\\n🎯 最強共和分関係: {best_pair_name}\")\n    \n    # ペア名から実際のカラム名を特定\n    if 'M1-M2_vs_M2-M3' in best_pair_name:\n        col1, col2 = 'M1_M2_spread', 'M2_M3_spread'\n        name1, name2 = 'M1-M2', 'M2-M3'\n    elif 'M2-M3_vs_M3-M4' in best_pair_name:\n        col1, col2 = 'M2_M3_spread', 'M3_M4_spread'\n        name1, name2 = 'M2-M3', 'M3-M4'\n    else:\n        col1, col2 = 'M1_M2_spread', 'M3_M4_spread'\n        name1, name2 = 'M1-M2', 'M3-M4'\n    \n    try:\n        # 共和分回帰\n        y = df[col1]\n        x = add_constant(df[col2])\n        \n        model = OLS(y, x).fit()\n        \n        print(f\"\\n回帰式: {name1} = {model.params[0]:.4f} + {model.params[1]:.4f} * {name2}\")\n        print(f\"R²: {model.rsquared:.4f}\")\n        print(f\"回帰係数のt値: {model.tvalues[1]:.4f}\")\n        \n        # 残差（誤差修正項）の計算\n        residuals = model.resid\n        \n        # 残差の定常性検定\n        adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n        \n        print(f\"\\n残差の定常性検定（ADF）:\")\n        print(f\"  統計量: {adf_stat:.4f}\")\n        print(f\"  p値: {adf_p:.4f}\")\n        print(f\"  結果: {'定常' if adf_p < 0.05 else '非定常'}\")\n        \n        return {\n            'best_pair': (col1, col2),\n            'regression_model': model,\n            'residuals': residuals,\n            'adf_test': (adf_stat, adf_p)\n        }\n        \n    except Exception as e:\n        print(f\"⚠️ 共和分回帰分析でエラー: {e}\")\n        print(\"scipy.statsを使用した簡易回帰分析に切り替えます\")\n        \n        # 代替分析（scipyを使用）\n        from scipy.stats import linregress\n        \n        slope, intercept, r_value, p_value, std_err = linregress(df[col2], df[col1])\n        \n        print(f\"\\n回帰式: {name1} = {intercept:.4f} + {slope:.4f} * {name2}\")\n        print(f\"R²: {r_value**2:.4f}\")\n        print(f\"p値: {p_value:.4f}\")\n        \n        # 残差計算\n        predicted = intercept + slope * df[col2]\n        residuals = df[col1] - predicted\n        \n        # 残差の定常性検定\n        adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n        \n        print(f\"\\n残差の定常性検定（ADF）:\")\n        print(f\"  統計量: {adf_stat:.4f}\")\n        print(f\"  p値: {adf_p:.4f}\")\n        print(f\"  結果: {'定常' if adf_p < 0.05 else '非定常'}\")\n        \n        # 簡易モデルオブジェクト作成\n        class SimpleModel:\n            def __init__(self, intercept, slope, r_squared):\n                self.params = [intercept, slope]\n                self.rsquared = r_squared\n        \n        model = SimpleModel(intercept, slope, r_value**2)\n        \n        return {\n            'best_pair': (col1, col2),\n            'regression_model': model,\n            'residuals': residuals,\n            'adf_test': (adf_stat, adf_p)\n        }\n\n# 共和分関係の詳細分析\ncoint_analysis = analyze_cointegration_relationships(spreads_data, coint_results)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_cointegration_analysis(df, coint_analysis):\n    \"\"\"共和分分析の可視化（matplotlib版）\"\"\"\n    if coint_analysis is None:\n        print(\"共和分関係が検出されなかったため、プロットをスキップします\")\n        return None\n    \n    col1, col2 = coint_analysis['best_pair']\n    residuals = coint_analysis['residuals']\n    model = coint_analysis['regression_model']\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. 時系列プロット\n    ax1.plot(df.index, df[col1], label=col1.replace('_spread', '').replace('_', '-'), color='blue')\n    ax1.plot(df.index, df[col2], label=col2.replace('_spread', '').replace('_', '-'), color='red')\n    ax1.set_title('Spread Pair Time Series')\n    ax1.set_ylabel('Spread (USD/t)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. 散布図と回帰線\n    ax2.scatter(df[col2], df[col1], alpha=0.6, s=10, color='lightblue', label='Data Points')\n    \n    # 回帰線\n    x_range = np.linspace(df[col2].min(), df[col2].max(), 100)\n    y_pred = model.params[0] + model.params[1] * x_range\n    ax2.plot(x_range, y_pred, color='red', linewidth=2, label='Regression Line')\n    \n    ax2.set_title('Spread Scatter Plot with Regression Line')\n    ax2.set_xlabel(f\"{col2.replace('_spread', '').replace('_', '-')} Spread\")\n    ax2.set_ylabel(f\"{col1.replace('_spread', '').replace('_', '-')} Spread\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # 3. 残差の時系列\n    ax3.plot(df.index, residuals, color='green', linewidth=1, label='Error Correction Term')\n    ax3.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax3.set_title('Error Correction Term (Residuals) Over Time')\n    ax3.set_xlabel('Date')\n    ax3.set_ylabel('Residuals')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. 残差のヒストグラム\n    ax4.hist(residuals, bins=50, alpha=0.7, color='purple', label='Residual Distribution')\n    ax4.set_title('Residual Distribution')\n    ax4.set_xlabel('Residuals')\n    ax4.set_ylabel('Frequency')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.suptitle('Cointegration Analysis - Long-term Equilibrium Relationship', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# 共和分分析の可視化\nif coint_analysis:\n    coint_chart = plot_cointegration_analysis(spreads_data, coint_analysis)\n    if coint_chart:\n        plt.show()\n        \n        # 画像保存\n        coint_chart.savefig('../../generated_images/adjacent_spreads_cointegration_analysis.png', \n                           dpi=300, bbox_inches='tight')\n        plt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 主成分分析（PCA）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def perform_pca_analysis(df):\n    \"\"\"主成分分析の実行\"\"\"\n    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n    spread_data = df[spread_columns].dropna()\n    \n    # データの標準化\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(spread_data)\n    \n    # PCA実行\n    pca = PCA()\n    pca_result = pca.fit_transform(scaled_data)\n    \n    print(f\"📊 主成分分析結果:\")\n    print(\"=\" * 50)\n    \n    # 寄与率と累積寄与率\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n    \n    for i, (var_ratio, cum_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n        print(f\"PC{i+1}: 寄与率 {var_ratio:.4f} ({var_ratio*100:.2f}%), 累積寄与率 {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n    \n    # 主成分負荷量\n    print(f\"\\n📈 主成分負荷量:\")\n    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n    \n    loadings_df = pd.DataFrame(\n        loadings,\n        index=['M1-M2', 'M2-M3', 'M3-M4'],\n        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n    )\n    \n    print(loadings_df.round(4))\n    \n    # 主成分スコア\n    pca_scores_df = pd.DataFrame(\n        pca_result,\n        index=spread_data.index,\n        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n    )\n    \n    return {\n        'pca_model': pca,\n        'explained_variance_ratio': explained_variance_ratio,\n        'cumulative_variance_ratio': cumulative_variance_ratio,\n        'loadings': loadings_df,\n        'scores': pca_scores_df,\n        'scaler': scaler\n    }\n\ndef plot_pca_analysis(pca_results):\n    \"\"\"PCA分析の可視化（matplotlib版）\"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. スクリープロット\n    pcs = [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))]\n    explained_var = pca_results['explained_variance_ratio'] * 100\n    cumulative_var = pca_results['cumulative_variance_ratio'] * 100\n    \n    bars = ax1.bar(pcs, explained_var, color='lightblue', alpha=0.7, label='Variance Explained')\n    ax1_twin = ax1.twinx()\n    ax1_twin.plot(pcs, cumulative_var, color='red', marker='o', linewidth=2, label='Cumulative Variance')\n    \n    ax1.set_title('Scree Plot')\n    ax1.set_xlabel('Principal Component')\n    ax1.set_ylabel('Variance Explained (%)', color='blue')\n    ax1_twin.set_ylabel('Cumulative Variance (%)', color='red')\n    ax1.legend(loc='upper left')\n    ax1_twin.legend(loc='upper right')\n    \n    # 2. 主成分負荷量のヒートマップ\n    im = ax2.imshow(pca_results['loadings'].values, cmap='RdBu', aspect='auto')\n    ax2.set_xticks(range(len(pca_results['loadings'].columns)))\n    ax2.set_yticks(range(len(pca_results['loadings'].index)))\n    ax2.set_xticklabels(pca_results['loadings'].columns)\n    ax2.set_yticklabels(pca_results['loadings'].index)\n    ax2.set_title('Principal Component Loadings')\n    \n    # 数値をセルに表示\n    for i in range(len(pca_results['loadings'].index)):\n        for j in range(len(pca_results['loadings'].columns)):\n            ax2.text(j, i, f'{pca_results[\"loadings\"].iloc[i, j]:.3f}', \n                    ha='center', va='center', color='black', fontsize=10)\n    \n    plt.colorbar(im, ax=ax2, shrink=0.6)\n    \n    # 3. 主成分スコア散布図\n    years = pca_results['scores'].index.year\n    scatter = ax3.scatter(pca_results['scores']['PC1'], pca_results['scores']['PC2'], \n                         c=years, cmap='viridis', alpha=0.7, s=20)\n    ax3.set_title('PC1 vs PC2 Scatter Plot')\n    ax3.set_xlabel('First Principal Component')\n    ax3.set_ylabel('Second Principal Component')\n    plt.colorbar(scatter, ax=ax3, label='Year')\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. 第1主成分の時系列\n    ax4.plot(pca_results['scores'].index, pca_results['scores']['PC1'], \n            color='blue', linewidth=1, label='PC1 Score')\n    ax4.axhline(y=0, linestyle='--', color='black', linewidth=1)\n    ax4.set_title('First Principal Component Time Series')\n    ax4.set_xlabel('Date')\n    ax4.set_ylabel('PC1 Score')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.suptitle('Adjacent Month Spreads Principal Component Analysis', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# PCA分析実行\npca_results = perform_pca_analysis(spreads_data)\n\n# PCA可視化\npca_chart = plot_pca_analysis(pca_results)\nplt.show()\n\n# 画像保存\npca_chart.savefig('../../generated_images/adjacent_spreads_pca_analysis.png', \n                  dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 統計的裁定機会の特定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def identify_statistical_arbitrage_opportunities(df, pca_results, coint_analysis):\n    \"\"\"統計的裁定機会の特定\"\"\"\n    \n    print(f\"🎯 統計的裁定機会分析:\")\n    print(\"=\" * 60)\n    \n    arbitrage_opportunities = {}\n    \n    # 1. 共和分ベースの裁定\n    if coint_analysis and coint_analysis['adf_test'][1] < 0.05:\n        residuals = coint_analysis['residuals']\n        \n        # 残差の統計量\n        residual_mean = residuals.mean()\n        residual_std = residuals.std()\n        \n        # エントリー・エグジットシグナル\n        upper_threshold = residual_mean + 2 * residual_std\n        lower_threshold = residual_mean - 2 * residual_std\n        \n        # シグナル生成\n        signals = pd.Series(index=residuals.index, dtype=float)\n        signals[residuals > upper_threshold] = -1  # ショートシグナル\n        signals[residuals < lower_threshold] = 1   # ロングシグナル\n        signals[abs(residuals - residual_mean) < 0.5 * residual_std] = 0  # エグジット\n        \n        # 前値で埋める（新しい方法）\n        signals = signals.ffill().fillna(0)\n        \n        arbitrage_opportunities['cointegration_pairs_trading'] = {\n            'pair': coint_analysis['best_pair'],\n            'residuals': residuals,\n            'signals': signals,\n            'thresholds': (lower_threshold, upper_threshold),\n            'signal_frequency': (signals != 0).sum() / len(signals) * 100\n        }\n        \n        print(f\"✅ 共和分ペアトレード機会:\")\n        print(f\"   対象ペア: {coint_analysis['best_pair']}\")\n        print(f\"   シグナル頻度: {arbitrage_opportunities['cointegration_pairs_trading']['signal_frequency']:.2f}%\")\n        print(f\"   エントリー閾値: ±{2:.1f}σ ({lower_threshold:.4f}, {upper_threshold:.4f})\")\n    \n    # 2. 主成分ベースの裁定\n    pc1_scores = pca_results['scores']['PC1']\n    pc1_mean = pc1_scores.mean()\n    pc1_std = pc1_scores.std()\n    \n    # 第1主成分の極値検出\n    pc1_upper = pc1_mean + 2 * pc1_std\n    pc1_lower = pc1_mean - 2 * pc1_std\n    \n    pc1_signals = pd.Series(index=pc1_scores.index, dtype=float)\n    pc1_signals[pc1_scores > pc1_upper] = -1\n    pc1_signals[pc1_scores < pc1_lower] = 1\n    pc1_signals[abs(pc1_scores - pc1_mean) < 0.5 * pc1_std] = 0\n    pc1_signals = pc1_signals.ffill().fillna(0)\n    \n    arbitrage_opportunities['pca_factor_trading'] = {\n        'pc1_scores': pc1_scores,\n        'signals': pc1_signals,\n        'thresholds': (pc1_lower, pc1_upper),\n        'signal_frequency': (pc1_signals != 0).sum() / len(pc1_signals) * 100\n    }\n    \n    print(f\"\\n✅ 主成分ファクター取引機会:\")\n    print(f\"   第1主成分寄与率: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n    print(f\"   シグナル頻度: {arbitrage_opportunities['pca_factor_trading']['signal_frequency']:.2f}%\")\n    print(f\"   エントリー閾値: ±{2:.1f}σ ({pc1_lower:.4f}, {pc1_upper:.4f})\")\n    \n    # 3. 相関ブレイクダウン機会\n    rolling_corr_60d = spreads_data['M1_M2_spread'].rolling(window=60).corr(\n        spreads_data['M2_M3_spread']\n    ).dropna()\n    \n    corr_mean = rolling_corr_60d.mean()\n    corr_std = rolling_corr_60d.std()\n    \n    # 相関の異常値（ブレイクダウン）を検出\n    corr_breakdown_threshold = corr_mean - 2 * corr_std\n    correlation_breakdowns = rolling_corr_60d < corr_breakdown_threshold\n    \n    arbitrage_opportunities['correlation_breakdown'] = {\n        'rolling_correlation': rolling_corr_60d,\n        'breakdown_threshold': corr_breakdown_threshold,\n        'breakdown_periods': correlation_breakdowns,\n        'breakdown_frequency': correlation_breakdowns.sum() / len(correlation_breakdowns) * 100\n    }\n    \n    print(f\"\\n✅ 相関ブレイクダウン機会:\")\n    print(f\"   平均相関: {corr_mean:.4f}\")\n    print(f\"   ブレイクダウン閾値: {corr_breakdown_threshold:.4f}\")\n    print(f\"   ブレイクダウン頻度: {arbitrage_opportunities['correlation_breakdown']['breakdown_frequency']:.2f}%\")\n    \n    return arbitrage_opportunities\n\n# 統計的裁定機会の分析\narbitrage_opps = identify_statistical_arbitrage_opportunities(\n    spreads_data, pca_results, coint_analysis\n)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_arbitrage_opportunities(arbitrage_opps):\n    \"\"\"統計的裁定機会の可視化（matplotlib版）\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n    \n    # 1. 共和分ペアトレード\n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        coint_data = arbitrage_opps['cointegration_pairs_trading']\n        residuals = coint_data['residuals']\n        signals = coint_data['signals']\n        lower_thresh, upper_thresh = coint_data['thresholds']\n        \n        ax1.plot(residuals.index, residuals, color='blue', linewidth=1, label='Error Correction Term')\n        \n        # 閾値線\n        ax1.axhline(y=upper_thresh, linestyle='--', color='red', linewidth=1)\n        ax1.axhline(y=lower_thresh, linestyle='--', color='green', linewidth=1)\n        ax1.axhline(y=0, linestyle=':', color='black', linewidth=1)\n        \n        # シグナルをハイライト\n        buy_signals = signals[signals == 1]\n        sell_signals = signals[signals == -1]\n        \n        if len(buy_signals) > 0:\n            ax1.scatter(buy_signals.index, residuals.loc[buy_signals.index], \n                       color='green', marker='^', s=50, label='Buy Signal', zorder=5)\n        \n        if len(sell_signals) > 0:\n            ax1.scatter(sell_signals.index, residuals.loc[sell_signals.index], \n                       color='red', marker='v', s=50, label='Sell Signal', zorder=5)\n        \n        ax1.set_title('Cointegration Pairs Trading Signals')\n        ax1.set_ylabel('Residuals')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n    \n    # 2. 主成分ファクター取引\n    if 'pca_factor_trading' in arbitrage_opps:\n        pca_data = arbitrage_opps['pca_factor_trading']\n        pc1_scores = pca_data['pc1_scores']\n        pc1_signals = pca_data['signals']\n        pc1_lower, pc1_upper = pca_data['thresholds']\n        \n        ax2.plot(pc1_scores.index, pc1_scores, color='purple', linewidth=1, label='PC1 Score')\n        \n        # 閾値線\n        ax2.axhline(y=pc1_upper, linestyle='--', color='red', linewidth=1)\n        ax2.axhline(y=pc1_lower, linestyle='--', color='green', linewidth=1)\n        ax2.axhline(y=0, linestyle=':', color='black', linewidth=1)\n        \n        ax2.set_title('PCA Factor Trading Signals')\n        ax2.set_ylabel('PC1 Score')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n    \n    # 3. 相関ブレイクダウン\n    if 'correlation_breakdown' in arbitrage_opps:\n        corr_data = arbitrage_opps['correlation_breakdown']\n        rolling_corr = corr_data['rolling_correlation']\n        breakdown_threshold = corr_data['breakdown_threshold']\n        breakdowns = corr_data['breakdown_periods']\n        \n        ax3.plot(rolling_corr.index, rolling_corr, color='orange', linewidth=1, label='60-day Rolling Correlation')\n        \n        # ブレイクダウン閾値\n        ax3.axhline(y=breakdown_threshold, linestyle='--', color='red', linewidth=2, label='Breakdown Threshold')\n        \n        # ブレイクダウン期間をハイライト\n        breakdown_dates = breakdowns[breakdowns].index\n        if len(breakdown_dates) > 0:\n            ax3.scatter(breakdown_dates, rolling_corr.loc[breakdown_dates], \n                       color='red', s=30, label='Breakdown', zorder=5)\n        \n        ax3.set_title('Correlation Breakdown Opportunities')\n        ax3.set_ylabel('Correlation Coefficient')\n        ax3.set_xlabel('Date')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n    \n    plt.suptitle('Statistical Arbitrage Opportunities', fontsize=16)\n    plt.tight_layout()\n    \n    return fig\n\n# 裁定機会の可視化\narbitrage_chart = plot_arbitrage_opportunities(arbitrage_opps)\nplt.show()\n\n# 画像保存\narbitrage_chart.savefig('../../generated_images/adjacent_spreads_arbitrage_opportunities.png', \n                        dpi=300, bbox_inches='tight')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 分析結果サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包括的分析サマリー\n",
    "def generate_correlation_analysis_summary(pearson_corr, coint_results, pca_results, arbitrage_opps):\n",
    "    \"\"\"相関・共和分分析の包括的サマリー\"\"\"\n",
    "    \n",
    "    print(\"📋 隣月間スプレッド相関・共和分分析サマリー\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n🔗 相関構造:\")\n",
    "    print(f\"  M1-M2 vs M2-M3: {pearson_corr.loc['M1_M2_spread', 'M2_M3_spread']:.3f}\")\n",
    "    print(f\"  M2-M3 vs M3-M4: {pearson_corr.loc['M2_M3_spread', 'M3_M4_spread']:.3f}\")\n",
    "    print(f\"  M1-M2 vs M3-M4: {pearson_corr.loc['M1_M2_spread', 'M3_M4_spread']:.3f}\")\n",
    "    \n",
    "    # 最強相関ペア\n",
    "    max_corr = 0\n",
    "    max_pair = \"\"\n",
    "    for i in range(len(pearson_corr)):\n",
    "        for j in range(i+1, len(pearson_corr)):\n",
    "            corr_val = abs(pearson_corr.iloc[i, j])\n",
    "            if corr_val > max_corr:\n",
    "                max_corr = corr_val\n",
    "                max_pair = f\"{pearson_corr.index[i]} vs {pearson_corr.columns[j]}\"\n",
    "    \n",
    "    print(f\"  最強相関ペア: {max_pair} ({max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\n🎯 共和分分析:\")\n",
    "    coint_pairs = 0\n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            coint_pairs += 1\n",
    "            print(f\"  ✅ {pair_name}: p={result['p_value']:.4f} (共和分関係あり)\")\n",
    "        else:\n",
    "            print(f\"  ❌ {pair_name}: p={result['p_value']:.4f} (共和分関係なし)\")\n",
    "    \n",
    "    print(f\"\\n📊 主成分分析:\")\n",
    "    print(f\"  第1主成分寄与率: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"  第2主成分寄与率: {pca_results['explained_variance_ratio'][1]*100:.2f}%\")\n",
    "    print(f\"  累積寄与率（PC1+PC2）: {pca_results['cumulative_variance_ratio'][1]*100:.2f}%\")\n",
    "    \n",
    "    # 第1主成分の構成\n",
    "    pc1_loadings = pca_results['loadings']['PC1']\n",
    "    dominant_component = pc1_loadings.abs().idxmax()\n",
    "    print(f\"  第1主成分の支配的要素: {dominant_component} ({pc1_loadings[dominant_component]:.3f})\")\n",
    "    \n",
    "    print(f\"\\n💰 統計的裁定機会:\")\n",
    "    total_opportunities = 0\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_freq = arbitrage_opps['cointegration_pairs_trading']['signal_frequency']\n",
    "        print(f\"  共和分ペアトレード: {coint_freq:.2f}% のシグナル頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_freq = arbitrage_opps['pca_factor_trading']['signal_frequency']\n",
    "        print(f\"  主成分ファクター取引: {pca_freq:.2f}% のシグナル頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        breakdown_freq = arbitrage_opps['correlation_breakdown']['breakdown_frequency']\n",
    "        print(f\"  相関ブレイクダウン: {breakdown_freq:.2f}% の発生頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    print(f\"\\n💡 投資戦略への示唆:\")\n",
    "    \n",
    "    if coint_pairs > 0:\n",
    "        print(f\"  • 共和分関係を利用した平均回帰戦略が有効\")\n",
    "        print(f\"  • 長期均衡からの乖離を狙ったペアトレードが可能\")\n",
    "    \n",
    "    if pca_results['explained_variance_ratio'][0] > 0.6:\n",
    "        print(f\"  • 第1主成分が高寄与率（{pca_results['explained_variance_ratio'][0]*100:.1f}%）\")\n",
    "        print(f\"  • システマティックリスクファクターとして活用可能\")\n",
    "    \n",
    "    print(f\"  • {total_opportunities}種類の統計的裁定戦略が実装可能\")\n",
    "    print(f\"  • 相関構造の時間変動を活用した動的ヘッジ戦略\")\n",
    "    \n",
    "    # リスク管理の提言\n",
    "    avg_corr = np.mean([abs(pearson_corr.iloc[i, j]) for i in range(len(pearson_corr)) \n",
    "                       for j in range(i+1, len(pearson_corr))])\n",
    "    \n",
    "    print(f\"\\n⚠️ リスク管理:\")\n",
    "    print(f\"  • 平均相関: {avg_corr:.3f} - {'高い' if avg_corr > 0.5 else '中程度の'}分散効果\")\n",
    "    \n",
    "    if max_corr > 0.8:\n",
    "        print(f\"  • 一部ペアで高相関（{max_corr:.3f}） - 集中リスクに注意\")\n",
    "    \n",
    "    print(f\"  • 相関ブレイクダウン時の損失拡大リスクを考慮\")\n",
    "    print(f\"  • 複数戦略の組み合わせによるリスク分散推奨\")\n",
    "    \n",
    "    return {\n",
    "        'max_correlation': (max_pair, max_corr),\n",
    "        'cointegrated_pairs': coint_pairs,\n",
    "        'pca_pc1_contribution': pca_results['explained_variance_ratio'][0],\n",
    "        'arbitrage_opportunities': total_opportunities,\n",
    "        'average_correlation': avg_corr\n",
    "    }\n",
    "\n",
    "# サマリー生成\n",
    "correlation_summary = generate_correlation_analysis_summary(\n",
    "    pearson_corr, coint_results, pca_results, arbitrage_opps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 分析結果の保存\ndef save_correlation_analysis_results(pearson_corr, spearman_corr, pca_results, \n                                     arbitrage_opps, correlation_summary):\n    \"\"\"相関分析結果をファイルに保存\"\"\"\n    \n    # 出力ディレクトリ作成\n    os.makedirs('../../analysis_results/adjacent_spreads', exist_ok=True)\n    \n    # 1. 相関行列\n    pearson_corr.to_csv('../../analysis_results/adjacent_spreads/pearson_correlation.csv', \n                       encoding='utf-8-sig')\n    spearman_corr.to_csv('../../analysis_results/adjacent_spreads/spearman_correlation.csv', \n                        encoding='utf-8-sig')\n    \n    # 2. PCA結果\n    pca_results['loadings'].to_csv('../../analysis_results/adjacent_spreads/pca_loadings.csv', \n                                  encoding='utf-8-sig')\n    pca_results['scores'].to_csv('../../analysis_results/adjacent_spreads/pca_scores.csv', \n                                encoding='utf-8-sig')\n    \n    # 3. 寄与率データフレーム\n    variance_df = pd.DataFrame({\n        'Component': [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))],\n        'Explained_Variance_Ratio': pca_results['explained_variance_ratio'],\n        'Cumulative_Variance_Ratio': pca_results['cumulative_variance_ratio']\n    })\n    variance_df.to_csv('../../analysis_results/adjacent_spreads/pca_variance_explained.csv', \n                      encoding='utf-8-sig', index=False)\n    \n    # 4. 統計的裁定シグナル\n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        coint_signals = arbitrage_opps['cointegration_pairs_trading']['signals']\n        coint_signals.to_csv('../../analysis_results/adjacent_spreads/cointegration_signals.csv', \n                            encoding='utf-8-sig')\n    \n    if 'pca_factor_trading' in arbitrage_opps:\n        pca_signals = arbitrage_opps['pca_factor_trading']['signals']\n        pca_signals.to_csv('../../analysis_results/adjacent_spreads/pca_factor_signals.csv', \n                          encoding='utf-8-sig')\n    \n    # 5. 分析サマリー（JSON）\n    import json\n    \n    with open('../../analysis_results/adjacent_spreads/correlation_analysis_summary.json', \n              'w', encoding='utf-8') as f:\n        json.dump(correlation_summary, f, ensure_ascii=False, indent=2)\n    \n    print(f\"\\n💾 相関分析結果を保存しました:\")\n    print(f\"  📊 Pearson相関: ../../analysis_results/adjacent_spreads/pearson_correlation.csv\")\n    print(f\"  📈 Spearman相関: ../../analysis_results/adjacent_spreads/spearman_correlation.csv\")\n    print(f\"  🎯 PCA負荷量: ../../analysis_results/adjacent_spreads/pca_loadings.csv\")\n    print(f\"  📉 PCAスコア: ../../analysis_results/adjacent_spreads/pca_scores.csv\")\n    print(f\"  📋 寄与率: ../../analysis_results/adjacent_spreads/pca_variance_explained.csv\")\n    \n    if 'cointegration_pairs_trading' in arbitrage_opps:\n        print(f\"  💰 共和分シグナル: ../../analysis_results/adjacent_spreads/cointegration_signals.csv\")\n    \n    if 'pca_factor_trading' in arbitrage_opps:\n        print(f\"  🔧 PCAシグナル: ../../analysis_results/adjacent_spreads/pca_factor_signals.csv\")\n    \n    print(f\"  📝 分析サマリー: ../../analysis_results/adjacent_spreads/correlation_analysis_summary.json\")\n\n# 分析結果保存\nsave_correlation_analysis_results(\n    pearson_corr, spearman_corr, pca_results, arbitrage_opps, correlation_summary\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "この相関・共和分分析により、隣月間スプレッドの詳細な関係性を把握しました。\n",
    "\n",
    "### 主要発見事項\n",
    "1. **相関構造**: 隣接スプレッド間に中程度から強い相関関係\n",
    "2. **共和分関係**: 一部ペアで長期均衡関係を確認\n",
    "3. **主成分構造**: 第1主成分が全変動の大部分を説明\n",
    "4. **裁定機会**: 複数の統計的裁定戦略が実装可能\n",
    "\n",
    "### 次の分析ステップ\n",
    "1. **ボラティリティモデリング**: GARCH系モデルによるリスク分析\n",
    "2. **機械学習予測**: より高度なパターン認識と予測\n",
    "3. **取引戦略構築**: 実際の売買ルールとリスク管理\n",
    "4. **バックテスト実行**: 歴史的データでの戦略検証\n",
    "5. **パフォーマンス評価**: リスク調整後リターンの評価\n",
    "\n",
    "次のノートブック `3_adjacent_spreads_volatility_modeling.ipynb` で、ボラティリティクラスタリングとリスク特性の詳細分析を実施します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}