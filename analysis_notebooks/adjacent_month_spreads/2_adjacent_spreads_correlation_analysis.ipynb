{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LME銅先物 隣月間スプレッド 相関・共和分分析\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、隣月間スプレッド（M1-M2、M2-M3、M3-M4）の相関関係と共和分関係を詳細に分析します。\n",
    "\n",
    "### 分析目標\n",
    "- スプレッド間の相関構造の理解\n",
    "- 共和分関係（長期均衡関係）の検出\n",
    "- ペアトレードの機会特定\n",
    "- リスク分散効果の評価\n",
    "\n",
    "### 期待される成果\n",
    "- スプレッド間の相関パターンとその時間変動\n",
    "- 共和分関係による長期均衡メカニズムの発見\n",
    "- 統計的裁定機会の特定\n",
    "- ポートフォリオ構築への示唆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# 統計・時系列分析\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベース接続とデータ取得\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQLデータベースへの接続を取得\"\"\"\n",
    "    try:\n",
    "        engine = create_engine('postgresql://Yusuke@localhost:5432/lme_copper_db')\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"データベース接続エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_calculate_spreads():\n",
    "    \"\"\"スプレッドデータの取得と計算\"\"\"\n",
    "    engine = get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        trade_date,\n",
    "        contract_month,\n",
    "        close_price,\n",
    "        volume,\n",
    "        open_interest\n",
    "    FROM lme_copper_futures \n",
    "    WHERE contract_month IN (1, 2, 3, 4)\n",
    "        AND close_price IS NOT NULL\n",
    "        AND close_price > 0\n",
    "    ORDER BY trade_date, contract_month\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "    \n",
    "    # ピボットしてスプレッド計算\n",
    "    pivot_df = df.pivot(index='trade_date', columns='contract_month', values='close_price')\n",
    "    pivot_df.columns = [f'M{int(col)}' for col in pivot_df.columns]\n",
    "    \n",
    "    # スプレッド計算\n",
    "    spreads_df = pd.DataFrame(index=pivot_df.index)\n",
    "    spreads_df['M1_M2_spread'] = pivot_df['M1'] - pivot_df['M2']\n",
    "    spreads_df['M2_M3_spread'] = pivot_df['M2'] - pivot_df['M3']\n",
    "    spreads_df['M3_M4_spread'] = pivot_df['M3'] - pivot_df['M4']\n",
    "    \n",
    "    # 価格データも保持\n",
    "    spreads_df['M1_price'] = pivot_df['M1']\n",
    "    spreads_df['M2_price'] = pivot_df['M2']\n",
    "    spreads_df['M3_price'] = pivot_df['M3']\n",
    "    spreads_df['M4_price'] = pivot_df['M4']\n",
    "    \n",
    "    return spreads_df.dropna()\n",
    "\n",
    "# データ取得\n",
    "spreads_data = load_and_calculate_spreads()\n",
    "print(f\"✅ データ取得完了: {len(spreads_data):,} レコード\")\n",
    "print(f\"📅 分析期間: {spreads_data.index.min()} ～ {spreads_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本相関分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_basic_correlations(df):\n",
    "    \"\"\"基本相関分析\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    \n",
    "    # 相関行列計算\n",
    "    correlation_matrix = df[spread_columns].corr()\n",
    "    \n",
    "    print(\"📊 隣月間スプレッド相関行列:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(correlation_matrix.round(4))\n",
    "    \n",
    "    # Spearman順位相関も計算\n",
    "    spearman_corr = df[spread_columns].corr(method='spearman')\n",
    "    \n",
    "    print(\"\\n📈 Spearman順位相関行列:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(spearman_corr.round(4))\n",
    "    \n",
    "    return correlation_matrix, spearman_corr\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix, spearman_corr):\n",
    "    \"\"\"相関ヒートマップの作成\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Pearson相関', 'Spearman順位相関'),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    # Pearson相関ヒートマップ\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=spread_names,\n",
    "            y=spread_names,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            text=np.round(corr_matrix.values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 14},\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=0.45)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Spearman相関ヒートマップ\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=spearman_corr.values,\n",
    "            x=spread_names,\n",
    "            y=spread_names,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            text=np.round(spearman_corr.values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 14},\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=1.02)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"隣月間スプレッド相関分析\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=500,\n",
    "        width=900\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 基本相関分析実行\n",
    "pearson_corr, spearman_corr = analyze_basic_correlations(spreads_data)\n",
    "\n",
    "# 相関ヒートマップ表示\n",
    "corr_chart = plot_correlation_heatmap(pearson_corr, spearman_corr)\n",
    "corr_chart.show()\n",
    "\n",
    "# 画像保存\n",
    "os.makedirs('../generated_images', exist_ok=True)\n",
    "corr_chart.write_image('../generated_images/adjacent_spreads_correlation_heatmap.png', \n",
    "                      width=900, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 時間変動相関分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_correlations(df, window=60):\n",
    "    \"\"\"ローリング相関の計算\"\"\"\n",
    "    rolling_corr_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # M1-M2 vs M2-M3の相関\n",
    "    rolling_corr_df['M1M2_vs_M2M3'] = df['M1_M2_spread'].rolling(window=window).corr(\n",
    "        df['M2_M3_spread']\n",
    "    )\n",
    "    \n",
    "    # M2-M3 vs M3-M4の相関\n",
    "    rolling_corr_df['M2M3_vs_M3M4'] = df['M2_M3_spread'].rolling(window=window).corr(\n",
    "        df['M3_M4_spread']\n",
    "    )\n",
    "    \n",
    "    # M1-M2 vs M3-M4の相関\n",
    "    rolling_corr_df['M1M2_vs_M3M4'] = df['M1_M2_spread'].rolling(window=window).corr(\n",
    "        df['M3_M4_spread']\n",
    "    )\n",
    "    \n",
    "    return rolling_corr_df.dropna()\n",
    "\n",
    "def plot_rolling_correlations(rolling_corr_df):\n",
    "    \"\"\"ローリング相関のプロット\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    correlations = [\n",
    "        ('M1M2_vs_M2M3', 'M1-M2 vs M2-M3', 'blue'),\n",
    "        ('M2M3_vs_M3M4', 'M2-M3 vs M3-M4', 'red'),\n",
    "        ('M1M2_vs_M3M4', 'M1-M2 vs M3-M4', 'green')\n",
    "    ]\n",
    "    \n",
    "    for col, name, color in correlations:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_corr_df.index,\n",
    "                y=rolling_corr_df[col],\n",
    "                name=name,\n",
    "                line=dict(color=color, width=2),\n",
    "                mode='lines'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # 基準線を追加\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1)\n",
    "    fig.add_hline(y=0.5, line_dash=\"dot\", line_color=\"gray\", line_width=1)\n",
    "    fig.add_hline(y=-0.5, line_dash=\"dot\", line_color=\"gray\", line_width=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"隣月間スプレッド ローリング相関（60日）\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        xaxis_title=\"日付\",\n",
    "        yaxis_title=\"相関係数\",\n",
    "        height=500,\n",
    "        showlegend=True,\n",
    "        yaxis=dict(range=[-1, 1])\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ローリング相関計算\n",
    "rolling_corr_data = calculate_rolling_correlations(spreads_data, window=60)\n",
    "\n",
    "print(f\"📈 ローリング相関統計（60日窓）:\")\n",
    "print(\"=\" * 50)\n",
    "print(rolling_corr_data.describe().round(4))\n",
    "\n",
    "# ローリング相関チャート\n",
    "rolling_corr_chart = plot_rolling_correlations(rolling_corr_data)\n",
    "rolling_corr_chart.show()\n",
    "\n",
    "# 画像保存\n",
    "rolling_corr_chart.write_image('../generated_images/adjacent_spreads_rolling_correlation.png', \n",
    "                              width=1200, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 共和分分析（長期均衡関係）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cointegration(df):\n",
    "    \"\"\"共和分検定の実行\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_names = ['M1-M2', 'M2-M3', 'M3-M4']\n",
    "    \n",
    "    print(\"🔬 共和分検定結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    coint_results = {}\n",
    "    \n",
    "    # ペアワイズ共和分検定\n",
    "    pairs = [\n",
    "        (('M1_M2_spread', 'M2_M3_spread'), ('M1-M2', 'M2-M3')),\n",
    "        (('M2_M3_spread', 'M3_M4_spread'), ('M2-M3', 'M3-M4')),\n",
    "        (('M1_M2_spread', 'M3_M4_spread'), ('M1-M2', 'M3-M4'))\n",
    "    ]\n",
    "    \n",
    "    for (col1, col2), (name1, name2) in pairs:\n",
    "        # Engle-Granger共和分検定\n",
    "        coint_t, p_value, critical_values = coint(df[col1], df[col2])\n",
    "        \n",
    "        coint_results[f'{name1}_vs_{name2}'] = {\n",
    "            'test_statistic': coint_t,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name1} vs {name2}:\")\n",
    "        print(f\"  検定統計量: {coint_t:.4f}\")\n",
    "        print(f\"  p値: {p_value:.4f}\")\n",
    "        print(f\"  結果: {'共和分関係あり' if p_value < 0.05 else '共和分関係なし'}\")\n",
    "        print(f\"  臨界値 1%: {critical_values[0]:.4f}\")\n",
    "        print(f\"  臨界値 5%: {critical_values[1]:.4f}\")\n",
    "        print(f\"  臨界値 10%: {critical_values[2]:.4f}\")\n",
    "    \n",
    "    return coint_results\n",
    "\n",
    "def johansen_cointegration_test(df):\n",
    "    \"\"\"Johansen共和分検定（多変量）\"\"\"\n",
    "    spread_data = df[['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']].dropna()\n",
    "    \n",
    "    print(f\"\\n🔍 Johansen共和分検定（多変量）:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Johansen検定実行\n",
    "    johansen_result = coint_johansen(spread_data, det_order=0, k_ar_diff=1)\n",
    "    \n",
    "    print(f\"Trace統計量:\")\n",
    "    for i, (trace_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr1, johansen_result.cvt[:, 0], \n",
    "            johansen_result.cvt[:, 1], johansen_result.cvt[:, 2])\n",
    "    ):\n",
    "        print(f\"  r≤{i}: {trace_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "        \n",
    "        if trace_stat > cv_95:\n",
    "            print(f\"    → r>{i}の共和分関係あり（95%水準）\")\n",
    "        else:\n",
    "            print(f\"    → r≤{i}の共和分関係\")\n",
    "    \n",
    "    print(f\"\\nMaximum Eigenvalue統計量:\")\n",
    "    for i, (max_stat, cv_90, cv_95, cv_99) in enumerate(\n",
    "        zip(johansen_result.lr2, johansen_result.cvm[:, 0], \n",
    "            johansen_result.cvm[:, 1], johansen_result.cvm[:, 2])\n",
    "    ):\n",
    "        print(f\"  r={i}: {max_stat:.4f} (90%: {cv_90:.4f}, 95%: {cv_95:.4f}, 99%: {cv_99:.4f})\")\n",
    "    \n",
    "    return johansen_result\n",
    "\n",
    "# 共和分検定実行\n",
    "coint_results = test_cointegration(spreads_data)\n",
    "johansen_result = johansen_cointegration_test(spreads_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cointegration_relationships(df, coint_results):\n",
    "    \"\"\"共和分関係の詳細分析\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 共和分関係の詳細分析:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 有意な共和分関係を持つペアを特定\n",
    "    significant_pairs = []\n",
    "    \n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            significant_pairs.append(pair_name)\n",
    "            print(f\"✅ {pair_name}: 共和分関係あり（p={result['p_value']:.4f}）\")\n",
    "    \n",
    "    if not significant_pairs:\n",
    "        print(\"❌ 有意な共和分関係は検出されませんでした\")\n",
    "        return None\n",
    "    \n",
    "    # 最も強い共和分関係のペアで詳細分析\n",
    "    best_pair_name = min(coint_results.keys(), key=lambda x: coint_results[x]['p_value'])\n",
    "    print(f\"\\n🎯 最強共和分関係: {best_pair_name}\")\n",
    "    \n",
    "    # ペア名から実際のカラム名を特定\n",
    "    if 'M1-M2_vs_M2-M3' in best_pair_name:\n",
    "        col1, col2 = 'M1_M2_spread', 'M2_M3_spread'\n",
    "        name1, name2 = 'M1-M2', 'M2-M3'\n",
    "    elif 'M2-M3_vs_M3-M4' in best_pair_name:\n",
    "        col1, col2 = 'M2_M3_spread', 'M3_M4_spread'\n",
    "        name1, name2 = 'M2-M3', 'M3-M4'\n",
    "    else:\n",
    "        col1, col2 = 'M1_M2_spread', 'M3_M4_spread'\n",
    "        name1, name2 = 'M1-M2', 'M3-M4'\n",
    "    \n",
    "    # 共和分回帰\n",
    "    y = df[col1]\n",
    "    x = sm.add_constant(df[col2])\n",
    "    \n",
    "    model = OLS(y, x).fit()\n",
    "    \n",
    "    print(f\"\\n回帰式: {name1} = {model.params[0]:.4f} + {model.params[1]:.4f} * {name2}\")\n",
    "    print(f\"R²: {model.rsquared:.4f}\")\n",
    "    print(f\"回帰係数のt値: {model.tvalues[1]:.4f}\")\n",
    "    \n",
    "    # 残差（誤差修正項）の計算\n",
    "    residuals = model.resid\n",
    "    \n",
    "    # 残差の定常性検定\n",
    "    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(residuals)\n",
    "    \n",
    "    print(f\"\\n残差の定常性検定（ADF）:\")\n",
    "    print(f\"  統計量: {adf_stat:.4f}\")\n",
    "    print(f\"  p値: {adf_p:.4f}\")\n",
    "    print(f\"  結果: {'定常' if adf_p < 0.05 else '非定常'}\")\n",
    "    \n",
    "    return {\n",
    "        'best_pair': (col1, col2),\n",
    "        'regression_model': model,\n",
    "        'residuals': residuals,\n",
    "        'adf_test': (adf_stat, adf_p)\n",
    "    }\n",
    "\n",
    "# 共和分関係の詳細分析\n",
    "coint_analysis = analyze_cointegration_relationships(spreads_data, coint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cointegration_analysis(df, coint_analysis):\n",
    "    \"\"\"共和分分析の可視化\"\"\"\n",
    "    if coint_analysis is None:\n",
    "        print(\"共和分関係が検出されなかったため、プロットをスキップします\")\n",
    "        return None\n",
    "    \n",
    "    col1, col2 = coint_analysis['best_pair']\n",
    "    residuals = coint_analysis['residuals']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'スプレッドペアの時系列',\n",
    "            'スプレッド散布図と回帰線',\n",
    "            '誤差修正項（残差）の推移',\n",
    "            '残差の分布'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. 時系列プロット\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[col1],\n",
    "            name=col1.replace('_spread', '').replace('_', '-'),\n",
    "            line=dict(color='blue', width=1)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[col2],\n",
    "            name=col2.replace('_spread', '').replace('_', '-'),\n",
    "            line=dict(color='red', width=1)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. 散布図と回帰線\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[col2],\n",
    "            y=df[col1],\n",
    "            mode='markers',\n",
    "            name='データポイント',\n",
    "            marker=dict(color='lightblue', size=3, opacity=0.6)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 回帰線\n",
    "    model = coint_analysis['regression_model']\n",
    "    x_range = np.linspace(df[col2].min(), df[col2].max(), 100)\n",
    "    y_pred = model.params[0] + model.params[1] * x_range\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            name='回帰線',\n",
    "            line=dict(color='red', width=2)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. 残差の時系列\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=residuals,\n",
    "            name='誤差修正項',\n",
    "            line=dict(color='green', width=1)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # ゼロライン\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1, row=2, col=1)\n",
    "    \n",
    "    # 4. 残差のヒストグラム\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=residuals,\n",
    "            name='残差分布',\n",
    "            nbinsx=50,\n",
    "            marker_color='purple',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"共和分分析 - 長期均衡関係の可視化\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 軸ラベル更新\n",
    "    fig.update_yaxes(title_text=\"スプレッド (USD/t)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"日付\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=f\"{col2.replace('_spread', '').replace('_', '-')} スプレッド\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=f\"{col1.replace('_spread', '').replace('_', '-')} スプレッド\", row=1, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"残差\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"日付\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"残差\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"頻度\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 共和分分析の可視化\n",
    "if coint_analysis:\n",
    "    coint_chart = plot_cointegration_analysis(spreads_data, coint_analysis)\n",
    "    if coint_chart:\n",
    "        coint_chart.show()\n",
    "        \n",
    "        # 画像保存\n",
    "        coint_chart.write_image('../generated_images/adjacent_spreads_cointegration_analysis.png', \n",
    "                               width=1200, height=800, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 主成分分析（PCA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_analysis(df):\n",
    "    \"\"\"主成分分析の実行\"\"\"\n",
    "    spread_columns = ['M1_M2_spread', 'M2_M3_spread', 'M3_M4_spread']\n",
    "    spread_data = df[spread_columns].dropna()\n",
    "    \n",
    "    # データの標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(spread_data)\n",
    "    \n",
    "    # PCA実行\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    print(f\"📊 主成分分析結果:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 寄与率と累積寄与率\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    for i, (var_ratio, cum_ratio) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n",
    "        print(f\"PC{i+1}: 寄与率 {var_ratio:.4f} ({var_ratio*100:.2f}%), 累積寄与率 {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n",
    "    \n",
    "    # 主成分負荷量\n",
    "    print(f\"\\n📈 主成分負荷量:\")\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    \n",
    "    loadings_df = pd.DataFrame(\n",
    "        loadings,\n",
    "        index=['M1-M2', 'M2-M3', 'M3-M4'],\n",
    "        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n",
    "    )\n",
    "    \n",
    "    print(loadings_df.round(4))\n",
    "    \n",
    "    # 主成分スコア\n",
    "    pca_scores_df = pd.DataFrame(\n",
    "        pca_result,\n",
    "        index=spread_data.index,\n",
    "        columns=[f'PC{i+1}' for i in range(len(spread_columns))]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'pca_model': pca,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance_ratio': cumulative_variance_ratio,\n",
    "        'loadings': loadings_df,\n",
    "        'scores': pca_scores_df,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "def plot_pca_analysis(pca_results):\n",
    "    \"\"\"PCA分析の可視化\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            '寄与率（スクリープロット）',\n",
    "            '主成分負荷量',\n",
    "            '第1・第2主成分スコア',\n",
    "            '第1主成分の時系列'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. スクリープロット\n",
    "    pcs = [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=pcs,\n",
    "            y=pca_results['explained_variance_ratio'] * 100,\n",
    "            name='寄与率',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pcs,\n",
    "            y=pca_results['cumulative_variance_ratio'] * 100,\n",
    "            name='累積寄与率',\n",
    "            line=dict(color='red', width=2),\n",
    "            yaxis='y2'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. 主成分負荷量のヒートマップ\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=pca_results['loadings'].values,\n",
    "            x=pca_results['loadings'].columns,\n",
    "            y=pca_results['loadings'].index,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            text=np.round(pca_results['loadings'].values, 3),\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"size\": 12},\n",
    "            showscale=True\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. 主成分スコア散布図\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_results['scores']['PC1'],\n",
    "            y=pca_results['scores']['PC2'],\n",
    "            mode='markers',\n",
    "            name='PC1 vs PC2',\n",
    "            marker=dict(\n",
    "                color=pca_results['scores'].index.map(lambda x: x.year),\n",
    "                colorscale='Viridis',\n",
    "                size=4,\n",
    "                opacity=0.7,\n",
    "                colorbar=dict(title=\"年\")\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. 第1主成分の時系列\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pca_results['scores'].index,\n",
    "            y=pca_results['scores']['PC1'],\n",
    "            name='第1主成分',\n",
    "            line=dict(color='blue', width=1)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", line_width=1, row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"隣月間スプレッド主成分分析\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 軸ラベル更新\n",
    "    fig.update_yaxes(title_text=\"寄与率 (%)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"主成分\", row=1, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"第2主成分\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"第1主成分\", row=2, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"第1主成分スコア\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"日付\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# PCA分析実行\n",
    "pca_results = perform_pca_analysis(spreads_data)\n",
    "\n",
    "# PCA可視化\n",
    "pca_chart = plot_pca_analysis(pca_results)\n",
    "pca_chart.show()\n",
    "\n",
    "# 画像保存\n",
    "pca_chart.write_image('../generated_images/adjacent_spreads_pca_analysis.png', \n",
    "                     width=1200, height=800, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 統計的裁定機会の特定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_statistical_arbitrage_opportunities(df, pca_results, coint_analysis):\n",
    "    \"\"\"統計的裁定機会の特定\"\"\"\n",
    "    \n",
    "    print(f\"🎯 統計的裁定機会分析:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    arbitrage_opportunities = {}\n",
    "    \n",
    "    # 1. 共和分ベースの裁定\n",
    "    if coint_analysis and coint_analysis['adf_test'][1] < 0.05:\n",
    "        residuals = coint_analysis['residuals']\n",
    "        \n",
    "        # 残差の統計量\n",
    "        residual_mean = residuals.mean()\n",
    "        residual_std = residuals.std()\n",
    "        \n",
    "        # エントリー・エグジットシグナル\n",
    "        upper_threshold = residual_mean + 2 * residual_std\n",
    "        lower_threshold = residual_mean - 2 * residual_std\n",
    "        \n",
    "        # シグナル生成\n",
    "        signals = pd.Series(index=residuals.index, dtype=float)\n",
    "        signals[residuals > upper_threshold] = -1  # ショートシグナル\n",
    "        signals[residuals < lower_threshold] = 1   # ロングシグナル\n",
    "        signals[abs(residuals - residual_mean) < 0.5 * residual_std] = 0  # エグジット\n",
    "        \n",
    "        # 前値で埋める\n",
    "        signals = signals.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        arbitrage_opportunities['cointegration_pairs_trading'] = {\n",
    "            'pair': coint_analysis['best_pair'],\n",
    "            'residuals': residuals,\n",
    "            'signals': signals,\n",
    "            'thresholds': (lower_threshold, upper_threshold),\n",
    "            'signal_frequency': (signals != 0).sum() / len(signals) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 共和分ペアトレード機会:\")\n",
    "        print(f\"   対象ペア: {coint_analysis['best_pair']}\")\n",
    "        print(f\"   シグナル頻度: {arbitrage_opportunities['cointegration_pairs_trading']['signal_frequency']:.2f}%\")\n",
    "        print(f\"   エントリー閾値: ±{2:.1f}σ ({lower_threshold:.4f}, {upper_threshold:.4f})\")\n",
    "    \n",
    "    # 2. 主成分ベースの裁定\n",
    "    pc1_scores = pca_results['scores']['PC1']\n",
    "    pc1_mean = pc1_scores.mean()\n",
    "    pc1_std = pc1_scores.std()\n",
    "    \n",
    "    # 第1主成分の極値検出\n",
    "    pc1_upper = pc1_mean + 2 * pc1_std\n",
    "    pc1_lower = pc1_mean - 2 * pc1_std\n",
    "    \n",
    "    pc1_signals = pd.Series(index=pc1_scores.index, dtype=float)\n",
    "    pc1_signals[pc1_scores > pc1_upper] = -1\n",
    "    pc1_signals[pc1_scores < pc1_lower] = 1\n",
    "    pc1_signals[abs(pc1_scores - pc1_mean) < 0.5 * pc1_std] = 0\n",
    "    pc1_signals = pc1_signals.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    arbitrage_opportunities['pca_factor_trading'] = {\n",
    "        'pc1_scores': pc1_scores,\n",
    "        'signals': pc1_signals,\n",
    "        'thresholds': (pc1_lower, pc1_upper),\n",
    "        'signal_frequency': (pc1_signals != 0).sum() / len(pc1_signals) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ 主成分ファクター取引機会:\")\n",
    "    print(f\"   第1主成分寄与率: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"   シグナル頻度: {arbitrage_opportunities['pca_factor_trading']['signal_frequency']:.2f}%\")\n",
    "    print(f\"   エントリー閾値: ±{2:.1f}σ ({pc1_lower:.4f}, {pc1_upper:.4f})\")\n",
    "    \n",
    "    # 3. 相関ブレイクダウン機会\n",
    "    rolling_corr_60d = spreads_data['M1_M2_spread'].rolling(window=60).corr(\n",
    "        spreads_data['M2_M3_spread']\n",
    "    ).dropna()\n",
    "    \n",
    "    corr_mean = rolling_corr_60d.mean()\n",
    "    corr_std = rolling_corr_60d.std()\n",
    "    \n",
    "    # 相関の異常値（ブレイクダウン）を検出\n",
    "    corr_breakdown_threshold = corr_mean - 2 * corr_std\n",
    "    correlation_breakdowns = rolling_corr_60d < corr_breakdown_threshold\n",
    "    \n",
    "    arbitrage_opportunities['correlation_breakdown'] = {\n",
    "        'rolling_correlation': rolling_corr_60d,\n",
    "        'breakdown_threshold': corr_breakdown_threshold,\n",
    "        'breakdown_periods': correlation_breakdowns,\n",
    "        'breakdown_frequency': correlation_breakdowns.sum() / len(correlation_breakdowns) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ 相関ブレイクダウン機会:\")\n",
    "    print(f\"   平均相関: {corr_mean:.4f}\")\n",
    "    print(f\"   ブレイクダウン閾値: {corr_breakdown_threshold:.4f}\")\n",
    "    print(f\"   ブレイクダウン頻度: {arbitrage_opportunities['correlation_breakdown']['breakdown_frequency']:.2f}%\")\n",
    "    \n",
    "    return arbitrage_opportunities\n",
    "\n",
    "# 統計的裁定機会の分析\n",
    "arbitrage_opps = identify_statistical_arbitrage_opportunities(\n",
    "    spreads_data, pca_results, coint_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arbitrage_opportunities(arbitrage_opps):\n",
    "    \"\"\"統計的裁定機会の可視化\"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=(\n",
    "            '共和分ペアトレードシグナル',\n",
    "            '主成分ファクター取引シグナル',\n",
    "            '相関ブレイクダウン機会'\n",
    "        ),\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    # 1. 共和分ペアトレード\n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_data = arbitrage_opps['cointegration_pairs_trading']\n",
    "        residuals = coint_data['residuals']\n",
    "        signals = coint_data['signals']\n",
    "        lower_thresh, upper_thresh = coint_data['thresholds']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=residuals.index,\n",
    "                y=residuals,\n",
    "                name='誤差修正項',\n",
    "                line=dict(color='blue', width=1)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 閾値線\n",
    "        fig.add_hline(y=upper_thresh, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        fig.add_hline(y=lower_thresh, line_dash=\"dash\", line_color=\"green\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n",
    "                     line_width=1, row=1, col=1)\n",
    "        \n",
    "        # シグナルをハイライト\n",
    "        buy_signals = signals[signals == 1]\n",
    "        sell_signals = signals[signals == -1]\n",
    "        \n",
    "        if len(buy_signals) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=buy_signals.index,\n",
    "                    y=residuals.loc[buy_signals.index],\n",
    "                    mode='markers',\n",
    "                    name='買いシグナル',\n",
    "                    marker=dict(color='green', size=8, symbol='triangle-up')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        if len(sell_signals) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sell_signals.index,\n",
    "                    y=residuals.loc[sell_signals.index],\n",
    "                    mode='markers',\n",
    "                    name='売りシグナル',\n",
    "                    marker=dict(color='red', size=8, symbol='triangle-down')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # 2. 主成分ファクター取引\n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_data = arbitrage_opps['pca_factor_trading']\n",
    "        pc1_scores = pca_data['pc1_scores']\n",
    "        pc1_signals = pca_data['signals']\n",
    "        pc1_lower, pc1_upper = pca_data['thresholds']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pc1_scores.index,\n",
    "                y=pc1_scores,\n",
    "                name='第1主成分',\n",
    "                line=dict(color='purple', width=1)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 閾値線\n",
    "        fig.add_hline(y=pc1_upper, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "        fig.add_hline(y=pc1_lower, line_dash=\"dash\", line_color=\"green\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"black\", \n",
    "                     line_width=1, row=2, col=1)\n",
    "    \n",
    "    # 3. 相関ブレイクダウン\n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        corr_data = arbitrage_opps['correlation_breakdown']\n",
    "        rolling_corr = corr_data['rolling_correlation']\n",
    "        breakdown_threshold = corr_data['breakdown_threshold']\n",
    "        breakdowns = corr_data['breakdown_periods']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rolling_corr.index,\n",
    "                y=rolling_corr,\n",
    "                name='60日ローリング相関',\n",
    "                line=dict(color='orange', width=1)\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # ブレイクダウン閾値\n",
    "        fig.add_hline(y=breakdown_threshold, line_dash=\"dash\", line_color=\"red\", \n",
    "                     line_width=2, row=3, col=1)\n",
    "        \n",
    "        # ブレイクダウン期間をハイライト\n",
    "        breakdown_dates = breakdowns[breakdowns].index\n",
    "        if len(breakdown_dates) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=breakdown_dates,\n",
    "                    y=rolling_corr.loc[breakdown_dates],\n",
    "                    mode='markers',\n",
    "                    name='ブレイクダウン',\n",
    "                    marker=dict(color='red', size=6)\n",
    "                ),\n",
    "                row=3, col=1\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"統計的裁定機会の特定\",\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        height=1000,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 軸ラベル更新\n",
    "    fig.update_yaxes(title_text=\"残差\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"PC1スコア\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"相関係数\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"日付\", row=3, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 裁定機会の可視化\n",
    "arbitrage_chart = plot_arbitrage_opportunities(arbitrage_opps)\n",
    "arbitrage_chart.show()\n",
    "\n",
    "# 画像保存\n",
    "arbitrage_chart.write_image('../generated_images/adjacent_spreads_arbitrage_opportunities.png', \n",
    "                           width=1200, height=1000, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 分析結果サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包括的分析サマリー\n",
    "def generate_correlation_analysis_summary(pearson_corr, coint_results, pca_results, arbitrage_opps):\n",
    "    \"\"\"相関・共和分分析の包括的サマリー\"\"\"\n",
    "    \n",
    "    print(\"📋 隣月間スプレッド相関・共和分分析サマリー\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n🔗 相関構造:\")\n",
    "    print(f\"  M1-M2 vs M2-M3: {pearson_corr.loc['M1_M2_spread', 'M2_M3_spread']:.3f}\")\n",
    "    print(f\"  M2-M3 vs M3-M4: {pearson_corr.loc['M2_M3_spread', 'M3_M4_spread']:.3f}\")\n",
    "    print(f\"  M1-M2 vs M3-M4: {pearson_corr.loc['M1_M2_spread', 'M3_M4_spread']:.3f}\")\n",
    "    \n",
    "    # 最強相関ペア\n",
    "    max_corr = 0\n",
    "    max_pair = \"\"\n",
    "    for i in range(len(pearson_corr)):\n",
    "        for j in range(i+1, len(pearson_corr)):\n",
    "            corr_val = abs(pearson_corr.iloc[i, j])\n",
    "            if corr_val > max_corr:\n",
    "                max_corr = corr_val\n",
    "                max_pair = f\"{pearson_corr.index[i]} vs {pearson_corr.columns[j]}\"\n",
    "    \n",
    "    print(f\"  最強相関ペア: {max_pair} ({max_corr:.3f})\")\n",
    "    \n",
    "    print(f\"\\n🎯 共和分分析:\")\n",
    "    coint_pairs = 0\n",
    "    for pair_name, result in coint_results.items():\n",
    "        if result['p_value'] < 0.05:\n",
    "            coint_pairs += 1\n",
    "            print(f\"  ✅ {pair_name}: p={result['p_value']:.4f} (共和分関係あり)\")\n",
    "        else:\n",
    "            print(f\"  ❌ {pair_name}: p={result['p_value']:.4f} (共和分関係なし)\")\n",
    "    \n",
    "    print(f\"\\n📊 主成分分析:\")\n",
    "    print(f\"  第1主成分寄与率: {pca_results['explained_variance_ratio'][0]*100:.2f}%\")\n",
    "    print(f\"  第2主成分寄与率: {pca_results['explained_variance_ratio'][1]*100:.2f}%\")\n",
    "    print(f\"  累積寄与率（PC1+PC2）: {pca_results['cumulative_variance_ratio'][1]*100:.2f}%\")\n",
    "    \n",
    "    # 第1主成分の構成\n",
    "    pc1_loadings = pca_results['loadings']['PC1']\n",
    "    dominant_component = pc1_loadings.abs().idxmax()\n",
    "    print(f\"  第1主成分の支配的要素: {dominant_component} ({pc1_loadings[dominant_component]:.3f})\")\n",
    "    \n",
    "    print(f\"\\n💰 統計的裁定機会:\")\n",
    "    total_opportunities = 0\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_freq = arbitrage_opps['cointegration_pairs_trading']['signal_frequency']\n",
    "        print(f\"  共和分ペアトレード: {coint_freq:.2f}% のシグナル頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_freq = arbitrage_opps['pca_factor_trading']['signal_frequency']\n",
    "        print(f\"  主成分ファクター取引: {pca_freq:.2f}% のシグナル頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    if 'correlation_breakdown' in arbitrage_opps:\n",
    "        breakdown_freq = arbitrage_opps['correlation_breakdown']['breakdown_frequency']\n",
    "        print(f\"  相関ブレイクダウン: {breakdown_freq:.2f}% の発生頻度\")\n",
    "        total_opportunities += 1\n",
    "    \n",
    "    print(f\"\\n💡 投資戦略への示唆:\")\n",
    "    \n",
    "    if coint_pairs > 0:\n",
    "        print(f\"  • 共和分関係を利用した平均回帰戦略が有効\")\n",
    "        print(f\"  • 長期均衡からの乖離を狙ったペアトレードが可能\")\n",
    "    \n",
    "    if pca_results['explained_variance_ratio'][0] > 0.6:\n",
    "        print(f\"  • 第1主成分が高寄与率（{pca_results['explained_variance_ratio'][0]*100:.1f}%）\")\n",
    "        print(f\"  • システマティックリスクファクターとして活用可能\")\n",
    "    \n",
    "    print(f\"  • {total_opportunities}種類の統計的裁定戦略が実装可能\")\n",
    "    print(f\"  • 相関構造の時間変動を活用した動的ヘッジ戦略\")\n",
    "    \n",
    "    # リスク管理の提言\n",
    "    avg_corr = np.mean([abs(pearson_corr.iloc[i, j]) for i in range(len(pearson_corr)) \n",
    "                       for j in range(i+1, len(pearson_corr))])\n",
    "    \n",
    "    print(f\"\\n⚠️ リスク管理:\")\n",
    "    print(f\"  • 平均相関: {avg_corr:.3f} - {'高い' if avg_corr > 0.5 else '中程度の'}分散効果\")\n",
    "    \n",
    "    if max_corr > 0.8:\n",
    "        print(f\"  • 一部ペアで高相関（{max_corr:.3f}） - 集中リスクに注意\")\n",
    "    \n",
    "    print(f\"  • 相関ブレイクダウン時の損失拡大リスクを考慮\")\n",
    "    print(f\"  • 複数戦略の組み合わせによるリスク分散推奨\")\n",
    "    \n",
    "    return {\n",
    "        'max_correlation': (max_pair, max_corr),\n",
    "        'cointegrated_pairs': coint_pairs,\n",
    "        'pca_pc1_contribution': pca_results['explained_variance_ratio'][0],\n",
    "        'arbitrage_opportunities': total_opportunities,\n",
    "        'average_correlation': avg_corr\n",
    "    }\n",
    "\n",
    "# サマリー生成\n",
    "correlation_summary = generate_correlation_analysis_summary(\n",
    "    pearson_corr, coint_results, pca_results, arbitrage_opps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析結果の保存\n",
    "def save_correlation_analysis_results(pearson_corr, spearman_corr, pca_results, \n",
    "                                     arbitrage_opps, correlation_summary):\n",
    "    \"\"\"相関分析結果をファイルに保存\"\"\"\n",
    "    \n",
    "    # 出力ディレクトリ作成\n",
    "    os.makedirs('../analysis_results/adjacent_spreads', exist_ok=True)\n",
    "    \n",
    "    # 1. 相関行列\n",
    "    pearson_corr.to_csv('../analysis_results/adjacent_spreads/pearson_correlation.csv', \n",
    "                       encoding='utf-8-sig')\n",
    "    spearman_corr.to_csv('../analysis_results/adjacent_spreads/spearman_correlation.csv', \n",
    "                        encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. PCA結果\n",
    "    pca_results['loadings'].to_csv('../analysis_results/adjacent_spreads/pca_loadings.csv', \n",
    "                                  encoding='utf-8-sig')\n",
    "    pca_results['scores'].to_csv('../analysis_results/adjacent_spreads/pca_scores.csv', \n",
    "                                encoding='utf-8-sig')\n",
    "    \n",
    "    # 3. 寄与率データフレーム\n",
    "    variance_df = pd.DataFrame({\n",
    "        'Component': [f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))],\n",
    "        'Explained_Variance_Ratio': pca_results['explained_variance_ratio'],\n",
    "        'Cumulative_Variance_Ratio': pca_results['cumulative_variance_ratio']\n",
    "    })\n",
    "    variance_df.to_csv('../analysis_results/adjacent_spreads/pca_variance_explained.csv', \n",
    "                      encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # 4. 統計的裁定シグナル\n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        coint_signals = arbitrage_opps['cointegration_pairs_trading']['signals']\n",
    "        coint_signals.to_csv('../analysis_results/adjacent_spreads/cointegration_signals.csv', \n",
    "                            encoding='utf-8-sig')\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        pca_signals = arbitrage_opps['pca_factor_trading']['signals']\n",
    "        pca_signals.to_csv('../analysis_results/adjacent_spreads/pca_factor_signals.csv', \n",
    "                          encoding='utf-8-sig')\n",
    "    \n",
    "    # 5. 分析サマリー（JSON）\n",
    "    import json\n",
    "    \n",
    "    with open('../analysis_results/adjacent_spreads/correlation_analysis_summary.json', \n",
    "              'w', encoding='utf-8') as f:\n",
    "        json.dump(correlation_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 相関分析結果を保存しました:\")\n",
    "    print(f\"  📊 Pearson相関: ../analysis_results/adjacent_spreads/pearson_correlation.csv\")\n",
    "    print(f\"  📈 Spearman相関: ../analysis_results/adjacent_spreads/spearman_correlation.csv\")\n",
    "    print(f\"  🎯 PCA負荷量: ../analysis_results/adjacent_spreads/pca_loadings.csv\")\n",
    "    print(f\"  📉 PCAスコア: ../analysis_results/adjacent_spreads/pca_scores.csv\")\n",
    "    print(f\"  📋 寄与率: ../analysis_results/adjacent_spreads/pca_variance_explained.csv\")\n",
    "    \n",
    "    if 'cointegration_pairs_trading' in arbitrage_opps:\n",
    "        print(f\"  💰 共和分シグナル: ../analysis_results/adjacent_spreads/cointegration_signals.csv\")\n",
    "    \n",
    "    if 'pca_factor_trading' in arbitrage_opps:\n",
    "        print(f\"  🔧 PCAシグナル: ../analysis_results/adjacent_spreads/pca_factor_signals.csv\")\n",
    "    \n",
    "    print(f\"  📝 分析サマリー: ../analysis_results/adjacent_spreads/correlation_analysis_summary.json\")\n",
    "\n",
    "# 分析結果保存\n",
    "save_correlation_analysis_results(\n",
    "    pearson_corr, spearman_corr, pca_results, arbitrage_opps, correlation_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "この相関・共和分分析により、隣月間スプレッドの詳細な関係性を把握しました。\n",
    "\n",
    "### 主要発見事項\n",
    "1. **相関構造**: 隣接スプレッド間に中程度から強い相関関係\n",
    "2. **共和分関係**: 一部ペアで長期均衡関係を確認\n",
    "3. **主成分構造**: 第1主成分が全変動の大部分を説明\n",
    "4. **裁定機会**: 複数の統計的裁定戦略が実装可能\n",
    "\n",
    "### 次の分析ステップ\n",
    "1. **ボラティリティモデリング**: GARCH系モデルによるリスク分析\n",
    "2. **機械学習予測**: より高度なパターン認識と予測\n",
    "3. **取引戦略構築**: 実際の売買ルールとリスク管理\n",
    "4. **バックテスト実行**: 歴史的データでの戦略検証\n",
    "5. **パフォーマンス評価**: リスク調整後リターンの評価\n",
    "\n",
    "次のノートブック `3_adjacent_spreads_volatility_modeling.ipynb` で、ボラティリティクラスタリングとリスク特性の詳細分析を実施します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}