{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LME銅 Cash/3Mスプレッド 時系列分析\n",
    "\n",
    "## 分析目的\n",
    "このノートブックは、LME銅Cash/3Mスプレッドの時系列特性を詳細に分析し、予測モデリングの基盤を構築します。\n",
    "\n",
    "### 学習目標:\n",
    "1. **定常性の理解**: スプレッドデータの統計的性質\n",
    "2. **自己相関の分析**: 過去の値と現在の値の関係\n",
    "3. **差分化の効果**: 非定常データを定常化する手法\n",
    "4. **モデル選択**: ARIMA等のパラメータ決定\n",
    "5. **予測精度**: 時系列予測の評価方法\n",
    "\n",
    "### 教科書的アプローチ:\n",
    "各ステップを小さく分割し、理論と実践を組み合わせて学習していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cash/3M スプレッド時系列分析\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "\n",
    "# 時系列分析ライブラリ\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from arch import arch_model\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# データベース設定\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'lme_copper_db'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'password'),\n",
    "    'port': os.getenv('DB_PORT', '5432')\n",
    "}\n",
    "\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "# スタイル設定\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "print(\"Cash/3M スプレッド時系列分析\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: データ読み込みと基本的な可視化\n",
    "\n",
    "### 理論:\n",
    "時系列分析の第一歩は、データの基本的な特性を理解することです。\n",
    "- **トレンド**: 長期的な方向性\n",
    "- **季節性**: 定期的なパターン\n",
    "- **不規則変動**: ランダムな変動"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def load_comprehensive_spread_data():\n    \"\"\"\n    複数のデータソースからスプレッドデータを読み込み\n    \"\"\"\n    print(\"データ読み込み開始...\")\n    \n    data_sources = [\n        {\n            'name': 'lme_copper_prices',\n            'spread_query': \"\"\"\n                SELECT \n                    trade_date,\n                    close_price as spread_value\n                FROM lme_copper_prices \n                WHERE ric_code = 'CMCU0-3'\n                ORDER BY trade_date\n            \"\"\",\n            'component_query': \"\"\"\n                SELECT \n                    p1.trade_date,\n                    p1.close_price as cash_price,\n                    p2.close_price as future_3m_price,\n                    (p1.close_price - p2.close_price) as spread_value\n                FROM \n                    (SELECT trade_date, close_price FROM lme_copper_prices WHERE ric_code = 'CMCU0') p1\n                INNER JOIN \n                    (SELECT trade_date, close_price FROM lme_copper_prices WHERE ric_code = 'CMCU3') p2\n                    ON p1.trade_date = p2.trade_date\n                ORDER BY p1.trade_date\n            \"\"\"\n        },\n        {\n            'name': 'lme_copper_futures',\n            'spread_query': \"\"\"\n                SELECT \n                    trade_date,\n                    close_price as spread_value\n                FROM lme_copper_futures \n                WHERE ric_code = 'CMCU0-3'\n                ORDER BY trade_date\n            \"\"\",\n            'component_query': \"\"\"\n                SELECT \n                    p1.trade_date,\n                    p1.close_price as cash_price,\n                    p2.close_price as future_3m_price,\n                    (p1.close_price - p2.close_price) as spread_value\n                FROM \n                    (SELECT trade_date, close_price FROM lme_copper_futures WHERE ric_code = 'CMCU0') p1\n                INNER JOIN \n                    (SELECT trade_date, close_price FROM lme_copper_futures WHERE ric_code = 'CMCU3') p2\n                    ON p1.trade_date = p2.trade_date\n                ORDER BY p1.trade_date\n            \"\"\"\n        }\n    ]\n    \n    try:\n        engine = create_engine(connection_string)\n        \n        for source in data_sources:\n            print(f\"\\n🔍 {source['name']}からデータ取得中...\")\n            \n            # 直接スプレッドデータを試行\n            try:\n                df = pd.read_sql(source['spread_query'], engine)\n                if df is not None and len(df) > 0:\n                    df['trade_date'] = pd.to_datetime(df['trade_date'])\n                    df.set_index('trade_date', inplace=True)\n                    print(f\"✅ 直接スプレッドデータ: {len(df)} レコード\")\n                    return df\n            except Exception as e:\n                print(f\"   直接スプレッドデータ取得失敗: {e}\")\n            \n            # コンポーネントから計算を試行\n            try:\n                df = pd.read_sql(source['component_query'], engine)\n                if df is not None and len(df) > 0:\n                    df['trade_date'] = pd.to_datetime(df['trade_date'])\n                    df.set_index('trade_date', inplace=True)\n                    print(f\"✅ コンポーネント計算: {len(df)} レコード\")\n                    return df\n            except Exception as e:\n                print(f\"   コンポーネント計算失敗: {e}\")\n    \n    except Exception as e:\n        print(f\"⚠️ データベース接続失敗: {e}\")\n    \n    # 実データ取得失敗\n    print(\"\\n🚨 CRITICAL ERROR: 実データの取得に完全に失敗しました\")\n    print(\"\\n【データベース診断】\")\n    print(\"1. データベース接続: 失敗\")\n    print(\"2. テーブル存在確認: 未完了\") \n    print(\"3. データクエリ: 失敗\")\n    print(\"\\n【解決方法】\")\n    print(\"- データベースにCash/3Mスプレッドデータが存在するか確認してください\")\n    print(\"- 以下のコマンドでデータを確認:\")\n    print(\"  psql -h localhost -U postgres -d lme_copper_db\")\n    print(\"  \\\\dt\")\n    print(\"  SELECT COUNT(*) FROM lme_copper_prices WHERE ric_code = 'CMCU0-3';\")\n    print(\"- データ収集スクリプトを実行してデータを追加してください\")\n    print(\"\\n【トラブルシューティング】\")\n    print(\"1. PostgreSQLが起動しているか確認: pg_ctl status\")\n    print(\"2. 接続設定を確認: .envファイルのDB_HOST, DB_NAME, DB_USER, DB_PASSWORD\") \n    print(\"3. テーブル作成スクリプトが実行されているか確認\")\n    print(\"\\n分析を続行できません。上記の問題を解決してから再実行してください。\")\n    \n    return None\n\n# データ読み込み実行\ndf = load_comprehensive_spread_data()\nif df is None:\n    print(\"\\n実データが利用できないため、これ以降の分析セルは実行できません。\")\nelse:\n    print(f\"\\n📊 最終データセット: {len(df)} レコード\")\n    print(df.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ACF/PACF分析\nif df is not None:\n    spread_series = df['spread_value']\n    \n    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n    \n    # 元データのACF\n    plot_acf(spread_series, ax=axes[0,0], lags=40, alpha=0.05)\n    axes[0,0].set_title('Autocorrelation Function (ACF) - Original Data', fontsize=12, fontweight='bold')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # 元データのPACF\n    plot_pacf(spread_series, ax=axes[0,1], lags=40, alpha=0.05)\n    axes[0,1].set_title('Partial Autocorrelation Function (PACF) - Original Data', fontsize=12, fontweight='bold')\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # 一次差分のACF（定常化のため）\n    spread_diff = spread_series.diff().dropna()\n    plot_acf(spread_diff, ax=axes[1,0], lags=40, alpha=0.05)\n    axes[1,0].set_title('Autocorrelation Function (ACF) - First Difference', fontsize=12, fontweight='bold')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # 一次差分のPACF\n    plot_pacf(spread_diff, ax=axes[1,1], lags=40, alpha=0.05)\n    axes[1,1].set_title('Partial Autocorrelation Function (PACF) - First Difference', fontsize=12, fontweight='bold')\n    axes[1,1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 一次差分の定常性検定\n    print(\"\\n\" + \"=\"*70)\n    print(\"一次差分データの定常性検定\")\n    print(\"=\"*70)\n    \n    adf_diff = adf_test(spread_diff, \"Cash/3M Spread (一次差分)\")\n    kpss_diff = kpss_test(spread_diff, \"Cash/3M Spread (一次差分)\")\n    \n    # 数値的な相関分析\n    print(f\"\\n{'='*50}\")\n    print(\"相関分析サマリー\")\n    print(f\"{'='*50}\")\n    \n    # 主要ラグの自己相関\n    from statsmodels.tsa.stattools import acf, pacf\n    \n    acf_values = acf(spread_diff, nlags=10, alpha=0.05)\n    pacf_values = pacf(spread_diff, nlags=10, alpha=0.05)\n    \n    print(\"主要ラグの自己相関 (一次差分):\")\n    for i in range(1, 6):\n        print(f\"ラグ{i}: ACF = {acf_values[0][i]:.3f}, PACF = {pacf_values[0][i]:.3f}\")\n    \n    # Ljung-Box検定（残差の独立性検定）\n    lb_test = acorr_ljungbox(spread_diff, lags=10, return_df=True)\n    print(f\"\\nLjung-Box検定 (残差の独立性):\")\n    print(f\"p値 (ラグ10): {lb_test['lb_pvalue'].iloc[-1]:.6f}\")\n    \n    if lb_test['lb_pvalue'].iloc[-1] > 0.05:\n        print(\"✅ 残差は独立（ホワイトノイズ的）\")\n    else:\n        print(\"❌ 残差に自己相関あり（追加モデリングが必要）\")\nelse:\n    print(\"\\n⚠️ データが利用できないため、ACF/PACF分析をスキップします。\")\n    print(\"上記のデータ読み込みの問題を解決してから再実行してください。\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 定常性検定（データが存在する場合のみ実行）\nif df is not None:\n    spread_series = df['spread_value']\n    \n    def adf_test(series, title=\"\"):\n        \"\"\"\n        ADF検定（拡張ディッキー・フラー検定）の実行\n        H0: 単位根あり（非定常）\n        H1: 単位根なし（定常）\n        \"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"ADF検定結果: {title}\")\n        print(f\"{'='*50}\")\n        \n        result = adfuller(series, autolag='AIC')\n        \n        print(f\"ADF統計量: {result[0]:.6f}\")\n        print(f\"p値: {result[1]:.6f}\")\n        print(f\"使用ラグ数: {result[2]}\")\n        print(f\"観測数: {result[3]}\")\n        \n        print(\"\\n臨界値:\")\n        for key, value in result[4].items():\n            print(f\"\\t{key}: {value:.3f}\")\n        \n        # 結果の解釈\n        if result[1] <= 0.05:\n            print(f\"\\n✅ 結果: p値 = {result[1]:.6f} < 0.05\")\n            print(\"帰無仮説を棄却 → 時系列は定常です\")\n        else:\n            print(f\"\\n❌ 結果: p値 = {result[1]:.6f} > 0.05\")\n            print(\"帰無仮説を棄却できない → 時系列は非定常です\")\n        \n        return result\n\n    def kpss_test(series, title=\"\"):\n        \"\"\"\n        KPSS検定の実行\n        H0: 定常\n        H1: 非定常\n        \"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"KPSS検定結果: {title}\")\n        print(f\"{'='*50}\")\n        \n        result = kpss(series, regression='c')\n        \n        print(f\"KPSS統計量: {result[0]:.6f}\")\n        print(f\"p値: {result[1]:.6f}\")\n        print(f\"使用ラグ数: {result[2]}\")\n        \n        print(\"\\n臨界値:\")\n        for key, value in result[3].items():\n            print(f\"\\t{key}: {value:.3f}\")\n        \n        # 結果の解釈\n        if result[1] <= 0.05:\n            print(f\"\\n❌ 結果: p値 = {result[1]:.6f} < 0.05\")\n            print(\"帰無仮説を棄却 → 時系列は非定常です\")\n        else:\n            print(f\"\\n✅ 結果: p値 = {result[1]:.6f} > 0.05\")\n            print(\"帰無仮説を棄却できない → 時系列は定常です\")\n        \n        return result\n\n    # ADF検定\n    adf_result = adf_test(spread_series, \"Cash/3M Spread (元データ)\")\n    \n    # KPSS検定\n    kpss_result = kpss_test(spread_series, \"Cash/3M Spread (元データ)\")\n    \n    print(f\"\\n{'='*70}\")\n    print(\"定常性検定まとめ\")\n    print(f\"{'='*70}\")\n    print(\"検定結果:\")\n    print(f\"ADF検定: {'定常' if adf_result[1] <= 0.05 else '非定常'}\")\n    print(f\"KPSS検定: {'定常' if kpss_result[1] > 0.05 else '非定常'}\")\n    \n    if adf_result[1] <= 0.05 and kpss_result[1] > 0.05:\n        print(\"\\n✅ 総合判定: 時系列は定常です\")\n    elif adf_result[1] > 0.05 and kpss_result[1] <= 0.05:\n        print(\"\\n❌ 総合判定: 時系列は非定常です\")\n    else:\n        print(\"\\n⚠️ 総合判定: 検定結果が矛盾しています。追加検証が必要です\")\nelse:\n    print(\"\\n⚠️ データが利用できないため、定常性検定をスキップします。\")\n    print(\"上記のデータ読み込みの問題を解決してから再実行してください。\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 自己相関関数 (ACF) と偏自己相関関数 (PACF) の分析\n",
    "\n",
    "### 理論:\n",
    "**自己相関関数 (ACF):**\n",
    "- 時系列と自分自身のラグ版との相関\n",
    "- MA(q)モデルのqを決定するのに使用\n",
    "\n",
    "**偏自己相関関数 (PACF):**\n",
    "- 中間のラグの影響を除いた直接的な相関\n",
    "- AR(p)モデルのpを決定するのに使用\n",
    "\n",
    "**パターン識別:**\n",
    "- AR(p): PACF がp次で急激に減衰、ACF は指数的減衰\n",
    "- MA(q): ACF がq次で急激に減衰、PACF は指数的減衰\n",
    "- ARMA(p,q): 両方とも指数的減衰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ACF/PACF分析\nif df is not None:\n    spread_series = df['spread_value']\n    \n    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n    \n    # 元データのACF\n    plot_acf(spread_series, ax=axes[0,0], lags=40, alpha=0.05)\n    axes[0,0].set_title('自己相関関数 (ACF) - 元データ', fontsize=12, fontweight='bold')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # 元データのPACF\n    plot_pacf(spread_series, ax=axes[0,1], lags=40, alpha=0.05)\n    axes[0,1].set_title('偏自己相関関数 (PACF) - 元データ', fontsize=12, fontweight='bold')\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # 一次差分のACF（定常化のため）\n    spread_diff = spread_series.diff().dropna()\n    plot_acf(spread_diff, ax=axes[1,0], lags=40, alpha=0.05)\n    axes[1,0].set_title('自己相関関数 (ACF) - 一次差分', fontsize=12, fontweight='bold')\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # 一次差分のPACF\n    plot_pacf(spread_diff, ax=axes[1,1], lags=40, alpha=0.05)\n    axes[1,1].set_title('偏自己相関関数 (PACF) - 一次差分', fontsize=12, fontweight='bold')\n    axes[1,1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 一次差分の定常性検定\n    print(\"\\n\" + \"=\"*70)\n    print(\"一次差分データの定常性検定\")\n    print(\"=\"*70)\n    \n    adf_diff = adf_test(spread_diff, \"Cash/3M Spread (一次差分)\")\n    kpss_diff = kpss_test(spread_diff, \"Cash/3M Spread (一次差分)\")\n    \n    # 数値的な相関分析\n    print(f\"\\n{'='*50}\")\n    print(\"相関分析サマリー\")\n    print(f\"{'='*50}\")\n    \n    # 主要ラグの自己相関\n    from statsmodels.tsa.stattools import acf, pacf\n    \n    acf_values = acf(spread_diff, nlags=10, alpha=0.05)\n    pacf_values = pacf(spread_diff, nlags=10, alpha=0.05)\n    \n    print(\"主要ラグの自己相関 (一次差分):\")\n    for i in range(1, 6):\n        print(f\"ラグ{i}: ACF = {acf_values[0][i]:.3f}, PACF = {pacf_values[0][i]:.3f}\")\n    \n    # Ljung-Box検定（残差の独立性検定）\n    lb_test = acorr_ljungbox(spread_diff, lags=10, return_df=True)\n    print(f\"\\nLjung-Box検定 (残差の独立性):\")\n    print(f\"p値 (ラグ10): {lb_test['lb_pvalue'].iloc[-1]:.6f}\")\n    \n    if lb_test['lb_pvalue'].iloc[-1] > 0.05:\n        print(\"✅ 残差は独立（ホワイトノイズ的）\")\n    else:\n        print(\"❌ 残差に自己相関あり（追加モデリングが必要）\")\nelse:\n    print(\"\\n⚠️ データが利用できないため、ACF/PACF分析をスキップします。\")\n    print(\"上記のデータ読み込みの問題を解決してから再実行してください。\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ARIMA モデルの選択と推定\n",
    "\n",
    "### 理論:\n",
    "**ARIMA(p,d,q) モデル:**\n",
    "- p: 自己回帰項の次数 (AR)\n",
    "- d: 差分の次数 (I: Integrated)\n",
    "- q: 移動平均項の次数 (MA)\n",
    "\n",
    "**モデル選択基準:**\n",
    "- **AIC (赤池情報量基準)**: 小さいほど良い\n",
    "- **BIC (ベイズ情報量基準)**: 小さいほど良い\n",
    "- **残差診断**: ホワイトノイズ性の確認"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "        # Q-Qプロット\n        from scipy.stats import probplot\n        probplot(residuals, dist=\"norm\", plot=axes[1,0])\n        axes[1,0].set_title('Q-Q Plot (Normality Test)', fontsize=12, fontweight='bold')\n        axes[1,0].grid(True, alpha=0.3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 予測と評価\n",
    "\n",
    "### 理論:\n",
    "**予測評価指標:**\n",
    "- **MAE**: 平均絶対誤差\n",
    "- **RMSE**: 二乗平均平方根誤差  \n",
    "- **MAPE**: 平均絶対パーセント誤差\n",
    "- **方向性精度**: 上昇/下降の予測的中率\n",
    "\n",
    "**時系列クロスバリデーション:**\n",
    "- 時系列の順序を保持した分割\n",
    "- ウォークフォワード分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測と評価\n",
    "if spread_data is not None and best_model is not None:\n",
    "    # データ分割（最後の30日をテスト用）\n",
    "    test_size = 30\n",
    "    train_data = spread_series[:-test_size]\n",
    "    test_data = spread_series[-test_size:]\n",
    "    \n",
    "    print(f\"学習データ: {len(train_data)}日\")\n",
    "    print(f\"テストデータ: {len(test_data)}日\")\n",
    "    \n",
    "    # モデル再学習（学習データのみ）\n",
    "    train_model = ARIMA(train_data, order=best_order_aic)\n",
    "    fitted_train_model = train_model.fit()\n",
    "    \n",
    "    # 予測実行\n",
    "    forecast_result = fitted_train_model.forecast(steps=test_size, alpha=0.05)\n",
    "    forecast_values = forecast_result\n",
    "    \n",
    "    # 信頼区間取得\n",
    "    forecast_ci = fitted_train_model.get_forecast(steps=test_size, alpha=0.05)\n",
    "    conf_int = forecast_ci.conf_int()\n",
    "    \n",
    "    # 予測評価指標計算\n",
    "    mae = np.mean(np.abs(test_data - forecast_values))\n",
    "    rmse = np.sqrt(np.mean((test_data - forecast_values)**2))\n",
    "    mape = np.mean(np.abs((test_data - forecast_values) / test_data)) * 100\n",
    "    \n",
    "    # 方向性精度\n",
    "    actual_direction = np.sign(test_data.diff().dropna())\n",
    "    predicted_direction = np.sign(pd.Series(forecast_values, index=test_data.index).diff().dropna())\n",
    "    directional_accuracy = (actual_direction == predicted_direction).mean()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"予測精度評価\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MAE (平均絶対誤差): {mae:.3f}\")\n",
    "    print(f\"RMSE (二乗平均平方根誤差): {rmse:.3f}\")\n",
    "    print(f\"MAPE (平均絶対パーセント誤差): {mape:.3f}%\")\n",
    "    print(f\"方向性精度: {directional_accuracy:.3f} ({directional_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # 予測結果の可視化\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # 予測vs実績\n",
    "    ax1.plot(train_data.index[-60:], train_data[-60:], label='学習データ', color='blue', alpha=0.7)\n",
    "    ax1.plot(test_data.index, test_data, label='実際の値', color='green', linewidth=2)\n",
    "    ax1.plot(test_data.index, forecast_values, label='予測値', color='red', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # 信頼区間\n",
    "    ax1.fill_between(test_data.index, \n",
    "                     conf_int.iloc[:, 0], \n",
    "                     conf_int.iloc[:, 1], \n",
    "                     color='red', alpha=0.2, label='95%信頼区間')\n",
    "    \n",
    "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax1.set_title(f'ARIMA{best_order_aic} 予測結果', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('スプレッド (USD/tonne)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 予測誤差\n",
    "    prediction_errors = test_data - forecast_values\n",
    "    ax2.plot(test_data.index, prediction_errors, marker='o', linestyle='-', color='red')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.7)\n",
    "    ax2.set_title('予測誤差', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('誤差 (USD/tonne)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 誤差統計\n",
    "    print(f\"\\n予測誤差統計:\")\n",
    "    print(f\"誤差の平均: {prediction_errors.mean():.3f}\")\n",
    "    print(f\"誤差の標準偏差: {prediction_errors.std():.3f}\")\n",
    "    print(f\"最大正誤差: {prediction_errors.max():.3f}\")\n",
    "    print(f\"最大負誤差: {prediction_errors.min():.3f}\")\n",
    "    \n",
    "    # 信頼区間カバレッジ\n",
    "    coverage = ((test_data >= conf_int.iloc[:, 0]) & \n",
    "                (test_data <= conf_int.iloc[:, 1])).mean()\n",
    "    print(f\"95%信頼区間カバレッジ: {coverage:.3f} ({coverage*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 結果の解釈とまとめ\n",
    "\n",
    "### 時系列分析から得られた知見の整理\n",
    "分析結果をトレーディング戦略に活用するための総合的な解釈を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定常化データをDataFrameに保存（次のステップで使用）\n",
    "if spread_data is not None:\n",
    "    # 分析結果のまとめ\n",
    "    stationary_data = pd.DataFrame(index=spread_data.index)\n",
    "    stationary_data['original_spread'] = spread_data['spread_price']\n",
    "    stationary_data['diff_spread'] = spread_data['spread_price'].diff()\n",
    "    \n",
    "    # 移動統計（ローリング平均・標準偏差）\n",
    "    stationary_data['rolling_mean_20'] = spread_data['spread_price'].rolling(20).mean()\n",
    "    stationary_data['rolling_std_20'] = spread_data['spread_price'].rolling(20).std()\n",
    "    \n",
    "    # Z-score（標準化）\n",
    "    stationary_data['zscore'] = (spread_data['spread_price'] - stationary_data['rolling_mean_20']) / stationary_data['rolling_std_20']\n",
    "    \n",
    "    # 欠損値除去\n",
    "    stationary_data = stationary_data.dropna()\n",
    "    \n",
    "    print(\"定常化データ（stationary_data）を作成しました\")\n",
    "    print(f\"データ期間: {stationary_data.index.min().date()} to {stationary_data.index.max().date()}\")\n",
    "    print(f\"データ数: {len(stationary_data)}\")\n",
    "    \n",
    "    # 最新10日間のデータ表示\n",
    "    print(\"\\n最新データ（最後の10日間）:\")\n",
    "    print(stationary_data.tail(10).round(3))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"時系列分析 総合まとめ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(\"\\n【定常性について】\")\n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"✅ 元データは定常的 → 差分化は不要\")\n",
    "        print(\"   → レベル値での予測が可能\")\n",
    "    else:\n",
    "        print(\"❌ 元データは非定常 → 差分化が必要\")\n",
    "        print(\"   → 変化量での予測が適している\")\n",
    "    \n",
    "    print(\"\\n【モデリング結果】\")\n",
    "    if best_model is not None:\n",
    "        print(f\"最適モデル: ARIMA{best_order_aic}\")\n",
    "        print(f\"AIC: {best_aic:.2f}\")\n",
    "        print(f\"予測精度 (MAE): {mae:.3f}\")\n",
    "        print(f\"方向性精度: {directional_accuracy*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n【トレーディングへの示唆】\")\n",
    "    \n",
    "    # 現在のスプレッド位置\n",
    "    current_spread = stationary_data['original_spread'].iloc[-1]\n",
    "    current_zscore = stationary_data['zscore'].iloc[-1]\n",
    "    \n",
    "    print(f\"現在のスプレッド: ${current_spread:.1f}\")\n",
    "    print(f\"現在のZ-score: {current_zscore:.2f}\")\n",
    "    \n",
    "    if abs(current_zscore) > 2:\n",
    "        print(\"⚠️ 極端な水準 → 逆張り戦略を検討\")\n",
    "        if current_zscore > 2:\n",
    "            print(\"   → 強いバックワーデーション（売り検討）\")\n",
    "        else:\n",
    "            print(\"   → 強いコンタンゴ（買い検討）\")\n",
    "    elif abs(current_zscore) > 1:\n",
    "        print(\"📊 やや極端な水準 → 注意深く監視\")\n",
    "    else:\n",
    "        print(\"✅ 正常範囲 → トレンドフォロー戦略\")\n",
    "    \n",
    "    # 予測精度に基づく信頼性\n",
    "    if directional_accuracy > 0.6:\n",
    "        print(f\"\\n✅ 予測精度が高い（{directional_accuracy*100:.1f}%）→ モデル信号を重視\")\n",
    "    elif directional_accuracy > 0.5:\n",
    "        print(f\"\\n📊 予測精度は中程度（{directional_accuracy*100:.1f}%）→ 他の指標と組み合わせ\")\n",
    "    else:\n",
    "        print(f\"\\n❌ 予測精度が低い（{directional_accuracy*100:.1f}%）→ モデル改善が必要\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"次のステップ: 機械学習モデルとの比較・アンサンブル\")\n",
    "    print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}