{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMEéŠ… Cash/3Mã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°åˆ†æ\n",
    "\n",
    "## åˆ†æç›®çš„\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Cash/3Mã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã«å¯¾ã—ã¦ä»¥ä¸‹ã®åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n",
    "\n",
    "### å®Ÿè£…ãƒ¢ãƒ‡ãƒ«:\n",
    "1. **ARIMA/GARCH**: æ™‚ç³»åˆ—ã®å¹³å‡ãƒ»åˆ†æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "2. **Random Forest**: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ©Ÿæ¢°å­¦ç¿’\n",
    "3. **XGBoost**: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°\n",
    "4. **LSTM**: æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äºˆæ¸¬\n",
    "5. **Prophet**: Facebookã®æ™‚ç³»åˆ—äºˆæ¸¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "6. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›\n",
    "\n",
    "### è©•ä¾¡é …ç›®:\n",
    "- äºˆæ¸¬ç²¾åº¦ï¼ˆMAE, RMSE, MAPEï¼‰\n",
    "- æ–¹å‘æ€§ç²¾åº¦ï¼ˆä¸Šæ˜‡/ä¸‹é™ã®çš„ä¸­ç‡ï¼‰\n",
    "- ãƒªã‚¹ã‚¯èª¿æ•´ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªï¼‰\n",
    "- å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# æ™‚ç³»åˆ—åˆ†æ\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(\"âœ“ TensorFlow available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"TensorFlow not available - LSTM models will be skipped\")\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "    print(\"âœ“ Prophet available\")\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "    print(\"Prophet not available - Prophet models will be skipped\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'lme_copper_db'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'password'),\n",
    "    'port': os.getenv('DB_PORT', '5432')\n",
    "}\n",
    "\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "model_dir = '/Users/Yusuke/claude-code/RefinitivDB/models/cash_3m_spread'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(\"Ready for comprehensive Cash/3M spread modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ç‰¹å¾´é‡ä½œæˆ\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Cash/3Mã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿\n",
    "        spread_query = \"\"\"\n",
    "        SELECT \n",
    "            trade_date,\n",
    "            last_price as spread_price,\n",
    "            volume as spread_volume\n",
    "        FROM lme_copper_prices\n",
    "        WHERE price_type = 'CASH_3M_SPREAD'\n",
    "        AND last_price IS NOT NULL\n",
    "        ORDER BY trade_date\n",
    "        \"\"\"\n",
    "        \n",
    "        spread_df = pd.read_sql_query(spread_query, engine)\n",
    "        \n",
    "        # 3Mã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆè£œåŠ©æƒ…å ±ã¨ã—ã¦ï¼‰\n",
    "        outright_query = \"\"\"\n",
    "        SELECT \n",
    "            trade_date,\n",
    "            last_price as outright_price,\n",
    "            volume as outright_volume\n",
    "        FROM lme_copper_prices\n",
    "        WHERE price_type = '3M_OUTRIGHT'\n",
    "        AND last_price IS NOT NULL\n",
    "        ORDER BY trade_date\n",
    "        \"\"\"\n",
    "        \n",
    "        outright_df = pd.read_sql_query(outright_query, engine)\n",
    "        engine.dispose()\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸\n",
    "        df = pd.merge(spread_df, outright_df, on='trade_date', how='inner')\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df.set_index('trade_date', inplace=True)\n",
    "        \n",
    "        # æ•°å€¤å‹å¤‰æ›\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df = df.dropna()\n",
    "        \n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶\")\n",
    "        print(f\"æœŸé–“: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # åŸºæœ¬çš„ãªå¤‰åŒ–ç‡\n",
    "    features_df['spread_change'] = features_df['spread_price'].diff()\n",
    "    features_df['spread_pct_change'] = features_df['spread_price'].pct_change()\n",
    "    features_df['outright_change'] = features_df['outright_price'].diff()\n",
    "    features_df['outright_pct_change'] = features_df['outright_price'].pct_change()\n",
    "    \n",
    "    # ç§»å‹•å¹³å‡\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        features_df[f'spread_ma_{window}'] = features_df['spread_price'].rolling(window).mean()\n",
    "        features_df[f'outright_ma_{window}'] = features_df['outright_price'].rolling(window).mean()\n",
    "        \n",
    "        # ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä¹–é›¢\n",
    "        features_df[f'spread_ma_dev_{window}'] = features_df['spread_price'] - features_df[f'spread_ma_{window}']\n",
    "    \n",
    "    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "    for window in [10, 20, 30]:\n",
    "        features_df[f'spread_vol_{window}'] = features_df['spread_change'].rolling(window).std()\n",
    "        features_df[f'outright_vol_{window}'] = features_df['outright_change'].rolling(window).std()\n",
    "    \n",
    "    # RSI\n",
    "    def calculate_rsi(prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    features_df['spread_rsi'] = calculate_rsi(features_df['spread_price'])\n",
    "    features_df['outright_rsi'] = calculate_rsi(features_df['outright_price'])\n",
    "    \n",
    "    # Z-score\n",
    "    for window in [20, 50]:\n",
    "        mean_val = features_df['spread_price'].rolling(window).mean()\n",
    "        std_val = features_df['spread_price'].rolling(window).std()\n",
    "        features_df[f'spread_zscore_{window}'] = (features_df['spread_price'] - mean_val) / std_val\n",
    "    \n",
    "    # ãƒ©ã‚°ç‰¹å¾´é‡\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        features_df[f'spread_lag_{lag}'] = features_df['spread_price'].shift(lag)\n",
    "        features_df[f'spread_change_lag_{lag}'] = features_df['spread_change'].shift(lag)\n",
    "    \n",
    "    # å‡ºæ¥é«˜é–¢é€£\n",
    "    features_df['volume_ratio'] = features_df['spread_volume'] / features_df['outright_volume']\n",
    "    features_df['spread_volume_ma_5'] = features_df['spread_volume'].rolling(5).mean()\n",
    "    features_df['volume_change'] = features_df['spread_volume'].pct_change()\n",
    "    \n",
    "    # æ™‚é–“ç‰¹å¾´é‡\n",
    "    features_df['month'] = features_df.index.month\n",
    "    features_df['quarter'] = features_df.index.quarter\n",
    "    features_df['day_of_week'] = features_df.index.dayofweek\n",
    "    features_df['day_of_month'] = features_df.index.day\n",
    "    \n",
    "    # æœˆãƒ»æ›œæ—¥ã®ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯å¤‰æ›\n",
    "    features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['dow_sin'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    features_df['dow_cos'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    \n",
    "    # å¸‚å ´çŠ¶æ³æŒ‡æ¨™\n",
    "    features_df['backwardation'] = (features_df['spread_price'] > 0).astype(int)\n",
    "    features_df['extreme_backwardation'] = (features_df['spread_price'] > features_df['spread_price'].quantile(0.8)).astype(int)\n",
    "    features_df['extreme_contango'] = (features_df['spread_price'] < features_df['spread_price'].quantile(0.2)).astype(int)\n",
    "    \n",
    "    # æ¬ æå€¤å‡¦ç†\n",
    "    features_df = features_df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(f\"ç‰¹å¾´é‡ä½œæˆå®Œäº†: {features_df.shape[1]}å€‹ã®ç‰¹å¾´é‡\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Ÿè¡Œ\n",
    "raw_data = load_and_prepare_data()\n",
    "if raw_data is not None:\n",
    "    data = create_features(raw_data)\n",
    "    print(f\"\\næœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚·ã‚§ã‚¤ãƒ—: {data.shape}\")\n",
    "    print(f\"ç‰¹å¾´é‡ä¸€è¦§: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«1: ARIMA + GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA + GARCH ãƒ¢ãƒ‡ãƒ«\n",
    "class ARIMAGARCHModel:\n",
    "    def __init__(self):\n",
    "        self.arima_model = None\n",
    "        self.garch_model = None\n",
    "        self.fitted_arima = None\n",
    "        self.fitted_garch = None\n",
    "        \n",
    "    def fit(self, data, arima_order=(1,1,1), garch_p=1, garch_q=1):\n",
    "        \"\"\"ARIMA + GARCH ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\"\"\"\n",
    "        try:\n",
    "            # ARIMA ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°\n",
    "            self.arima_model = ARIMA(data, order=arima_order)\n",
    "            self.fitted_arima = self.arima_model.fit()\n",
    "            \n",
    "            # ARIMAæ®‹å·®ã‚’å–å¾—\n",
    "            residuals = self.fitted_arima.resid\n",
    "            \n",
    "            # GARCH ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆæ®‹å·®ã®åˆ†æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼‰\n",
    "            self.garch_model = arch_model(residuals, vol='GARCH', p=garch_p, q=garch_q)\n",
    "            self.fitted_garch = self.garch_model.fit(disp='off')\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ARIMA-GARCH ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"äºˆæ¸¬å®Ÿè¡Œ\"\"\"\n",
    "        if self.fitted_arima is None:\n",
    "            return None, None\n",
    "        \n",
    "        # ARIMAäºˆæ¸¬\n",
    "        arima_forecast = self.fitted_arima.forecast(steps=steps)\n",
    "        \n",
    "        # GARCHäºˆæ¸¬ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰\n",
    "        if self.fitted_garch is not None:\n",
    "            garch_forecast = self.fitted_garch.forecast(horizon=steps)\n",
    "            volatility = np.sqrt(garch_forecast.variance.iloc[-1, :].values)\n",
    "        else:\n",
    "            volatility = None\n",
    "        \n",
    "        return arima_forecast, volatility\n",
    "\n",
    "# ARIMA-GARCHå®Ÿè¡Œ\n",
    "if data is not None:\n",
    "    print(\"ARIMA + GARCH ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data['spread_price'][:train_size]\n",
    "    test_data = data['spread_price'][train_size:]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "    arima_garch = ARIMAGARCHModel()\n",
    "    success = arima_garch.fit(train_data)\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… ARIMA-GARCHå­¦ç¿’å®Œäº†\")\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬\n",
    "        predictions_ag = []\n",
    "        volatilities_ag = []\n",
    "        \n",
    "        # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰äºˆæ¸¬\n",
    "        for i in range(len(test_data)):\n",
    "            pred, vol = arima_garch.predict(steps=1)\n",
    "            predictions_ag.append(pred.iloc[0] if hasattr(pred, 'iloc') else pred[0])\n",
    "            volatilities_ag.append(vol[0] if vol is not None else np.nan)\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ ï¼‰\n",
    "            if i < len(test_data) - 1:\n",
    "                updated_data = pd.concat([train_data, test_data.iloc[:i+1]])\n",
    "                arima_garch.fit(updated_data)\n",
    "        \n",
    "        predictions_ag = pd.Series(predictions_ag, index=test_data.index)\n",
    "        \n",
    "        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—\n",
    "        mae_ag = mean_absolute_error(test_data, predictions_ag)\n",
    "        rmse_ag = np.sqrt(mean_squared_error(test_data, predictions_ag))\n",
    "        \n",
    "        print(f\"ARIMA-GARCH MAE: {mae_ag:.3f}\")\n",
    "        print(f\"ARIMA-GARCH RMSE: {rmse_ag:.3f}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "        with open(f'{model_dir}/arima_garch_model.pkl', 'wb') as f:\n",
    "            pickle.dump(arima_garch, f)\n",
    "        \n",
    "    else:\n",
    "        predictions_ag = None\n",
    "        print(\"âŒ ARIMA-GARCHå­¦ç¿’å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest ãƒ¢ãƒ‡ãƒ«\n",
    "def prepare_ml_data(data, target_col='spread_price', forecast_horizon=1):\n",
    "    \"\"\"æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™\"\"\"\n",
    "    # ç‰¹å¾´é‡é¸æŠï¼ˆåŸºæœ¬çš„ãªæ•°å€¤ç‰¹å¾´é‡ã®ã¿ï¼‰\n",
    "    feature_cols = [col for col in data.columns if col not in [target_col] \n",
    "                   and not col.startswith('spread_lag')  # ãƒ©ã‚°ç‰¹å¾´é‡ã¯åˆ¥é€”å‡¦ç†\n",
    "                   and data[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # ãƒ©ã‚°ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãï¼‰\n",
    "    lag_features = [col for col in data.columns if col.startswith('spread_lag')]\n",
    "    feature_cols.extend(lag_features)\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    \n",
    "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆå°†æ¥ã®å€¤ã‚’äºˆæ¸¬ï¼‰\n",
    "    y = data[target_col].shift(-forecast_horizon)  # 1æœŸå…ˆã‚’äºˆæ¸¬\n",
    "    \n",
    "    # æ¬ æå€¤å‡¦ç†\n",
    "    valid_idx = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "if data is not None:\n",
    "    print(\"Random Forest ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    X, y, feature_cols = prepare_ml_data(data)\n",
    "    \n",
    "    print(f\"ç‰¹å¾´é‡æ•°: {len(feature_cols)}\")\n",
    "    print(f\"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Random Forest ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Time Series Cross Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, rf_params, n_iter=20, cv=tscv, \n",
    "        scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    rf_search.fit(X_train_scaled, y_train)\n",
    "    best_rf = rf_search.best_estimator_\n",
    "    \n",
    "    print(f\"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {rf_search.best_params_}\")\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    predictions_rf = best_rf.predict(X_test_scaled)\n",
    "    predictions_rf = pd.Series(predictions_rf, index=y_test.index)\n",
    "    \n",
    "    # è©•ä¾¡\n",
    "    mae_rf = mean_absolute_error(y_test, predictions_rf)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, predictions_rf))\n",
    "    r2_rf = r2_score(y_test, predictions_rf)\n",
    "    \n",
    "    print(f\"Random Forest MAE: {mae_rf:.3f}\")\n",
    "    print(f\"Random Forest RMSE: {rmse_rf:.3f}\")\n",
    "    print(f\"Random Forest RÂ²: {r2_rf:.3f}\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nä¸Šä½10ç‰¹å¾´é‡:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    joblib.dump(best_rf, f'{model_dir}/random_forest_model.pkl')\n",
    "    joblib.dump(scaler, f'{model_dir}/scaler.pkl')\n",
    "    \n",
    "    print(\"âœ… Random Forestå­¦ç¿’å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost ãƒ¢ãƒ‡ãƒ«\n",
    "if data is not None:\n",
    "    print(\"XGBoost ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "    \n",
    "    # XGBoost ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb_model, xgb_params, n_iter=20, cv=tscv,\n",
    "        scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_search.fit(X_train_scaled, y_train)\n",
    "    best_xgb = xgb_search.best_estimator_\n",
    "    \n",
    "    print(f\"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {xgb_search.best_params_}\")\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    predictions_xgb = best_xgb.predict(X_test_scaled)\n",
    "    predictions_xgb = pd.Series(predictions_xgb, index=y_test.index)\n",
    "    \n",
    "    # è©•ä¾¡\n",
    "    mae_xgb = mean_absolute_error(y_test, predictions_xgb)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "    r2_xgb = r2_score(y_test, predictions_xgb)\n",
    "    \n",
    "    print(f\"XGBoost MAE: {mae_xgb:.3f}\")\n",
    "    print(f\"XGBoost RMSE: {rmse_xgb:.3f}\")\n",
    "    print(f\"XGBoost RÂ²: {r2_xgb:.3f}\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡é‡è¦åº¦\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nä¸Šä½10ç‰¹å¾´é‡:\")\n",
    "    print(xgb_importance.head(10))\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    joblib.dump(best_xgb, f'{model_dir}/xgboost_model.pkl')\n",
    "    \n",
    "    print(\"âœ… XGBoostå­¦ç¿’å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«4: LSTM ï¼ˆæ·±å±¤å­¦ç¿’ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ãƒ¢ãƒ‡ãƒ«\n",
    "def create_lstm_sequences(data, sequence_length=30):\n",
    "    \"\"\"LSTMç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ä½œæˆ\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        sequences.append(data[i-sequence_length:i])\n",
    "        targets.append(data[i])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "if TENSORFLOW_AVAILABLE and data is not None:\n",
    "    print(\"LSTM ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "    \n",
    "    # LSTMç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    spread_values = data['spread_price'].values\n",
    "    \n",
    "    # æ­£è¦åŒ–\n",
    "    lstm_scaler = MinMaxScaler()\n",
    "    spread_scaled = lstm_scaler.fit_transform(spread_values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ\n",
    "    sequence_length = 30\n",
    "    X_seq, y_seq = create_lstm_sequences(spread_scaled, sequence_length)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_size = int(len(X_seq) * 0.8)\n",
    "    X_train_seq = X_seq[:train_size]\n",
    "    X_test_seq = X_seq[train_size:]\n",
    "    y_train_seq = y_seq[:train_size]\n",
    "    y_test_seq = y_seq[train_size:]\n",
    "    \n",
    "    # LSTMå½¢çŠ¶èª¿æ•´\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], X_train_seq.shape[1], 1))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))\n",
    "    \n",
    "    # LSTMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    history = model_lstm.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    predictions_lstm_scaled = model_lstm.predict(X_test_seq, verbose=0)\n",
    "    predictions_lstm = lstm_scaler.inverse_transform(predictions_lstm_scaled).flatten()\n",
    "    \n",
    "    # å®Ÿéš›ã®å€¤ã‚‚é€†å¤‰æ›\n",
    "    y_test_actual = lstm_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # è©•ä¾¡\n",
    "    mae_lstm = mean_absolute_error(y_test_actual, predictions_lstm)\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, predictions_lstm))\n",
    "    \n",
    "    print(f\"LSTM MAE: {mae_lstm:.3f}\")\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.3f}\")\n",
    "    \n",
    "    # äºˆæ¸¬çµæœã‚’Seriesã«å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª¿æ•´ï¼‰\n",
    "    test_dates = data.index[train_size + sequence_length:]\n",
    "    predictions_lstm = pd.Series(predictions_lstm, index=test_dates[:len(predictions_lstm)])\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    model_lstm.save(f'{model_dir}/lstm_model.h5')\n",
    "    joblib.dump(lstm_scaler, f'{model_dir}/lstm_scaler.pkl')\n",
    "    \n",
    "    print(\"âœ… LSTMå­¦ç¿’å®Œäº†\")\n",
    "    \n",
    "else:\n",
    "    predictions_lstm = None\n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        print(\"â­ï¸ TensorFlowãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€LSTMã‚’ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«5: Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet ãƒ¢ãƒ‡ãƒ«\n",
    "if PROPHET_AVAILABLE and data is not None:\n",
    "    print(\"Prophet ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "    \n",
    "    # Prophetç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    prophet_data = pd.DataFrame({\n",
    "        'ds': data.index,\n",
    "        'y': data['spread_price'].values\n",
    "    })\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    train_size = int(len(prophet_data) * 0.8)\n",
    "    prophet_train = prophet_data[:train_size]\n",
    "    prophet_test = prophet_data[train_size:]\n",
    "    \n",
    "    # Prophetãƒ¢ãƒ‡ãƒ«\n",
    "    model_prophet = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10.0\n",
    "    )\n",
    "    \n",
    "    # ã‚«ã‚¹ã‚¿ãƒ å­£ç¯€æ€§è¿½åŠ \n",
    "    model_prophet.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    model_prophet.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    model_prophet.fit(prophet_train)\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    future_prophet = model_prophet.make_future_dataframe(periods=len(prophet_test), freq='D')\n",
    "    forecast_prophet = model_prophet.predict(future_prophet)\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®äºˆæ¸¬æŠ½å‡º\n",
    "    predictions_prophet = forecast_prophet['yhat'].iloc[train_size:].values\n",
    "    predictions_prophet = pd.Series(predictions_prophet, index=prophet_test['ds'])\n",
    "    \n",
    "    # è©•ä¾¡\n",
    "    mae_prophet = mean_absolute_error(prophet_test['y'], predictions_prophet)\n",
    "    rmse_prophet = np.sqrt(mean_squared_error(prophet_test['y'], predictions_prophet))\n",
    "    \n",
    "    print(f\"Prophet MAE: {mae_prophet:.3f}\")\n",
    "    print(f\"Prophet RMSE: {rmse_prophet:.3f}\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    with open(f'{model_dir}/prophet_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model_prophet, f)\n",
    "    \n",
    "    print(\"âœ… Prophetå­¦ç¿’å®Œäº†\")\n",
    "    \n",
    "else:\n",
    "    predictions_prophet = None\n",
    "    if not PROPHET_AVAILABLE:\n",
    "        print(\"â­ï¸ ProphetãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Prophetã‚’ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«6: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n",
    "def create_ensemble_predictions(*predictions_list):\n",
    "    \"\"\"è¤‡æ•°ã®äºˆæ¸¬çµæœã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\"\"\"\n",
    "    # æœ‰åŠ¹ãªäºˆæ¸¬ã®ã¿ä½¿ç”¨\n",
    "    valid_predictions = [pred for pred in predictions_list if pred is not None]\n",
    "    \n",
    "    if not valid_predictions:\n",
    "        return None\n",
    "    \n",
    "    # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "    common_index = valid_predictions[0].index\n",
    "    for pred in valid_predictions[1:]:\n",
    "        common_index = common_index.intersection(pred.index)\n",
    "    \n",
    "    # å„äºˆæ¸¬ã‚’å…±é€šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆã‚ã›ã‚‹\n",
    "    aligned_predictions = []\n",
    "    for pred in valid_predictions:\n",
    "        aligned_predictions.append(pred.loc[common_index])\n",
    "    \n",
    "    # å˜ç´”å¹³å‡ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "    ensemble_pred = pd.concat(aligned_predictions, axis=1).mean(axis=1)\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "if data is not None:\n",
    "    print(\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ä½œæˆä¸­...\")\n",
    "    \n",
    "    # åˆ©ç”¨å¯èƒ½ãªäºˆæ¸¬ã‚’åé›†\n",
    "    available_predictions = []\n",
    "    model_names = []\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        available_predictions.append(predictions_rf)\n",
    "        model_names.append('Random Forest')\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        available_predictions.append(predictions_xgb)\n",
    "        model_names.append('XGBoost')\n",
    "    \n",
    "    if predictions_lstm is not None:\n",
    "        available_predictions.append(predictions_lstm)\n",
    "        model_names.append('LSTM')\n",
    "    \n",
    "    if predictions_prophet is not None:\n",
    "        available_predictions.append(predictions_prophet)\n",
    "        model_names.append('Prophet')\n",
    "    \n",
    "    if available_predictions:\n",
    "        ensemble_predictions = create_ensemble_predictions(*available_predictions)\n",
    "        \n",
    "        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡ï¼ˆå…±é€šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å®Ÿéš›å€¤ã¨æ¯”è¼ƒï¼‰\n",
    "        common_test_data = y_test.loc[ensemble_predictions.index]\n",
    "        \n",
    "        mae_ensemble = mean_absolute_error(common_test_data, ensemble_predictions)\n",
    "        rmse_ensemble = np.sqrt(mean_squared_error(common_test_data, ensemble_predictions))\n",
    "        \n",
    "        print(f\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {', '.join(model_names)}\")\n",
    "        print(f\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« MAE: {mae_ensemble:.3f}\")\n",
    "        print(f\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« RMSE: {rmse_ensemble:.3f}\")\n",
    "        \n",
    "        print(\"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬å®Œäº†\")\n",
    "    else:\n",
    "        ensemble_predictions = None\n",
    "        print(\"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ã®äºˆæ¸¬ãŒä¸è¶³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã¨å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ãƒ¢ãƒ‡ãƒ«ã®çµæœæ¯”è¼ƒ\n",
    "if data is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"å…¨ãƒ¢ãƒ‡ãƒ« ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # çµæœã¾ã¨ã‚\n",
    "    results = []\n",
    "    \n",
    "    if 'mae_ag' in locals() and predictions_ag is not None:\n",
    "        results.append(['ARIMA-GARCH', mae_ag, rmse_ag])\n",
    "    \n",
    "    if 'mae_rf' in locals():\n",
    "        results.append(['Random Forest', mae_rf, rmse_rf])\n",
    "    \n",
    "    if 'mae_xgb' in locals():\n",
    "        results.append(['XGBoost', mae_xgb, rmse_xgb])\n",
    "    \n",
    "    if 'mae_lstm' in locals():\n",
    "        results.append(['LSTM', mae_lstm, rmse_lstm])\n",
    "    \n",
    "    if 'mae_prophet' in locals():\n",
    "        results.append(['Prophet', mae_prophet, rmse_prophet])\n",
    "    \n",
    "    if 'mae_ensemble' in locals():\n",
    "        results.append(['Ensemble', mae_ensemble, rmse_ensemble])\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE'])\n",
    "    results_df = results_df.sort_values('MAE')\n",
    "    \n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
    "    best_model = results_df.iloc[0]['Model']\n",
    "    best_mae = results_df.iloc[0]['MAE']\n",
    "    \n",
    "    print(f\"\\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model} (MAE: {best_mae:.3f})\")\n",
    "    \n",
    "    # äºˆæ¸¬çµæœå¯è¦–åŒ–\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 15))\n",
    "    \n",
    "    # å®Ÿéš›ã®å€¤ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼‰\n",
    "    test_period_data = data['spread_price'][train_size:]\n",
    "    ax1.plot(test_period_data.index, test_period_data.values, \n",
    "             label='å®Ÿéš›ã®å€¤', linewidth=2, color='black', alpha=0.8)\n",
    "    \n",
    "    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        ax1.plot(predictions_rf.index, predictions_rf.values, \n",
    "                 label='Random Forest', linestyle='--', color=colors[0], alpha=0.7)\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        ax1.plot(predictions_xgb.index, predictions_xgb.values, \n",
    "                 label='XGBoost', linestyle='--', color=colors[1], alpha=0.7)\n",
    "    \n",
    "    if predictions_lstm is not None:\n",
    "        ax1.plot(predictions_lstm.index, predictions_lstm.values, \n",
    "                 label='LSTM', linestyle='--', color=colors[2], alpha=0.7)\n",
    "    \n",
    "    if predictions_prophet is not None:\n",
    "        ax1.plot(predictions_prophet.index, predictions_prophet.values, \n",
    "                 label='Prophet', linestyle='--', color=colors[3], alpha=0.7)\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        ax1.plot(ensemble_predictions.index, ensemble_predictions.values, \n",
    "                 label='Ensemble', linestyle='-', color=colors[4], linewidth=2, alpha=0.9)\n",
    "    \n",
    "    ax1.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    ax1.set_title('Cash/3M Spread äºˆæ¸¬çµæœæ¯”è¼ƒ', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ (USD/tonne)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAEæ¯”è¼ƒãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ\n",
    "    ax2.bar(results_df['Model'], results_df['MAE'], color=colors[:len(results_df)], alpha=0.7)\n",
    "    ax2.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥ MAE æ¯”è¼ƒ', fontsize=16, fontweight='bold')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ \n",
    "    for i, v in enumerate(results_df['MAE']):\n",
    "        ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../generated_images/cash_3m_spread_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # æ–¹å‘æ€§ç²¾åº¦è¨ˆç®—\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"æ–¹å‘æ€§ç²¾åº¦ (ä¸Šæ˜‡/ä¸‹é™ã®çš„ä¸­ç‡)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    actual_direction = np.sign(test_period_data.diff().dropna())\n",
    "    \n",
    "    direction_results = []\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        pred_direction = np.sign(predictions_rf.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['Random Forest', direction_acc])\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        pred_direction = np.sign(predictions_xgb.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['XGBoost', direction_acc])\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        pred_direction = np.sign(ensemble_predictions.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['Ensemble', direction_acc])\n",
    "    \n",
    "    direction_df = pd.DataFrame(direction_results, columns=['Model', 'Direction_Accuracy'])\n",
    "    direction_df['Direction_Accuracy_Pct'] = direction_df['Direction_Accuracy'] * 100\n",
    "    direction_df = direction_df.sort_values('Direction_Accuracy', ascending=False)\n",
    "    \n",
    "    print(direction_df[['Model', 'Direction_Accuracy_Pct']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Cash/3M ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ ãƒ¢ãƒ‡ãƒªãƒ³ã‚°å®Œäº†\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å˜ãªå–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "def run_trading_simulation(predictions, actual_prices, model_name=\"Model\"):\n",
    "    \"\"\"äºˆæ¸¬ã«åŸºã¥ãå–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\"\"\"\n",
    "    if predictions is None or len(predictions) == 0:\n",
    "        return None\n",
    "    \n",
    "    # å…±é€šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    common_idx = predictions.index.intersection(actual_prices.index)\n",
    "    pred_aligned = predictions.loc[common_idx]\n",
    "    actual_aligned = actual_prices.loc[common_idx]\n",
    "    \n",
    "    if len(pred_aligned) < 2:\n",
    "        return None\n",
    "    \n",
    "    # å–å¼•ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ\n",
    "    predicted_changes = pred_aligned.diff()\n",
    "    actual_changes = actual_aligned.diff()\n",
    "    \n",
    "    # ã‚·ã‚°ãƒŠãƒ«: äºˆæ¸¬ä¸Šæ˜‡ãªã‚‰è²·ã„(1)ã€äºˆæ¸¬ä¸‹é™ãªã‚‰å£²ã‚Š(-1)\n",
    "    signals = np.where(predicted_changes > 0, 1, -1)\n",
    "    signals = pd.Series(signals, index=predicted_changes.index)\n",
    "    \n",
    "    # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—ï¼ˆå®Ÿéš›ã®ä¾¡æ ¼å¤‰åŒ– Ã— ã‚·ã‚°ãƒŠãƒ«ï¼‰\n",
    "    strategy_returns = actual_changes * signals.shift(1)  # 1æœŸå‰ã®ã‚·ã‚°ãƒŠãƒ«ã§å–å¼•\n",
    "    strategy_returns = strategy_returns.dropna()\n",
    "    \n",
    "    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—\n",
    "    total_return = strategy_returns.sum()\n",
    "    win_rate = (strategy_returns > 0).mean()\n",
    "    avg_win = strategy_returns[strategy_returns > 0].mean() if (strategy_returns > 0).any() else 0\n",
    "    avg_loss = strategy_returns[strategy_returns < 0].mean() if (strategy_returns < 0).any() else 0\n",
    "    sharpe_ratio = strategy_returns.mean() / strategy_returns.std() if strategy_returns.std() > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'total_return': total_return,\n",
    "        'win_rate': win_rate,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'num_trades': len(strategy_returns)\n",
    "    }\n",
    "\n",
    "if data is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    # å„ãƒ¢ãƒ‡ãƒ«ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\n",
    "    if 'predictions_rf' in locals():\n",
    "        sim_rf = run_trading_simulation(predictions_rf, test_period_data, \"Random Forest\")\n",
    "        if sim_rf:\n",
    "            simulation_results.append(sim_rf)\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        sim_xgb = run_trading_simulation(predictions_xgb, test_period_data, \"XGBoost\")\n",
    "        if sim_xgb:\n",
    "            simulation_results.append(sim_xgb)\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        sim_ensemble = run_trading_simulation(ensemble_predictions, test_period_data, \"Ensemble\")\n",
    "        if sim_ensemble:\n",
    "            simulation_results.append(sim_ensemble)\n",
    "    \n",
    "    # Buy & Hold æˆ¦ç•¥ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰\n",
    "    buy_hold_return = test_period_data.iloc[-1] - test_period_data.iloc[0]\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    if simulation_results:\n",
    "        sim_df = pd.DataFrame(simulation_results)\n",
    "        sim_df['win_rate_pct'] = sim_df['win_rate'] * 100\n",
    "        \n",
    "        print(\"ãƒ¢ãƒ‡ãƒ«åˆ¥å–å¼•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:\")\n",
    "        display_cols = ['model', 'total_return', 'win_rate_pct', 'sharpe_ratio', 'num_trades']\n",
    "        print(sim_df[display_cols].to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (Buy & Hold): {buy_hold_return:.2f}\")\n",
    "        \n",
    "        # æœ€è‰¯å–å¼•æˆ¦ç•¥\n",
    "        best_strategy = sim_df.loc[sim_df['total_return'].idxmax()]\n",
    "        print(f\"\\nğŸ† æœ€è‰¯å–å¼•æˆ¦ç•¥: {best_strategy['model']}\")\n",
    "        print(f\"   ç·ãƒªã‚¿ãƒ¼ãƒ³: {best_strategy['total_return']:.2f}\")\n",
    "        print(f\"   å‹ç‡: {best_strategy['win_rate_pct']:.1f}%\")\n",
    "        print(f\"   ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª: {best_strategy['sharpe_ratio']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"åˆ†æå®Œäº†: Cash/3Mã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã®åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n }\n,\n "nbformat": 4,\n "nbformat_minor": 4\n}