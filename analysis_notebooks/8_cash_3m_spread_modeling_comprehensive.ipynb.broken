{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LME銅 Cash/3Mスプレッド 包括的モデリング分析\n",
    "\n",
    "## 分析目的\n",
    "このノートブックでは、Cash/3Mスプレッドに対して以下の包括的なモデリングを実行します：\n",
    "\n",
    "### 実装モデル:\n",
    "1. **ARIMA/GARCH**: 時系列の平均・分散モデリング\n",
    "2. **Random Forest**: アンサンブル機械学習\n",
    "3. **XGBoost**: 勾配ブースティング\n",
    "4. **LSTM**: 深層学習による時系列予測\n",
    "5. **Prophet**: Facebookの時系列予測ライブラリ\n",
    "6. **アンサンブル**: 複数モデルの組み合わせ\n",
    "\n",
    "### 評価項目:\n",
    "- 予測精度（MAE, RMSE, MAPE）\n",
    "- 方向性精度（上昇/下降の的中率）\n",
    "- リスク調整リターン（シャープレシオ）\n",
    "- 取引シミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# 時系列分析\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "\n",
    "# ディープラーニング\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(\"✓ TensorFlow available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"TensorFlow not available - LSTM models will be skipped\")\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "    print(\"✓ Prophet available\")\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "    print(\"Prophet not available - Prophet models will be skipped\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# データベース設定\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'lme_copper_db'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'password'),\n",
    "    'port': os.getenv('DB_PORT', '5432')\n",
    "}\n",
    "\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "# スタイル設定\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# モデル保存ディレクトリ\n",
    "model_dir = '/Users/Yusuke/claude-code/RefinitivDB/models/cash_3m_spread'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(\"✓ ライブラリ読み込み完了\")\n",
    "print(\"Ready for comprehensive Cash/3M spread modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備と特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込みと特徴量作成\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Cash/3Mスプレッドデータの読み込みと特徴量作成\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # スプレッドデータ\n",
    "        spread_query = \"\"\"\n",
    "        SELECT \n",
    "            trade_date,\n",
    "            last_price as spread_price,\n",
    "            volume as spread_volume\n",
    "        FROM lme_copper_prices\n",
    "        WHERE price_type = 'CASH_3M_SPREAD'\n",
    "        AND last_price IS NOT NULL\n",
    "        ORDER BY trade_date\n",
    "        \"\"\"\n",
    "        \n",
    "        spread_df = pd.read_sql_query(spread_query, engine)\n",
    "        \n",
    "        # 3Mアウトライトデータ（補助情報として）\n",
    "        outright_query = \"\"\"\n",
    "        SELECT \n",
    "            trade_date,\n",
    "            last_price as outright_price,\n",
    "            volume as outright_volume\n",
    "        FROM lme_copper_prices\n",
    "        WHERE price_type = '3M_OUTRIGHT'\n",
    "        AND last_price IS NOT NULL\n",
    "        ORDER BY trade_date\n",
    "        \"\"\"\n",
    "        \n",
    "        outright_df = pd.read_sql_query(outright_query, engine)\n",
    "        engine.dispose()\n",
    "        \n",
    "        # データマージ\n",
    "        df = pd.merge(spread_df, outright_df, on='trade_date', how='inner')\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df.set_index('trade_date', inplace=True)\n",
    "        \n",
    "        # 数値型変換\n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df = df.dropna()\n",
    "        \n",
    "        print(f\"データ読み込み完了: {len(df)}件\")\n",
    "        print(f\"期間: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"データ読み込みエラー: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"包括的な特徴量作成\"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # 基本的な変化率\n",
    "    features_df['spread_change'] = features_df['spread_price'].diff()\n",
    "    features_df['spread_pct_change'] = features_df['spread_price'].pct_change()\n",
    "    features_df['outright_change'] = features_df['outright_price'].diff()\n",
    "    features_df['outright_pct_change'] = features_df['outright_price'].pct_change()\n",
    "    \n",
    "    # 移動平均\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        features_df[f'spread_ma_{window}'] = features_df['spread_price'].rolling(window).mean()\n",
    "        features_df[f'outright_ma_{window}'] = features_df['outright_price'].rolling(window).mean()\n",
    "        \n",
    "        # 移動平均からの乖離\n",
    "        features_df[f'spread_ma_dev_{window}'] = features_df['spread_price'] - features_df[f'spread_ma_{window}']\n",
    "    \n",
    "    # ボラティリティ\n",
    "    for window in [10, 20, 30]:\n",
    "        features_df[f'spread_vol_{window}'] = features_df['spread_change'].rolling(window).std()\n",
    "        features_df[f'outright_vol_{window}'] = features_df['outright_change'].rolling(window).std()\n",
    "    \n",
    "    # RSI\n",
    "    def calculate_rsi(prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    features_df['spread_rsi'] = calculate_rsi(features_df['spread_price'])\n",
    "    features_df['outright_rsi'] = calculate_rsi(features_df['outright_price'])\n",
    "    \n",
    "    # Z-score\n",
    "    for window in [20, 50]:\n",
    "        mean_val = features_df['spread_price'].rolling(window).mean()\n",
    "        std_val = features_df['spread_price'].rolling(window).std()\n",
    "        features_df[f'spread_zscore_{window}'] = (features_df['spread_price'] - mean_val) / std_val\n",
    "    \n",
    "    # ラグ特徴量\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        features_df[f'spread_lag_{lag}'] = features_df['spread_price'].shift(lag)\n",
    "        features_df[f'spread_change_lag_{lag}'] = features_df['spread_change'].shift(lag)\n",
    "    \n",
    "    # 出来高関連\n",
    "    features_df['volume_ratio'] = features_df['spread_volume'] / features_df['outright_volume']\n",
    "    features_df['spread_volume_ma_5'] = features_df['spread_volume'].rolling(5).mean()\n",
    "    features_df['volume_change'] = features_df['spread_volume'].pct_change()\n",
    "    \n",
    "    # 時間特徴量\n",
    "    features_df['month'] = features_df.index.month\n",
    "    features_df['quarter'] = features_df.index.quarter\n",
    "    features_df['day_of_week'] = features_df.index.dayofweek\n",
    "    features_df['day_of_month'] = features_df.index.day\n",
    "    \n",
    "    # 月・曜日のサイクリック変換\n",
    "    features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['dow_sin'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    features_df['dow_cos'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    \n",
    "    # 市場状況指標\n",
    "    features_df['backwardation'] = (features_df['spread_price'] > 0).astype(int)\n",
    "    features_df['extreme_backwardation'] = (features_df['spread_price'] > features_df['spread_price'].quantile(0.8)).astype(int)\n",
    "    features_df['extreme_contango'] = (features_df['spread_price'] < features_df['spread_price'].quantile(0.2)).astype(int)\n",
    "    \n",
    "    # 欠損値処理\n",
    "    features_df = features_df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(f\"特徴量作成完了: {features_df.shape[1]}個の特徴量\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# データ準備実行\n",
    "raw_data = load_and_prepare_data()\n",
    "if raw_data is not None:\n",
    "    data = create_features(raw_data)\n",
    "    print(f\"\\n最終データシェイプ: {data.shape}\")\n",
    "    print(f\"特徴量一覧: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル1: ARIMA + GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA + GARCH モデル\n",
    "class ARIMAGARCHModel:\n",
    "    def __init__(self):\n",
    "        self.arima_model = None\n",
    "        self.garch_model = None\n",
    "        self.fitted_arima = None\n",
    "        self.fitted_garch = None\n",
    "        \n",
    "    def fit(self, data, arima_order=(1,1,1), garch_p=1, garch_q=1):\n",
    "        \"\"\"ARIMA + GARCH モデルの学習\"\"\"\n",
    "        try:\n",
    "            # ARIMA フィッティング\n",
    "            self.arima_model = ARIMA(data, order=arima_order)\n",
    "            self.fitted_arima = self.arima_model.fit()\n",
    "            \n",
    "            # ARIMA残差を取得\n",
    "            residuals = self.fitted_arima.resid\n",
    "            \n",
    "            # GARCH フィッティング（残差の分散モデリング）\n",
    "            self.garch_model = arch_model(residuals, vol='GARCH', p=garch_p, q=garch_q)\n",
    "            self.fitted_garch = self.garch_model.fit(disp='off')\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ARIMA-GARCH フィッティングエラー: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"予測実行\"\"\"\n",
    "        if self.fitted_arima is None:\n",
    "            return None, None\n",
    "        \n",
    "        # ARIMA予測\n",
    "        arima_forecast = self.fitted_arima.forecast(steps=steps)\n",
    "        \n",
    "        # GARCH予測（ボラティリティ）\n",
    "        if self.fitted_garch is not None:\n",
    "            garch_forecast = self.fitted_garch.forecast(horizon=steps)\n",
    "            volatility = np.sqrt(garch_forecast.variance.iloc[-1, :].values)\n",
    "        else:\n",
    "            volatility = None\n",
    "        \n",
    "        return arima_forecast, volatility\n",
    "\n",
    "# ARIMA-GARCH実行\n",
    "if data is not None:\n",
    "    print(\"ARIMA + GARCH モデル学習中...\")\n",
    "    \n",
    "    # データ分割\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data['spread_price'][:train_size]\n",
    "    test_data = data['spread_price'][train_size:]\n",
    "    \n",
    "    # モデル学習\n",
    "    arima_garch = ARIMAGARCHModel()\n",
    "    success = arima_garch.fit(train_data)\n",
    "    \n",
    "    if success:\n",
    "        print(\"✅ ARIMA-GARCH学習完了\")\n",
    "        \n",
    "        # テストデータ予測\n",
    "        predictions_ag = []\n",
    "        volatilities_ag = []\n",
    "        \n",
    "        # ウォークフォワード予測\n",
    "        for i in range(len(test_data)):\n",
    "            pred, vol = arima_garch.predict(steps=1)\n",
    "            predictions_ag.append(pred.iloc[0] if hasattr(pred, 'iloc') else pred[0])\n",
    "            volatilities_ag.append(vol[0] if vol is not None else np.nan)\n",
    "            \n",
    "            # モデル更新（新しいデータポイントを追加）\n",
    "            if i < len(test_data) - 1:\n",
    "                updated_data = pd.concat([train_data, test_data.iloc[:i+1]])\n",
    "                arima_garch.fit(updated_data)\n",
    "        \n",
    "        predictions_ag = pd.Series(predictions_ag, index=test_data.index)\n",
    "        \n",
    "        # 評価指標計算\n",
    "        mae_ag = mean_absolute_error(test_data, predictions_ag)\n",
    "        rmse_ag = np.sqrt(mean_squared_error(test_data, predictions_ag))\n",
    "        \n",
    "        print(f\"ARIMA-GARCH MAE: {mae_ag:.3f}\")\n",
    "        print(f\"ARIMA-GARCH RMSE: {rmse_ag:.3f}\")\n",
    "        \n",
    "        # モデル保存\n",
    "        with open(f'{model_dir}/arima_garch_model.pkl', 'wb') as f:\n",
    "            pickle.dump(arima_garch, f)\n",
    "        \n",
    "    else:\n",
    "        predictions_ag = None\n",
    "        print(\"❌ ARIMA-GARCH学習失敗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest モデル\n",
    "def prepare_ml_data(data, target_col='spread_price', forecast_horizon=1):\n",
    "    \"\"\"機械学習用データ準備\"\"\"\n",
    "    # 特徴量選択（基本的な数値特徴量のみ）\n",
    "    feature_cols = [col for col in data.columns if col not in [target_col] \n",
    "                   and not col.startswith('spread_lag')  # ラグ特徴量は別途処理\n",
    "                   and data[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # ラグ特徴量を追加（データリークを防ぐ）\n",
    "    lag_features = [col for col in data.columns if col.startswith('spread_lag')]\n",
    "    feature_cols.extend(lag_features)\n",
    "    \n",
    "    X = data[feature_cols].copy()\n",
    "    \n",
    "    # ターゲット変数（将来の値を予測）\n",
    "    y = data[target_col].shift(-forecast_horizon)  # 1期先を予測\n",
    "    \n",
    "    # 欠損値処理\n",
    "    valid_idx = ~(X.isna().any(axis=1) | y.isna())\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "if data is not None:\n",
    "    print(\"Random Forest モデル学習中...\")\n",
    "    \n",
    "    # データ準備\n",
    "    X, y, feature_cols = prepare_ml_data(data)\n",
    "    \n",
    "    print(f\"特徴量数: {len(feature_cols)}\")\n",
    "    print(f\"サンプル数: {len(X)}\")\n",
    "    \n",
    "    # データ分割\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # 特徴量スケーリング\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Random Forest ハイパーパラメータ調整\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Time Series Cross Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, rf_params, n_iter=20, cv=tscv, \n",
    "        scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    rf_search.fit(X_train_scaled, y_train)\n",
    "    best_rf = rf_search.best_estimator_\n",
    "    \n",
    "    print(f\"最適パラメータ: {rf_search.best_params_}\")\n",
    "    \n",
    "    # 予測\n",
    "    predictions_rf = best_rf.predict(X_test_scaled)\n",
    "    predictions_rf = pd.Series(predictions_rf, index=y_test.index)\n",
    "    \n",
    "    # 評価\n",
    "    mae_rf = mean_absolute_error(y_test, predictions_rf)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, predictions_rf))\n",
    "    r2_rf = r2_score(y_test, predictions_rf)\n",
    "    \n",
    "    print(f\"Random Forest MAE: {mae_rf:.3f}\")\n",
    "    print(f\"Random Forest RMSE: {rmse_rf:.3f}\")\n",
    "    print(f\"Random Forest R²: {r2_rf:.3f}\")\n",
    "    \n",
    "    # 特徴量重要度\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n上位10特徴量:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # モデル保存\n",
    "    joblib.dump(best_rf, f'{model_dir}/random_forest_model.pkl')\n",
    "    joblib.dump(scaler, f'{model_dir}/scaler.pkl')\n",
    "    \n",
    "    print(\"✅ Random Forest学習完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost モデル\n",
    "if data is not None:\n",
    "    print(\"XGBoost モデル学習中...\")\n",
    "    \n",
    "    # XGBoost ハイパーパラメータ\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb_model, xgb_params, n_iter=20, cv=tscv,\n",
    "        scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_search.fit(X_train_scaled, y_train)\n",
    "    best_xgb = xgb_search.best_estimator_\n",
    "    \n",
    "    print(f\"最適パラメータ: {xgb_search.best_params_}\")\n",
    "    \n",
    "    # 予測\n",
    "    predictions_xgb = best_xgb.predict(X_test_scaled)\n",
    "    predictions_xgb = pd.Series(predictions_xgb, index=y_test.index)\n",
    "    \n",
    "    # 評価\n",
    "    mae_xgb = mean_absolute_error(y_test, predictions_xgb)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "    r2_xgb = r2_score(y_test, predictions_xgb)\n",
    "    \n",
    "    print(f\"XGBoost MAE: {mae_xgb:.3f}\")\n",
    "    print(f\"XGBoost RMSE: {rmse_xgb:.3f}\")\n",
    "    print(f\"XGBoost R²: {r2_xgb:.3f}\")\n",
    "    \n",
    "    # 特徴量重要度\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n上位10特徴量:\")\n",
    "    print(xgb_importance.head(10))\n",
    "    \n",
    "    # モデル保存\n",
    "    joblib.dump(best_xgb, f'{model_dir}/xgboost_model.pkl')\n",
    "    \n",
    "    print(\"✅ XGBoost学習完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル4: LSTM （深層学習）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM モデル\n",
    "def create_lstm_sequences(data, sequence_length=30):\n",
    "    \"\"\"LSTM用のシーケンスデータ作成\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(sequence_length, len(data)):\n",
    "        sequences.append(data[i-sequence_length:i])\n",
    "        targets.append(data[i])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "if TENSORFLOW_AVAILABLE and data is not None:\n",
    "    print(\"LSTM モデル学習中...\")\n",
    "    \n",
    "    # LSTM用データ準備\n",
    "    spread_values = data['spread_price'].values\n",
    "    \n",
    "    # 正規化\n",
    "    lstm_scaler = MinMaxScaler()\n",
    "    spread_scaled = lstm_scaler.fit_transform(spread_values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # シーケンス作成\n",
    "    sequence_length = 30\n",
    "    X_seq, y_seq = create_lstm_sequences(spread_scaled, sequence_length)\n",
    "    \n",
    "    # データ分割\n",
    "    train_size = int(len(X_seq) * 0.8)\n",
    "    X_train_seq = X_seq[:train_size]\n",
    "    X_test_seq = X_seq[train_size:]\n",
    "    y_train_seq = y_seq[:train_size]\n",
    "    y_test_seq = y_seq[train_size:]\n",
    "    \n",
    "    # LSTM形状調整\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], X_train_seq.shape[1], 1))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))\n",
    "    \n",
    "    # LSTMモデル構築\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # コールバック\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "    \n",
    "    # 学習\n",
    "    history = model_lstm.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 予測\n",
    "    predictions_lstm_scaled = model_lstm.predict(X_test_seq, verbose=0)\n",
    "    predictions_lstm = lstm_scaler.inverse_transform(predictions_lstm_scaled).flatten()\n",
    "    \n",
    "    # 実際の値も逆変換\n",
    "    y_test_actual = lstm_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 評価\n",
    "    mae_lstm = mean_absolute_error(y_test_actual, predictions_lstm)\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, predictions_lstm))\n",
    "    \n",
    "    print(f\"LSTM MAE: {mae_lstm:.3f}\")\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.3f}\")\n",
    "    \n",
    "    # 予測結果をSeriesに変換（インデックス調整）\n",
    "    test_dates = data.index[train_size + sequence_length:]\n",
    "    predictions_lstm = pd.Series(predictions_lstm, index=test_dates[:len(predictions_lstm)])\n",
    "    \n",
    "    # モデル保存\n",
    "    model_lstm.save(f'{model_dir}/lstm_model.h5')\n",
    "    joblib.dump(lstm_scaler, f'{model_dir}/lstm_scaler.pkl')\n",
    "    \n",
    "    print(\"✅ LSTM学習完了\")\n",
    "    \n",
    "else:\n",
    "    predictions_lstm = None\n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        print(\"⏭️ TensorFlowが利用できないため、LSTMをスキップ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル5: Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet モデル\n",
    "if PROPHET_AVAILABLE and data is not None:\n",
    "    print(\"Prophet モデル学習中...\")\n",
    "    \n",
    "    # Prophet用データ準備\n",
    "    prophet_data = pd.DataFrame({\n",
    "        'ds': data.index,\n",
    "        'y': data['spread_price'].values\n",
    "    })\n",
    "    \n",
    "    # データ分割\n",
    "    train_size = int(len(prophet_data) * 0.8)\n",
    "    prophet_train = prophet_data[:train_size]\n",
    "    prophet_test = prophet_data[train_size:]\n",
    "    \n",
    "    # Prophetモデル\n",
    "    model_prophet = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10.0\n",
    "    )\n",
    "    \n",
    "    # カスタム季節性追加\n",
    "    model_prophet.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    model_prophet.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n",
    "    \n",
    "    # 学習\n",
    "    model_prophet.fit(prophet_train)\n",
    "    \n",
    "    # 予測\n",
    "    future_prophet = model_prophet.make_future_dataframe(periods=len(prophet_test), freq='D')\n",
    "    forecast_prophet = model_prophet.predict(future_prophet)\n",
    "    \n",
    "    # テスト期間の予測抽出\n",
    "    predictions_prophet = forecast_prophet['yhat'].iloc[train_size:].values\n",
    "    predictions_prophet = pd.Series(predictions_prophet, index=prophet_test['ds'])\n",
    "    \n",
    "    # 評価\n",
    "    mae_prophet = mean_absolute_error(prophet_test['y'], predictions_prophet)\n",
    "    rmse_prophet = np.sqrt(mean_squared_error(prophet_test['y'], predictions_prophet))\n",
    "    \n",
    "    print(f\"Prophet MAE: {mae_prophet:.3f}\")\n",
    "    print(f\"Prophet RMSE: {rmse_prophet:.3f}\")\n",
    "    \n",
    "    # モデル保存\n",
    "    with open(f'{model_dir}/prophet_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model_prophet, f)\n",
    "    \n",
    "    print(\"✅ Prophet学習完了\")\n",
    "    \n",
    "else:\n",
    "    predictions_prophet = None\n",
    "    if not PROPHET_AVAILABLE:\n",
    "        print(\"⏭️ Prophetが利用できないため、Prophetをスキップ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル6: アンサンブル予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンサンブル予測\n",
    "def create_ensemble_predictions(*predictions_list):\n",
    "    \"\"\"複数の予測結果をアンサンブル\"\"\"\n",
    "    # 有効な予測のみ使用\n",
    "    valid_predictions = [pred for pred in predictions_list if pred is not None]\n",
    "    \n",
    "    if not valid_predictions:\n",
    "        return None\n",
    "    \n",
    "    # 共通のインデックスを見つける\n",
    "    common_index = valid_predictions[0].index\n",
    "    for pred in valid_predictions[1:]:\n",
    "        common_index = common_index.intersection(pred.index)\n",
    "    \n",
    "    # 各予測を共通インデックスに合わせる\n",
    "    aligned_predictions = []\n",
    "    for pred in valid_predictions:\n",
    "        aligned_predictions.append(pred.loc[common_index])\n",
    "    \n",
    "    # 単純平均でアンサンブル\n",
    "    ensemble_pred = pd.concat(aligned_predictions, axis=1).mean(axis=1)\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "if data is not None:\n",
    "    print(\"アンサンブル予測作成中...\")\n",
    "    \n",
    "    # 利用可能な予測を収集\n",
    "    available_predictions = []\n",
    "    model_names = []\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        available_predictions.append(predictions_rf)\n",
    "        model_names.append('Random Forest')\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        available_predictions.append(predictions_xgb)\n",
    "        model_names.append('XGBoost')\n",
    "    \n",
    "    if predictions_lstm is not None:\n",
    "        available_predictions.append(predictions_lstm)\n",
    "        model_names.append('LSTM')\n",
    "    \n",
    "    if predictions_prophet is not None:\n",
    "        available_predictions.append(predictions_prophet)\n",
    "        model_names.append('Prophet')\n",
    "    \n",
    "    if available_predictions:\n",
    "        ensemble_predictions = create_ensemble_predictions(*available_predictions)\n",
    "        \n",
    "        # アンサンブル評価（共通インデックスの実際値と比較）\n",
    "        common_test_data = y_test.loc[ensemble_predictions.index]\n",
    "        \n",
    "        mae_ensemble = mean_absolute_error(common_test_data, ensemble_predictions)\n",
    "        rmse_ensemble = np.sqrt(mean_squared_error(common_test_data, ensemble_predictions))\n",
    "        \n",
    "        print(f\"アンサンブル使用モデル: {', '.join(model_names)}\")\n",
    "        print(f\"アンサンブル MAE: {mae_ensemble:.3f}\")\n",
    "        print(f\"アンサンブル RMSE: {rmse_ensemble:.3f}\")\n",
    "        \n",
    "        print(\"✅ アンサンブル予測完了\")\n",
    "    else:\n",
    "        ensemble_predictions = None\n",
    "        print(\"❌ アンサンブル用の予測が不足\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル比較と可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全モデルの結果比較\n",
    "if data is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"全モデル パフォーマンス比較\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 結果まとめ\n",
    "    results = []\n",
    "    \n",
    "    if 'mae_ag' in locals() and predictions_ag is not None:\n",
    "        results.append(['ARIMA-GARCH', mae_ag, rmse_ag])\n",
    "    \n",
    "    if 'mae_rf' in locals():\n",
    "        results.append(['Random Forest', mae_rf, rmse_rf])\n",
    "    \n",
    "    if 'mae_xgb' in locals():\n",
    "        results.append(['XGBoost', mae_xgb, rmse_xgb])\n",
    "    \n",
    "    if 'mae_lstm' in locals():\n",
    "        results.append(['LSTM', mae_lstm, rmse_lstm])\n",
    "    \n",
    "    if 'mae_prophet' in locals():\n",
    "        results.append(['Prophet', mae_prophet, rmse_prophet])\n",
    "    \n",
    "    if 'mae_ensemble' in locals():\n",
    "        results.append(['Ensemble', mae_ensemble, rmse_ensemble])\n",
    "    \n",
    "    # 結果表示\n",
    "    results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE'])\n",
    "    results_df = results_df.sort_values('MAE')\n",
    "    \n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # 最良モデル\n",
    "    best_model = results_df.iloc[0]['Model']\n",
    "    best_mae = results_df.iloc[0]['MAE']\n",
    "    \n",
    "    print(f\"\\n🏆 最良モデル: {best_model} (MAE: {best_mae:.3f})\")\n",
    "    \n",
    "    # 予測結果可視化\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 15))\n",
    "    \n",
    "    # 実際の値（テストデータ期間）\n",
    "    test_period_data = data['spread_price'][train_size:]\n",
    "    ax1.plot(test_period_data.index, test_period_data.values, \n",
    "             label='実際の値', linewidth=2, color='black', alpha=0.8)\n",
    "    \n",
    "    # 各モデルの予測\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        ax1.plot(predictions_rf.index, predictions_rf.values, \n",
    "                 label='Random Forest', linestyle='--', color=colors[0], alpha=0.7)\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        ax1.plot(predictions_xgb.index, predictions_xgb.values, \n",
    "                 label='XGBoost', linestyle='--', color=colors[1], alpha=0.7)\n",
    "    \n",
    "    if predictions_lstm is not None:\n",
    "        ax1.plot(predictions_lstm.index, predictions_lstm.values, \n",
    "                 label='LSTM', linestyle='--', color=colors[2], alpha=0.7)\n",
    "    \n",
    "    if predictions_prophet is not None:\n",
    "        ax1.plot(predictions_prophet.index, predictions_prophet.values, \n",
    "                 label='Prophet', linestyle='--', color=colors[3], alpha=0.7)\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        ax1.plot(ensemble_predictions.index, ensemble_predictions.values, \n",
    "                 label='Ensemble', linestyle='-', color=colors[4], linewidth=2, alpha=0.9)\n",
    "    \n",
    "    ax1.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    ax1.set_title('Cash/3M Spread 予測結果比較', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('スプレッド (USD/tonne)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE比較バーチャート\n",
    "    ax2.bar(results_df['Model'], results_df['MAE'], color=colors[:len(results_df)], alpha=0.7)\n",
    "    ax2.set_title('モデル別 MAE 比較', fontsize=16, fontweight='bold')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 値ラベル追加\n",
    "    for i, v in enumerate(results_df['MAE']):\n",
    "        ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../generated_images/cash_3m_spread_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 方向性精度計算\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"方向性精度 (上昇/下降の的中率)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    actual_direction = np.sign(test_period_data.diff().dropna())\n",
    "    \n",
    "    direction_results = []\n",
    "    \n",
    "    if 'predictions_rf' in locals():\n",
    "        pred_direction = np.sign(predictions_rf.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['Random Forest', direction_acc])\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        pred_direction = np.sign(predictions_xgb.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['XGBoost', direction_acc])\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        pred_direction = np.sign(ensemble_predictions.diff().dropna())\n",
    "        common_idx = actual_direction.index.intersection(pred_direction.index)\n",
    "        direction_acc = (actual_direction.loc[common_idx] == pred_direction.loc[common_idx]).mean()\n",
    "        direction_results.append(['Ensemble', direction_acc])\n",
    "    \n",
    "    direction_df = pd.DataFrame(direction_results, columns=['Model', 'Direction_Accuracy'])\n",
    "    direction_df['Direction_Accuracy_Pct'] = direction_df['Direction_Accuracy'] * 100\n",
    "    direction_df = direction_df.sort_values('Direction_Accuracy', ascending=False)\n",
    "    \n",
    "    print(direction_df[['Model', 'Direction_Accuracy_Pct']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Cash/3M スプレッド モデリング完了\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取引シミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単な取引シミュレーション\n",
    "def run_trading_simulation(predictions, actual_prices, model_name=\"Model\"):\n",
    "    \"\"\"予測に基づく取引シミュレーション\"\"\"\n",
    "    if predictions is None or len(predictions) == 0:\n",
    "        return None\n",
    "    \n",
    "    # 共通インデックス\n",
    "    common_idx = predictions.index.intersection(actual_prices.index)\n",
    "    pred_aligned = predictions.loc[common_idx]\n",
    "    actual_aligned = actual_prices.loc[common_idx]\n",
    "    \n",
    "    if len(pred_aligned) < 2:\n",
    "        return None\n",
    "    \n",
    "    # 取引シグナル生成\n",
    "    predicted_changes = pred_aligned.diff()\n",
    "    actual_changes = actual_aligned.diff()\n",
    "    \n",
    "    # シグナル: 予測上昇なら買い(1)、予測下降なら売り(-1)\n",
    "    signals = np.where(predicted_changes > 0, 1, -1)\n",
    "    signals = pd.Series(signals, index=predicted_changes.index)\n",
    "    \n",
    "    # リターン計算（実際の価格変化 × シグナル）\n",
    "    strategy_returns = actual_changes * signals.shift(1)  # 1期前のシグナルで取引\n",
    "    strategy_returns = strategy_returns.dropna()\n",
    "    \n",
    "    # パフォーマンス計算\n",
    "    total_return = strategy_returns.sum()\n",
    "    win_rate = (strategy_returns > 0).mean()\n",
    "    avg_win = strategy_returns[strategy_returns > 0].mean() if (strategy_returns > 0).any() else 0\n",
    "    avg_loss = strategy_returns[strategy_returns < 0].mean() if (strategy_returns < 0).any() else 0\n",
    "    sharpe_ratio = strategy_returns.mean() / strategy_returns.std() if strategy_returns.std() > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'total_return': total_return,\n",
    "        'win_rate': win_rate,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'num_trades': len(strategy_returns)\n",
    "    }\n",
    "\n",
    "if data is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"取引シミュレーション結果\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    # 各モデルでシミュレーション実行\n",
    "    if 'predictions_rf' in locals():\n",
    "        sim_rf = run_trading_simulation(predictions_rf, test_period_data, \"Random Forest\")\n",
    "        if sim_rf:\n",
    "            simulation_results.append(sim_rf)\n",
    "    \n",
    "    if 'predictions_xgb' in locals():\n",
    "        sim_xgb = run_trading_simulation(predictions_xgb, test_period_data, \"XGBoost\")\n",
    "        if sim_xgb:\n",
    "            simulation_results.append(sim_xgb)\n",
    "    \n",
    "    if ensemble_predictions is not None:\n",
    "        sim_ensemble = run_trading_simulation(ensemble_predictions, test_period_data, \"Ensemble\")\n",
    "        if sim_ensemble:\n",
    "            simulation_results.append(sim_ensemble)\n",
    "    \n",
    "    # Buy & Hold 戦略（ベンチマーク）\n",
    "    buy_hold_return = test_period_data.iloc[-1] - test_period_data.iloc[0]\n",
    "    \n",
    "    # 結果表示\n",
    "    if simulation_results:\n",
    "        sim_df = pd.DataFrame(simulation_results)\n",
    "        sim_df['win_rate_pct'] = sim_df['win_rate'] * 100\n",
    "        \n",
    "        print(\"モデル別取引パフォーマンス:\")\n",
    "        display_cols = ['model', 'total_return', 'win_rate_pct', 'sharpe_ratio', 'num_trades']\n",
    "        print(sim_df[display_cols].to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nベンチマーク (Buy & Hold): {buy_hold_return:.2f}\")\n",
    "        \n",
    "        # 最良取引戦略\n",
    "        best_strategy = sim_df.loc[sim_df['total_return'].idxmax()]\n",
    "        print(f\"\\n🏆 最良取引戦略: {best_strategy['model']}\")\n",
    "        print(f\"   総リターン: {best_strategy['total_return']:.2f}\")\n",
    "        print(f\"   勝率: {best_strategy['win_rate_pct']:.1f}%\")\n",
    "        print(f\"   シャープレシオ: {best_strategy['sharpe_ratio']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"分析完了: Cash/3Mスプレッドの包括的モデリング\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n }\n,\n "nbformat": 4,\n "nbformat_minor": 4\n}